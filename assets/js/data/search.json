[ { "title": "How to secure Form in Qualtrics", "url": "/posts/secureFormQualtrics/", "categories": "Corpo", "tags": "qualtrics", "date": "2025-05-16 07:45:00 -0400", "snippet": "post 1To create a public Qualtrics survey while ensuring robust protection against bots, fraudulent responses, and unauthorized access, you can leverage several security features. Since the survey is public (likely using an anonymous link), balancing accessibility with security is key. Below are the available options to protect your survey, tailored to a public distribution context:1. Bot and Fraud Detection reCAPTCHA (Bot Detection): What It Does: Integrates Google’s reCAPTCHA v3 to assign a Q_RecaptchaScore (0–1) to each response, identifying potential bots (scores &amp;lt;0.5 suggest bots). How to Enable: Go to Survey Options &amp;gt; Security &amp;gt; Bot Detection and turn on “Enable Bot Detection.” Pros: Invisible to respondents, doesn’t disrupt user experience. Cons: May fail if respondents disable JavaScript or are in regions where Google is blocked (e.g., China). Empty scores may indicate bots. Tip: In the Data &amp;amp; Analysis tab, filter out responses with low or missing Q_RecaptchaScore values. CAPTCHA Verification Question: What It Does: Adds a reCAPTCHA v2 checkbox or image challenge (e.g., “I’m not a robot”) as a survey question to verify human respondents. How to Enable: In the survey builder, add a CAPTCHA question (found in question types) as the first question. Pros: Effective against basic bots, easy to implement. Cons: May deter some legitimate respondents due to added friction; sophisticated bots can bypass it. Tip: Use this alongside reCAPTCHA v3 for layered protection. RelevantID: What It Does: Analyzes metadata (e.g., IP address, browser data) to detect duplicate or fraudulent responses, assigning a Q_RelevantIDFraudScore and flagging duplicates with Q_RelevantIDDuplicate. How to Enable: Go to Survey Options &amp;gt; Security &amp;gt; Fraud Detection and enable RelevantID. Pros: Reduces duplicate submissions from the same user or device. Cons: May flag legitimate responses (e.g., multiple users on the same IP, like in a household) as duplicates. Tip: Review flagged responses manually in Data &amp;amp; Analysis to avoid excluding valid data. 2. Preventing Multiple Submissions Prevent Ballot Box Stuffing: What It Does: Blocks respondents from submitting the survey multiple times by using cookies or metadata to identify repeat attempts. Respondents see a message stating they’ve already taken the survey. How to Enable: Go to Survey Options &amp;gt; Security &amp;gt; Prevent Multiple Submissions and enable it. Pros: Simple way to limit one response per user. Cons: Respondents can clear cookies or use different devices to bypass. Not foolproof for anonymous links. Tip: Combine with RelevantID for stronger duplicate prevention. IP Address Restrictions: What It Does: Limits responses to one per IP address or blocks responses from suspicious IPs known for fraud. How to Enable: Go to Survey Options &amp;gt; Security &amp;gt; IP Address Options and configure restrictions. Pros: Reduces duplicates from the same network. Cons: May block legitimate respondents sharing an IP (e.g., in offices or schools). Use cautiously for public surveys. Tip: Avoid strict IP blocking unless fraud is a major concern, as it can exclude valid users. 3. Access Control for Public Surveys Password Protection: What It Does: Requires respondents to enter a password to access the survey, even with an anonymous link. How to Enable: Go to Survey Options &amp;gt; Security &amp;gt; Survey Password and set a password. Pros: Adds a barrier to unauthorized access. Cons: Complicates access for legitimate respondents; you’ll need to share the password securely (e.g., via email or a trusted website). Tip: Use a simple password and communicate it clearly to your target audience. HTTP Referer Checking: What It Does: Restricts survey access to respondents coming from a specific website URL (e.g., your organization’s website). How to Enable: Go to Survey Options &amp;gt; Security &amp;gt; HTTP Referer Checking and specify the allowed URL. Pros: Prevents access from unauthorized sources or random links. Cons: May block respondents if the referring URL isn’t set up correctly or if they access the link directly. Tip: Embed the survey link on a trusted webpage to control access. 4. Data Privacy and Anonymity Anonymize Responses: What It Does: Excludes IP addresses, location data, or other identifiable information from being recorded. How to Enable: Go to Survey Options &amp;gt; Security &amp;gt; Anonymize Responses and enable it. Pros: Protects respondent privacy, encourages honest responses, and aligns with GDPR/CCPA. Cons: Limits ability to track duplicates via IP or location, so rely on other fraud detection tools. Tip: Use if respondent anonymity is a priority, but pair with reCAPTCHA and RelevantID. TLS Encryption: What It Does: Automatically encrypts all survey data in transit using TLS/HTTPS to prevent eavesdropping or data tampering. How to Enable: Enabled by default for all Qualtrics surveys. Pros: Ensures data security without extra setup. Cons: None, as it’s standard. Tip: Verify your survey link starts with “https://” to confirm encryption. 5. Additional Safeguards Block Search Engine Indexing: What It Does: Prevents the survey from appearing in search engine results, reducing exposure to bots or unintended respondents. How to Enable: Go to Survey Options &amp;gt; Security &amp;gt; Prevent Search Engine Indexing and enable it. Pros: Keeps the survey link private and reduces bot traffic. Cons: None, as it doesn’t affect legitimate respondents. Tip: Always enable for public surveys to minimize unwanted access. Honeypot Questions (Custom Implementation): What It Does: Adds hidden or trick questions (e.g., “Leave this blank” or “Select ‘Blue’”) to catch bots or inattentive respondents. How to Enable: Manually create a question with display logic to hide it from humans (requires custom JavaScript) or use a visible attention check. Pros: Effective against bots that auto-fill surveys. Cons: Requires advanced setup; may confuse some respondents if not designed carefully. Tip: Consult Qualtrics Support or the Qualtrics Community for JavaScript examples to implement honeypots. ExpertReview: What It Does: Audits your survey for quality and suggests security or design improvements to enhance response integrity. How to Enable: Access ExpertReview in the survey builder under the Tools menu. Pros: Provides actionable recommendations to strengthen survey security. Cons: May require additional configuration based on suggestions. Tip: Run ExpertReview before launching to catch potential vulnerabilities. Recommended Configuration for a Protected Public SurveyTo balance accessibility and security for your public survey, consider this setup: Enable reCAPTCHA v3 (Bot Detection) to filter out bots invisibly. Add a CAPTCHA question as the first survey question for an extra layer of bot protection. Turn on RelevantID to detect duplicates and fraudulent responses. Enable Prevent Ballot Box Stuffing to limit multiple submissions. Use Password Protection if you can share the password with your target audience (e.g., via a website or email). Anonymize Responses to protect privacy, unless you need IP data for fraud detection. Block Search Engine Indexing to keep the survey private. Monitor Responses: Regularly check the Data &amp;amp; Analysis tab for low Q_RecaptchaScore, high Q_RelevantIDFraudScore, or duplicate flags, and filter out suspicious entries.Limitations to Consider Bypass Risks: Sophisticated bots or determined respondents can bypass reCAPTCHA, cookies, or CAPTCHAs (e.g., by clearing cookies, using VPNs, or outsourcing to human solvers). User Experience: Adding too many protections (e.g., CAPTCHA, password, HTTP referer) may deter legitimate respondents, especially on mobile devices. Regional Issues: reCAPTCHA may not work in regions where Google is blocked, so consider alternative fraud detection (e.g., honeypots) if targeting such areas. False Positives: RelevantID or IP restrictions may flag legitimate responses as duplicates (e.g., shared Wi-Fi networks), so review flagged data carefully.Next Steps Test Your Survey: Create a test version and share the anonymous link with colleagues to ensure protections work without blocking legitimate users. Consult Qualtrics Resources: Visit the Qualtrics Support Portal or Trust Center for detailed setup guides on security features. Monitor and Adjust: After launching, check response quality in Data &amp;amp; Analysis and adjust settings (e.g., add honeypots or tighten restrictions) if fraud is detected.For further assistance, contact Qualtrics Support or refer to their Security Statement at qualtrics.com/trust. If you have specific survey goals or a target audience, let me know, and I can tailor these recommendations further!posts 2,3 …How to secure surveyTo secure a Qualtrics survey, restrict access, and protect it from internet attacks, you can implement a combination of built-in security features and best practices. Below is a comprehensive guide based on Qualtrics’ security capabilities and recommendations:1. Restricting Access to the SurveyTo control who can access your survey and limit unauthorized responses: Password Protection: Add a password to the survey to ensure only those with the password can access it. Navigate to Survey Options &amp;gt; Security and enable Survey Password. Provide a password that respondents must enter. This is useful for small, controlled groups but requires secure distribution of the password. Invitation-Only Access: Restrict the survey to invited respondents only. In Survey Options &amp;gt; Security, enable By Invitation Only. This prevents access via anonymous links and requires respondents to receive a personal link or be invited via email. Use Qualtrics’ Contact List to manage and distribute individual links to specific respondents. Prevent Multiple Responses: Enable Prevent Ballot Box Stuffing in Survey Options &amp;gt; Security. This places a cookie in the respondent’s browser to block multiple submissions from the same device. Note: Savvy users may clear cookies or use different devices to bypass this. For stronger control, use an Authenticator in the Survey Flow to require login via Single Sign-On (SSO) or a unique identifier (e.g., email or ID). This ensures respondents are verified and can only take the survey once. IP Address Restrictions: Use Survey Flow to filter respondents based on IP addresses if you want to limit access to specific networks (e.g., a university or corporate network). Alternatively, enable Anonymize Responses in Survey Options &amp;gt; Security to avoid collecting IP addresses if privacy is a concern, but this won’t restrict access. HTTP Referer Verification: In Survey Options &amp;gt; Security, enable HTTP Referer Verification to restrict access to respondents coming from a specific URL (e.g., your organization’s website). This prevents direct access to the survey link from unauthorized sources. Single Sign-On (SSO) Authentication: For institutional or organizational surveys, integrate SSO (e.g., Shibboleth or institutional login like PennKey) to require respondents to authenticate before accessing the survey. In Survey Flow, add an Authenticator element, select SSO, and configure it with your organization’s authentication system. 2. Protecting Against Internet AttacksQualtrics employs robust security measures to safeguard surveys from internet-based threats, and you can enhance these with specific settings: Encryption: Qualtrics uses Transport Layer Security (TLS) encryption (HTTPS) for all data in transit, protecting against eavesdropping and session hijacking. Ensure your survey is accessed via HTTPS (Qualtrics enforces this by default). For sensitive data, Qualtrics offers an additional layer of encryption where you can bring your own key for extra protection. Contact Qualtrics support to enable this for enterprise accounts. Bot Detection: Enable Bot Detection in Survey Options &amp;gt; Security. This uses Google’s invisible reCAPTCHA to assign a score (0–1) to each response, indicating whether it’s likely from a human or a bot. Responses with low scores can be flagged or filtered. Add a CAPTCHA Verification Question to the survey to further deter automated bots. Fraud Detection: Use Qualtrics’ Fraud Detection features to identify suspicious responses. This includes checking for duplicate submissions, unusual response patterns, or metadata analysis to detect repeated attempts by the same user. Enable Prevent Email Scanning in Survey Options &amp;gt; Security to block email scanning software from accessing the survey via links in emails. Server-Side Security: Qualtrics’ servers are protected by high-end firewall systems and undergo regular vulnerability scans and annual third-party penetration tests. Data centers are audited using SSAE-18 standards, ensuring physical and technical security. Qualtrics’ Information Security Management System (ISMS) enforces strict access controls, with role-based access and multi-factor authentication (MFA) for internal users. Data Anonymization: To protect respondent privacy and reduce the risk of data exposure, enable Anonymize Responses in Survey Options &amp;gt; Security. This removes identifiable information like IP addresses and location data. Note that enabling this after responses are collected may not retroactively remove data already stored. 3. Additional Security Best Practices Role-Based Access for Survey Admins: Use Qualtrics’ User, Group, &amp;amp; Division Permissions to limit who can view or edit the survey and its responses. For sensitive data (e.g., salary requests), restrict access to specific users and shield responses from “super admins” where possible. Monitor user activity via the Control Panel or APIs to track who accesses survey data. Secure Distribution: Avoid sharing survey links publicly. Use Qualtrics’ Email Distribution tool or Contact Lists to send links to verified respondents. If using anonymous links, combine with Prevent Ballot Box Stuffing and CAPTCHA to reduce abuse. Compliance with Regulations: For surveys collecting sensitive data (e.g., PHI under HIPAA or personal data under GDPR), ensure compliance by leveraging Qualtrics’ features like GDPR-compliant data deletion and HIPAA-compliant settings. Qualtrics is SOC2 Type 2 certified and supports HIPAA through Business Associate Agreements (BAAs). Redact or restrict sensitive data collection (e.g., PII) using Qualtrics’ tools to comply with privacy laws. Survey Design for Security: Include a Consent Question at the start of the survey (e.g., Agree/Disagree) and use Branch Logic to ensure only consenting respondents proceed. This provides a record of consent for IRB or regulatory purposes. Avoid collecting unnecessary personal data to minimize risk in case of a breach. Monitor and Audit: Regularly review response data for anomalies (e.g., duplicate submissions or bot-like patterns). Qualtrics’ Security Operations Center monitors platform activity, and you can enable MFA for your account to enhance admin security. 4. Limitations and Considerations Bypassing Restrictions: Cookie-based restrictions (e.g., Prevent Ballot Box Stuffing) can be circumvented by clearing cookies or using different devices. For high-stakes surveys, use SSO or a custom webservice to track unique respondents. Third-Party Risks: If embedding surveys on external websites, ensure those sites are secure, as Qualtrics cannot control third-party environments. Keylogger Risks: While Qualtrics secures data in transit and at rest, respondents using compromised devices (e.g., with keyloggers) may still be at risk. Advise respondents to use trusted devices. Shared Accounts: Avoid sharing Qualtrics accounts, as this increases the risk of unauthorized access. Use individual user accounts with specific permissions.5. For Advanced Needs Custom Webservice: For strict control over respondent access (e.g., preventing multiple submissions by the same email), set up a webservice to verify respondent eligibility at the survey’s start. This requires technical expertise and API integration. Data Isolation: For highly sensitive surveys, explore Qualtrics’ Data Isolation options to store data in a specific geographic region or virtual environment. Contact Qualtrics Support: For enterprise features like custom encryption keys, advanced fraud detection, or HIPAA-specific settings, contact Qualtrics to configure your account accordingly.By combining these measures—password protection, invitation-only access, bot detection, encryption, and compliance tools—you can significantly secure your Qualtrics survey, restrict access, and protect it from internet attacks. Always tailor settings to your survey’s sensitivity and audience, and consult your organization’s IT or compliance team for additional requirements. For further details, refer to Qualtrics’ Security Statement or Support Portal.&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;&amp;amp;To secure your Qualtrics survey using only Qualtrics’ built-in security features, while balancing ease of use, robust access restriction, and protection against internet attacks, I recommend the following combination of settings based on common use cases and Qualtrics’ capabilities. This assumes you want a secure survey accessible to a specific group of respondents, with protection against bots and unauthorized access, without requiring advanced technical integrations like SSO or custom webservices.Recommended Combination of Qualtrics Security Features By Invitation Only (Restrict Access): Why: Ensures only respondents with a unique, trackable link (sent via Qualtrics’ distribution tools or a contact list) can access the survey, preventing public access or link sharing. How: Go to Survey Options &amp;gt; Security and enable By Invitation Only. Impact: Respondents must be invited via email or receive a personal link, reducing the risk of unauthorized access. Prevent Ballot Box Stuffing (Limit Multiple Responses): Why: Places a cookie in the respondent’s browser to prevent multiple submissions from the same device, deterring repeat attempts. How: In Survey Options &amp;gt; Security, enable Prevent Ballot Box Stuffing. Impact: Simple to implement, though tech-savvy users could bypass it by clearing cookies. Suitable for most surveys unless highly sensitive. Bot Detection with Invisible reCAPTCHA (Protect Against Bots): Why: Uses Google’s invisible reCAPTCHA to assign a score (0–1) to each response, flagging or filtering likely bot submissions without disrupting legitimate respondents. How: In Survey Options &amp;gt; Security, enable Bot Detection. Impact: Automatically protects against automated attacks while maintaining a seamless user experience. Survey Password (Add an Extra Layer of Access Control): Why: Requires respondents to enter a shared password, adding a barrier for unauthorized users even if they obtain the survey link. How: In Survey Options &amp;gt; Security, enable Survey Password and set a strong password. Distribute the password securely to intended respondents (e.g., via separate email or communication). Impact: Easy to set up and effective for small groups, but requires secure password distribution to avoid leaks. Anonymize Responses (Protect Respondent Privacy): Why: Removes identifiable information like IP addresses and location data, reducing the risk of data exposure in case of a breach and ensuring compliance with privacy regulations (e.g., GDPR). How: In Survey Options &amp;gt; Security, enable Anonymize Responses. Ensure this is set before collecting responses, as it’s not retroactive. Impact: Enhances privacy but prevents tracking respondents by IP, which may be a trade-off if you need to monitor response origins. Prevent Email Scanning (Secure Email Distribution): Why: Blocks email scanning software from accessing the survey via links in emails, reducing the risk of bots or unauthorized access through email leaks. How: In Survey Options &amp;gt; Security, enable Prevent Email Scanning. Impact: Adds protection for surveys distributed via email, which is common with invitation-only settings. Why This Combination? Access Restriction: By Invitation Only and Survey Password ensure only authorized respondents can access the survey, with unique links and a shared password as dual barriers. Attack Protection: Bot Detection and Prevent Email Scanning guard against automated bots and email-based threats, leveraging Qualtrics’ built-in tools. Response Integrity: Prevent Ballot Box Stuffing reduces multiple submissions, maintaining data quality. Privacy Compliance: Anonymize Responses minimizes data exposure, aligning with privacy best practices and regulations. Ease of Use: These settings are all configurable within Qualtrics’ interface, requiring no advanced technical setup or external integrations.Implementation Steps Go to your survey in Qualtrics. Navigate to Survey Options &amp;gt; Security: Check By Invitation Only. Check Prevent Ballot Box Stuffing. Check Bot Detection. Check Prevent Email Scanning. Enable Survey Password and set a strong password. Check Anonymize Responses (if privacy is a priority). Create a Contact List in Qualtrics for your respondents. Distribute the survey via Qualtrics’ Email Distribution tool, sending unique links to each respondent. Share the survey password securely (e.g., via a separate email or trusted communication channel). Monitor responses in the Data &amp;amp; Analysis tab to flag any suspicious activity (e.g., low reCAPTCHA scores).Considerations Password Management: Ensure the survey password is shared securely to avoid leaks. For large groups, this may be cumbersome, so rely more on unique links. Cookie Bypass: Prevent Ballot Box Stuffing can be bypassed by clearing cookies or using different devices. If this is a concern, consider adding a CAPTCHA Verification Question (available in Qualtrics’ question types) for extra bot protection, though it may slightly disrupt user experience. Anonymization Trade-Off: Enabling Anonymize Responses prevents tracking IP addresses, which could limit your ability to detect suspicious response patterns. If tracking is needed, skip this setting but ensure compliance with privacy laws. Sensitive Surveys: If collecting highly sensitive data (e.g., PHI for HIPAA), contact Qualtrics support to enable HIPAA-compliant settings or additional encryption options, as these may require enterprise account configurations.Alternatives or Additions If you expect respondents to come from a specific website, enable HTTP Referer Verification in Survey Options &amp;gt; Security to restrict access to those referred from a trusted URL. For surveys requiring verified identities (e.g., employees or students), consider using an Authenticator in the Survey Flow with a contact list to match respondents to preloaded data (e.g., email or ID). This avoids external SSO but still requires setting up a contact list.This combination leverages Qualtrics’ core security features to provide strong protection without needing advanced integrations. For further guidance, check Qualtrics’ Support Portal or Security Statement. If your survey has specific requirements (e.g., regulatory compliance or large-scale distribution), let me know, and I can tailor the recommendations further!ÇcccccccccccccccccThanks for clarifying that the survey will function as a service request form used 4–6 times yearly. This implies a low-frequency, likely targeted use case where respondents (e.g., employees, clients, or specific users) submit requests periodically, and security, ease of access for authorized users, and data integrity are critical. Given this context, I’ll refine the previous recommendation to optimize for a service request form using only Qualtrics’ built-in security features, ensuring robust protection, restricted access, and minimal administrative overhead for infrequent use.Optimized Combination of Qualtrics Security Features for a Service Request Form By Invitation Only (Restrict Access to Authorized Users): Why: Limits access to only those with a unique, trackable link, ideal for a controlled group (e.g., employees or pre-identified clients) submitting service requests. Prevents public access or link sharing, which is critical for a form used sparingly. How: In Survey Options &amp;gt; Security, enable By Invitation Only. Benefit for Your Use Case: Since the form is used 4–6 times yearly, you can maintain a Contact List in Qualtrics with pre-approved respondents (e.g., email addresses or IDs) and send unique links each time the form is active, ensuring only authorized users can respond. Authenticator in Survey Flow (Verify Respondent Identity): Why: Instead of a survey password (which requires secure distribution and can be cumbersome for periodic use), use an authenticator to verify respondents against a contact list. This ensures only pre-identified users can access the form and prevents multiple submissions by the same person without relying on cookies. How: Create a Contact List with respondent details (e.g., email, name, or unique ID). In Survey Flow, add an Authenticator element at the start. Set it to Panel or File Authentication, select your contact list, and choose a field (e.g., email or ID) for respondents to enter. Optionally, require a password field from the contact list for added security. Enable Prevent multiple submissions in the authenticator settings to block repeat submissions by the same user. Benefit for Your Use Case: Simplifies access for respondents (no shared password to manage) while ensuring only listed users can submit requests. Ideal for a small, recurring group, as you can reuse or update the contact list each cycle. Bot Detection with Invisible reCAPTCHA (Protect Against Automated Attacks): Why: Protects the form from bots or automated submissions, which could disrupt service requests or submit fraudulent data. Invisible reCAPTCHA minimizes user friction while flagging suspicious activity. How: In Survey Options &amp;gt; Security, enable Bot Detection. Benefit for Your Use Case: Since the form is active only a few times a year, bot detection ensures clean data without requiring respondents to complete CAPTCHA challenges, maintaining a smooth experience for legitimate users. Anonymize Responses (Protect Privacy and Reduce Data Risk): Why: Removes identifiable metadata (e.g., IP addresses, location) to minimize exposure of respondent data, especially if the form collects sensitive service request details (e.g., personal or financial information). This aligns with privacy best practices and regulations like GDPR. How: In Survey Options &amp;gt; Security, enable Anonymize Responses before collecting responses. Benefit for Your Use Case: Reduces risk for a form used infrequently, where data breaches could have significant impact. Note that this prevents tracking respondents by IP, but the authenticator already verifies identity, so this trade-off is minimal. Prevent Email Scanning (Secure Email Distribution): Why: Since you’ll likely distribute the form via email to a contact list, this prevents email scanning software from accessing the form through links, reducing the risk of unauthorized submissions. How: In Survey Options &amp;gt; Security, enable Prevent Email Scanning. Benefit for Your Use Case: Ensures secure delivery of unique links to your respondents, critical for a form distributed periodically to a specific group. Why This Combination is Optimal for Your Service Request Form? Targeted Access: By Invitation Only and the Authenticator ensure only pre-approved respondents (e.g., those in your contact list) can access and submit the form, ideal for a controlled, recurring process like service requests. Streamlined Security: The authenticator replaces the need for a shared survey password, reducing administrative effort (no need to distribute and manage passwords 4–6 times a year) while maintaining strong access control. Attack Protection: Bot Detection and Prevent Email Scanning safeguard the form from bots and email-based threats, ensuring clean and legitimate submissions. Privacy Compliance: Anonymize Responses minimizes data exposure, critical for a form that may collect sensitive request details and is used infrequently, where data security is paramount. Low Maintenance: These settings are easy to configure and maintain, suitable for a form used only a few times yearly. The contact list can be reused or updated each cycle, and settings remain consistent.Implementation Steps Prepare a Contact List: In Qualtrics, go to Contacts and create a contact list with respondent details (e.g., email, name, or unique ID). Include a password field if you want extra security. Update the list as needed before each of the 4–6 annual cycles. Configure Survey Settings: Go to Survey Options &amp;gt; Security: Enable By Invitation Only. Enable Bot Detection. Enable Prevent Email Scanning. Enable Anonymize Responses (if privacy is a priority). Set Up Authenticator: In Survey Flow, add an Authenticator element at the top. Choose Panel or File Authentication, select your contact list, and specify the field (e.g., email or ID) respondents will enter to authenticate. Check Prevent multiple submissions to block repeat entries. Optionally, require a password field from the contact list for added security. Distribute the Form: Use Qualtrics’ Email Distribution tool to send unique links to your contact list each time the form is active (4–6 times yearly). Include clear instructions for respondents to enter their email/ID in the authentication step. Monitor Responses: In the Data &amp;amp; Analysis tab, review submissions and check reCAPTCHA scores to flag any bot-like activity. Export data securely after each cycle and clear responses if needed to maintain privacy. Additional Considerations Why Not Prevent Ballot Box Stuffing?: I excluded Prevent Ballot Box Stuffing because the authenticator’s Prevent multiple submissions feature is more robust, tying submissions to verified identities (e.g., email or ID) rather than cookies, which can be bypassed. This is better for a service request form where data integrity is critical. Why Not Survey Password?: A shared password adds complexity for periodic use (e.g., secure distribution 4–6 times a year) and is less secure than the authenticator, which verifies individual identities. Use a password only if your respondent group is very small and you can securely share it each cycle. Sensitive Data: If the form collects highly sensitive information (e.g., financial or health-related requests), confirm with Qualtrics support that your account supports HIPAA-compliant settings or additional encryption (enterprise feature) to meet regulatory requirements. Form Availability: Since the form is used 4–6 times yearly, consider setting an Open and Close Date in Survey Options for each cycle to limit when the form is active, reducing exposure to unauthorized access. Respondent Experience: The authenticator requires respondents to enter an email or ID, which is straightforward but may need clear instructions. Test the form to ensure ease of use, especially for non-technical users.Alternatives CAPTCHA Verification Question: If you suspect high bot activity and want extra protection, add a CAPTCHA Verification Question (available in Qualtrics’ question types) after the authenticator. However, this may add friction for legitimate users, so use sparingly. HTTP Referer Verification: If respondents access the form via a specific website (e.g., an internal portal), enable HTTP Referer Verification in Survey Options &amp;gt; Security to restrict access to those coming from that URL. This is less relevant if distributing via email.This combination—By Invitation Only, Authenticator, Bot Detection, Anonymize Responses, and Prevent Email Scanning—is tailored for a service request form used 4–6 times yearly. It ensures secure, restricted access, protects against attacks, and minimizes administrative effort while maintaining compliance and data integrity. For further assistance, refer to Qualtrics’ Support Portal or contact their support for enterprise features if needed. If you have details about the respondent group size, sensitivity of data, or specific compliance needs, let me know, and I can refine this further!FfffffffffffffSecuring Your Qualtrics Survey: Access Restrictions &amp;amp; ProtectionTo secure your Qualtrics survey and protect it from unauthorized access and internet attacks, follow these best practices:Access Restrictions Password Protection Enable survey password under “Survey Options” → “Security” Use a strong, unique password and share it only with authorized respondents IP Address Restrictions Restrict access by IP range (available in paid plans) Configure under “Survey Options” → “Security” → “Advanced Security Settings” Authentication Methods Use embedded data for unique access tokens Implement Single Sign-On (SSO) integration for enterprise users Set up anonymous links with unique IDs for tracking Response Requirements Require respondents to log in with specific email domains Enable “Prevent Ballot Box Stuffing” to block multiple responses Security Protection HTTPS Encryption Qualtrics automatically uses HTTPS for all surveys Verify the padlock icon appears in browser address bar Data Protection Enable “Anonymize Responses” if collecting sensitive data Use “Data Retention” settings to automatically delete old responses Bot Protection Enable CAPTCHA under “Survey Options” → “Security” Use reCAPTCHA v3 for invisible bot detection Survey Termination Set expiration dates for your survey Configure “Close Survey After” options under “Survey Options” Advanced Security Features Enable TLS 1.2 encryption (contact Qualtrics Support) Use API tokens with limited permissions for integrations Additional Recommendations Regularly review response data for suspicious patterns Limit the use of open text fields which could be exploited Keep your Qualtrics account credentials secure with 2FA Educate respondents about phishing attempts (never share survey links publicly)For enterprise-level security needs, consider Qualtrics’ advanced security packages or consulting their security team for custom solutions.NnnnnnHere’s a recommended combination of built-in Qualtrics functionalities to secure your survey while balancing usability and protection:1. Basic Access Control Password Protection(Survey Options → Security → Password Protect Survey) Simple but effective for small groups Combine with unique passwords per respondent if needed (shared via email) Prevent Ballot Box Stuffing(Survey Options → Security → Prevent Multiple Submissions) Blocks duplicate responses from the same device 2. Stronger Security (Recommended for Sensitive Data) Anonymous Links + Unique IDs(Distributions → Anonymous Link → Enable “Unique Answer Link”) Generates one-time-use links for each respondent Prevents link sharing Email Verification (if using Panel or Email Distribution)(Survey Options → Security → “Require Email Verification”) Ensures only invited respondents can access the survey CAPTCHA (reCAPTCHA v2 or v3)(Survey Options → Security → Enable CAPTCHA) Blocks automated bots 3. Additional Hardening (For High-Risk Surveys) IP Address Restriction (Enterprise Feature)(Survey Options → Security → “Restrict by IP Range”) Limits access to specific networks (e.g., company VPN) Survey Expiration &amp;amp; Close Date(Survey Options → Survey Termination → Set Close Date) Automatically shuts down the survey after a deadline Response Review Before Submission(Survey Flow → Add “Authenticator” or “Verify Response”) Ensures respondents meet criteria before submitting Best Combination for Most CasesFor general security:✅ Password Protection + Prevent Ballot Stuffing + CAPTCHAFor higher sensitivity:✅ Anonymous Links (Unique IDs) + Email Verification + CAPTCHA + Survey ExpirationFor maximum security (enterprise):✅ IP Restriction + Anonymous Links + CAPTCHA + Survey TerminationAll of these are built-in Qualtrics features—no coding or external tools required.Would you like help setting up a specific combination?YyyyyyyyySince your survey will be used internally by employees on corporate devices and deployed multiple times per year, here’s the optimal built-in Qualtrics security combination tailored for your needs:🔒 Recommended Security Setup (Enterprise Employees on Corporate Devices)1. Access Control (Ensure Only Employees Respond)✅ Single Sign-On (SSO) Authentication (Best for enterprise) (Survey Options → Security → “Require Authentication” → Configure SSO) Forces employees to log in via your company’s identity provider (e.g., Azure AD, Okta). Best choice—eliminates password sharing and ensures only verified employees access the survey.Alternative if SSO isn’t available:✅ Corporate Email Domain Restriction (Survey Options → Security → “Restrict to Email Domains” → Add yourcompany.com) Only allows responses from verified corporate emails.2. Prevent Abuse &amp;amp; Duplicate Submissions✅ Prevent Ballot Box Stuffing (Blocks multiple responses from the same device) (Survey Options → Security → “Prevent Multiple Submissions”) Ensures one response per employee (unless you allow edits).✅ CAPTCHA (reCAPTCHA v3 - Invisible) (Blocks bots without annoying users) (Survey Options → Security → “Enable CAPTCHA”) Minimal friction since it runs silently in the background.3. Additional Hardening (Optional but Recommended)🔹 IP Restriction (If employees only access via corporate network/VPN) (Survey Options → Security → “Restrict by IP Range”) Limits access to office IPs or VPN ranges.🔹 Survey Expiration &amp;amp; Scheduled Closure (Survey Options → Survey Termination → Set Close Date) Automatically closes the survey after the submission period.🔹 Anonymous Links (If distributed manually via email/Teams) (Distributions → Anonymous Link → “Unique Answer Link”) Ensures only the intended recipient can respond.📌 Best Combination for Your Use CaseSince this is a recurring internal survey, the most secure yet user-friendly setup is:1️⃣ Primary Setup (Ideal for SSO-enabled companies)✔ SSO Authentication (Mandatory login via company credentials)✔ Prevent Multiple Submissions (1 response per employee)✔ Invisible CAPTCHA (Silent bot protection)2️⃣ Alternative (If SSO is not configured)✔ Corporate Email Domain Restriction (Only @yourcompany.com allowed)✔ Unique Anonymous Links (Sent via email for tracking)✔ Prevent Ballot Stuffing + CAPTCHA🚀 Why This Works for Your Scenario No passwords needed (SSO or email auth is seamless for employees). Blocks external access (only corporate emails/devices can respond). Low maintenance (reusable settings for each quarterly deployment). No impact on usability (employees won’t face annoying security steps).Would you like help setting up SSO integration or testing the survey flow?" }, { "title": "Dokerize mosquitto throw traefik", "url": "/posts/dockerize-traefik-mosquitto/", "categories": "Docker", "tags": "dockerize, mosquitto, mqtt, tarfeik", "date": "2025-04-30 12:35:00 -0400", "snippet": "Yes, it is possible to dockerize a Mosquitto MQTT broker with TLS (Transport Layer Security) using Traefik as a reverse proxy to handle TLS termination and certificate management (e.g., via Let’s Encrypt). This setup simplifies securing MQTT communication by offloading TLS handling to Traefik, allowing Mosquitto to operate on unencrypted ports internally while exposing secure endpoints externally. Below, I’ll explain the process in detail, including how certificates work in this context, why Traefik is useful, and provide a step-by-step guide with a sample configuration.Why Use Traefik with Mosquitto for TLS?Traefik is a modern reverse proxy that integrates well with Docker, automatically discovers services, and simplifies TLS certificate management using Let’s Encrypt. Using Traefik with Mosquitto offers several benefits: Centralized TLS Management: Traefik handles TLS termination, so Mosquitto doesn’t need to manage certificates directly. Automatic Certificate Renewal: Traefik can obtain and renew Let’s Encrypt certificates, reducing maintenance overhead. Scalability: Traefik supports load balancing across multiple Mosquitto instances if needed. Simplified Configuration: Traefik’s dynamic configuration via Docker labels reduces the need for manual certificate setup in Mosquitto. WebSocket Support: Traefik can handle MQTT over WebSockets, useful for browser-based or IoT applications.In this setup: Mosquitto runs in a Docker container, listening on unencrypted ports (e.g., 1883 for MQTT, 9001 for WebSockets). Traefik listens on secure ports (e.g., 8883 for MQTT, 443 for WebSockets), terminates TLS, and forwards traffic to Mosquitto’s unencrypted ports. Let’s Encrypt provides trusted certificates, eliminating the need for self-signed certificates (though self-signed certificates can be used for testing).How Certificates Work in This SetupTo understand how certificates are managed with Traefik and Mosquitto, let’s revisit the certificate concepts from your previous question and apply them to this context:Certificate Roles CA Certificate (cafile): In a traditional Mosquitto TLS setup, the client (e.g., ESP32) uses the CA certificate to verify the server’s certificate. With Traefik, the CA certificate is typically the Let’s Encrypt root or intermediate certificate, which is already trusted by most clients (e.g., browsers, MQTT libraries). Server Certificate (certfile): Traefik presents a server certificate (obtained from Let’s Encrypt) to clients during the TLS handshake. Mosquitto doesn’t need a server certificate since Traefik handles TLS. Private Key (keyfile): Traefik uses the private key associated with the Let’s Encrypt certificate for cryptographic operations. Mosquitto doesn’t need a private key in this setup. Client Certificates (optional): If mutual TLS is required, Mosquitto can be configured to verify client certificates, but this is less common with Traefik setups and requires additional configuration.File Formats Traefik stores Let’s Encrypt certificates in a JSON file (acme.json) in a mounted volume, not as separate .pem, .crt, or .key files. Clients (e.g., ESP32) may need the Let’s Encrypt CA certificate (e.g., ISRG Root X1) in .pem format to verify Traefik’s certificate. This is often unnecessary for modern clients, as Let’s Encrypt’s CA is widely trusted. If you use self-signed certificates for testing, you’ll need .pem or .crt files for the CA and server certificates, similar to a traditional Mosquitto setup.TLS Flow The client (e.g., ESP32) connects to Traefik on port 8883 (MQTT) or 443 (WebSocket). Traefik presents its Let’s Encrypt certificate to the client. The client verifies the certificate using the Let’s Encrypt CA (usually built into the client’s trust store). Traefik decrypts the incoming traffic and forwards it to Mosquitto on an unencrypted port (e.g., 1883). Mosquitto processes the MQTT messages and responds via Traefik, which re-encrypts the response.This approach simplifies Mosquitto’s configuration, as it doesn’t need to handle TLS directly, but it requires Traefik to be properly configured for TCP routing and TLS.Challenges and Considerations SNI Support: Traefik uses Server Name Indication (SNI) for routing, but many MQTT clients (e.g., mosquitto_pub) don’t support SNI. Traefik’s HostSNI(*) rule can bypass this for TCP services, but testing with such clients may require workarounds. Port Exposure: Mosquitto’s ports (1883, 9001) should not be exposed directly in Docker to ensure all traffic goes through Traefik. Use expose instead of ports in the Docker Compose file. WebSocket Support: If you need MQTT over WebSockets (e.g., for browser-based clients), Traefik must be configured to handle WebSocket traffic on port 443. Certificate Verification: Clients must trust the Let’s Encrypt CA. For self-signed certificates, you’ll need to provide the CA certificate to clients (e.g., ESP32). Mutual TLS: If you require client certificate authentication, Mosquitto must handle it, as Traefik typically doesn’t verify client certificates for TCP services. This requires additional Mosquitto configuration.Step-by-Step Guide to Dockerize Mosquitto with TLS via TraefikBelow is a detailed guide to set up a Mosquitto MQTT broker with TLS using Traefik and Let’s Encrypt in a Docker environment. This setup assumes you have a publicly accessible domain (e.g., mqtt.example.com) for Let’s Encrypt.Prerequisites Docker and Docker Compose installed. A publicly accessible domain (e.g., mqtt.example.com) pointing to your server’s public IP. Ports 80, 443, and 8883 open on your server for HTTP, HTTPS, and MQTT. Basic understanding of Docker, Traefik, and Mosquitto.1. Create Directory StructureCreate a project directory with the following structure:mosquitto-traefik/├── traefik/│ ├── traefik.yml│ └── acme.json├── mosquitto/│ ├── config/│ │ ├── mosquitto.conf│ │ └── passwd│ ├── data/│ └── log/├── .env└── docker-compose.yml2. Configure Environment VariablesCreate a .env file to store configuration variables:MQTT_DOCKER_TAG=latestTRAEFIK_DOCKER_TAG=latestMQTT_LETSENCRYPT_EMAIL=your-email@example.comMQTT_VIRTUAL_HOST=mqtt.example.com MQTT_VIRTUAL_HOST: Your domain for the MQTT broker (e.g., mqtt.example.com). MQTT_LETSENCRYPT_EMAIL: Email for Let’s Encrypt notifications.3. Configure TraefikCreate traefik/traefik.yml for Traefik’s static configuration:entryPoints: web: address: &quot;:80&quot; websecure: address: &quot;:443&quot; mqtt: address: &quot;:8883&quot;providers: docker: exposedByDefault: false network: mqttcertificatesResolvers: letsencrypt: acme: email: &quot;${MQTT_LETSENCRYPT_EMAIL}&quot; storage: /letsencrypt/acme.json tlsChallenge: truelog: level: DEBUG Defines entry points for HTTP (80), HTTPS (443), and MQTT (8883). Configures Let’s Encrypt for automatic certificate generation. Uses the mqtt Docker network for communication.Create an empty traefik/acme.json and set permissions:touch traefik/acme.jsonchmod 600 traefik/acme.json4. Configure MosquittoCreate mosquitto/config/mosquitto.conf for Mosquitto:persistence truepersistence_location /mosquitto/data/log_dest file /mosquitto/log/mosquitto.loglistener 1883protocol mqttallow_anonymous falsepassword_file /mosquitto/config/passwdlistener 9001protocol websocketsallow_anonymous falsepassword_file /mosquitto/config/passwd Listens on port 1883 for MQTT and 9001 for WebSockets. Disables anonymous access and uses a password file for authentication.Create a password file for Mosquitto:docker run -it eclipse-mosquitto:${MQTT_DOCKER_TAG} mosquitto_passwd -c /mosquitto/config/passwd &amp;lt;username&amp;gt;Copy the generated passwd file to mosquitto/config/passwd.5. Create Docker Compose FileCreate docker-compose.yml to define Traefik and Mosquitto services:version: &quot;3.8&quot;services: traefik: image: traefik:${TRAEFIK_DOCKER_TAG} container_name: traefik ports: - &quot;80:80&quot; - &quot;443:443&quot; - &quot;8883:8883&quot; volumes: - ./traefik/traefik.yml:/etc/traefik/traefik.yml - ./traefik/acme.json:/letsencrypt/acme.json - /var/run/docker.sock:/var/run/docker.sock networks: - mqtt restart: always mosquitto: image: eclipse-mosquitto:${MQTT_DOCKER_TAG} container_name: mosquitto expose: - &quot;1883&quot; - &quot;9001&quot; volumes: - ./mosquitto/config:/mosquitto/config - ./mosquitto/data:/mosquitto/data - ./mosquitto/log:/mosquitto/log networks: - mqtt restart: always labels: - &quot;traefik.enable=true&quot; - &quot;traefik.docker.network=mqtt&quot; # MQTT TCP Service - &quot;traefik.tcp.routers.mqtt.rule=HostSNI(`*`)&quot; - &quot;traefik.tcp.routers.mqtt.entrypoints=mqtt&quot; - &quot;traefik.tcp.routers.mqtt.tls=true&quot; - &quot;traefik.tcp.routers.mqtt.tls.certresolver=letsencrypt&quot; - &quot;traefik.tcp.services.mqtt.loadbalancer.server.port=1883&quot; # WebSocket Service - &quot;traefik.http.routers.mqtt-ws.rule=Host(`${MQTT_VIRTUAL_HOST}`)&quot; - &quot;traefik.http.routers.mqtt-ws.entrypoints=websecure&quot; - &quot;traefik.http.routers.mqtt-ws.tls=true&quot; - &quot;traefik.http.routers.mqtt-ws.tls.certresolver=letsencrypt&quot; - &quot;traefik.http.services.mqtt-ws.loadbalancer.server.port=9001&quot;networks: mqtt: driver: bridge Traefik: Exposes ports 80, 443, and 8883, mounts configuration and certificate files, and listens for Docker events. Mosquitto: Exposes internal ports 1883 and 9001 (not bound to the host), mounts configuration and data volumes, and uses Traefik labels for routing. Labels: HostSNI(*) for MQTT TCP routing (bypasses SNI issues). TLS enabled with Let’s Encrypt (tls=true, certresolver=letsencrypt). WebSocket routing uses the domain (Host(${MQTT_VIRTUAL_HOST})). 6. Start the ServicesRun the Docker Compose stack:docker-compose up -d Traefik will obtain a Let’s Encrypt certificate for mqtt.example.com. Mosquitto will start and listen on 1883 (MQTT) and 9001 (WebSockets) internally. Traefik will route external traffic from port 8883 (MQTT) to Mosquitto’s 1883 and from port 443 (WebSocket) to Mosquitto’s 9001.7. Configure ESP32 ClientModify the ESP32 code to connect to the secure MQTT endpoint. Since Traefik uses Let’s Encrypt, most clients trust the CA by default, but you may need to provide the Let’s Encrypt CA certificate for some MQTT libraries.Example Arduino code (using WiFiClientSecure and PubSubClient):#include &amp;lt;WiFi.h&amp;gt;#include &amp;lt;WiFiClientSecure.h&amp;gt;#include &amp;lt;PubSubClient.h&amp;gt;// WiFi credentialsconst char* ssid = &quot;YOUR_WIFI_SSID&quot;;const char* password = &quot;YOUR_WIFI_PASSWORD&quot;;// MQTT broker settingsconst char* mqtt_server = &quot;mqtt.example.com&quot;;const int mqtt_port = 8883;const char* mqtt_user = &quot;YOUR_MQTT_USERNAME&quot;;const char* mqtt_password = &quot;YOUR_MQTT_PASSWORD&quot;;const char* mqtt_client_id = &quot;ESP32_Client&quot;;// Let’s Encrypt CA certificate (optional, if not trusted by default)const char* ca_cert = \\&quot;-----BEGIN CERTIFICATE-----\\n&quot; \\&quot;MIIFazCCA1OgAwIBAgIRAIIQz7DSQONZRGPgu2OCiwAwDQYJKoZIhvcNAQELBQAw\\n&quot; \\&quot;... (ISRG Root X1 certificate, obtainable from https://letsencrypt.org/certs/) ...\\n&quot; \\&quot;-----END CERTIFICATE-----\\n&quot;;WiFiClientSecure espClient;PubSubClient client(espClient);void setup() { Serial.begin(115200); // Connect to WiFi WiFi.begin(ssid, password); while (WiFi.status() != WL_CONNECTED) { delay(1000); Serial.println(&quot;Connecting to WiFi...&quot;); } Serial.println(&quot;Connected to WiFi&quot;); // Set CA certificate (optional if Let’s Encrypt is trusted) espClient.setCACert(ca_cert); // Configure MQTT client.setServer(mqtt_server, mqtt_port); client.setCallback(callback); // Connect to MQTT broker reconnect();}void callback(char* topic, byte* payload, unsigned int length) { Serial.print(&quot;Message received on topic: &quot;); Serial.println(topic); Serial.print(&quot;Payload: &quot;); for (int i = 0; i &amp;lt; length; i++) { Serial.print((char)payload[i]); } Serial.println();}void reconnect() { while (!client.connected()) { Serial.println(&quot;Attempting MQTT connection...&quot;); if (client.connect(mqtt_client_id, mqtt_user, mqtt_password)) { Serial.println(&quot;Connected to MQTT broker&quot;); client.subscribe(&quot;test/topic&quot;); } else { Serial.print(&quot;Failed, rc=&quot;); Serial.print(client.state()); Serial.println(&quot; Retrying in 5 seconds...&quot;); delay(5000); } }}void loop() { if (!client.connected()) { reconnect(); } client.loop();} Use mqtt.example.com as the broker address and port 8883. Include the Let’s Encrypt CA certificate if your MQTT library doesn’t trust it by default.8. Test the Setup MQTT: Use an MQTT client (e.g., MQTT Explorer) to connect to mqtt.example.com:8883 with TLS enabled, using the username and password from the passwd file. WebSocket: Connect to wss://mqtt.example.com:443 using a WebSocket-capable MQTT client (e.g., HiveMQ WebSocket client). Command Line: mosquitto_sub -h mqtt.example.com -p 8883 -t &quot;test/topic&quot; -u &amp;lt;username&amp;gt; -P &amp;lt;password&amp;gt; --tls-version tlsv1.2 Note: Some clients (e.g., mosquitto_sub) may require the Let’s Encrypt CA certificate (--cafile ca.crt) if they don’t trust it by default. 9. Verify Logs Check Traefik logs for certificate issuance and routing: docker logs traefik Check Mosquitto logs for client connections: docker logs mosquitto Optional: Mutual TLS with MosquittoIf you need client certificate authentication (mutual TLS): Configure Mosquitto to require client certificates by adding to mosquitto.conf: listener 1883cafile /mosquitto/config/certs/ca.crtcertfile /mosquitto/config/certs/server.crtkeyfile /mosquitto/config/certs/server.keyrequire_certificate trueuse_identity_as_username true Mount self-signed certificates into the Mosquitto container:```yamlvolumes: ./mosquitto/config/certs:/mosquitto/config/certs``` Generate client certificates and provide them to the ESP32 (as shown in the previous response). Configure Traefik to pass through TLS traffic (tls.passthrough=true) to Mosquitto, as Traefik won’t terminate the client certificate authentication.This approach requires Mosquitto to handle TLS, reducing Traefik’s role to routing, and is more complex due to certificate management.Troubleshooting Certificate Errors: Ensure mqtt.example.com resolves to your server’s public IP. Verify Let’s Encrypt certificate issuance in traefik/acme.json. Use openssl s_client -connect mqtt.example.com:8883 to inspect the presented certificate. Connection Refused: Check that ports 80, 443, and 8883 are open in your firewall. Ensure Mosquitto’s ports are not exposed directly (expose instead of ports). Verify username/password in the passwd file. SNI Issues: If clients don’t support SNI, ensure HostSNI(*) is used in Traefik’s TCP router. WebSocket Issues: Confirm the WebSocket client uses wss:// and port 443. Check Traefik logs for WebSocket routing errors. Logs: Enable verbose logging in Mosquitto (log_type all in mosquitto.conf). Use Traefik’s dashboard (add --api.insecure=true to traefik.yml) to inspect routing. Best Practices Use Let’s Encrypt for Production: Avoid self-signed certificates to simplify client configuration. Secure Passwords: Use strong, unique passwords in Mosquitto’s passwd file. Network Isolation: Place Traefik and Mosquitto in a dedicated Docker network (mqtt) to prevent direct access to Mosquitto. Monitor Certificates: Ensure Traefik renews Let’s Encrypt certificates automatically (stored in acme.json). Backup acme.json: Protect the Let’s Encrypt certificate store to avoid rate limit issues. Update Regularly: Keep Traefik and Mosquitto images up to date (latest or specific versions).Example SourcesThis setup is inspired by community examples and best practices: A Docker Compose configuration for Mosquitto with Traefik and Let’s Encrypt: https://github.com/synoniem/mosquitto-traefik-letsencrypt Traefik documentation on TCP routing: https://docs.traefik.io/routing/routers/#rule_1 Medium article on secure MQTT with Traefik: https://medium.com/himinds/mqtt-broker-with-secure-websocket-using-traefik-docker-compose-and-lets-encrypt-61441879f2f7ConclusionDockerizing Mosquitto with TLS through Traefik is not only possible but also a robust solution for securing MQTT communication. By leveraging Traefik’s TLS termination and Let’s Encrypt integration, you simplify certificate management and enhance scalability. The provided Docker Compose configuration supports both MQTT (port 8883) and WebSocket (port 443) with minimal Mosquitto configuration. For high-security use cases, mutual TLS can be added, though it requires Mosquitto to handle certificates directly.If you need help with specific configurations, testing, or integrating with your ESP32, let me know!" }, { "title": "Secure connection between ESP32 and a Mosquitto MQTT broker And how Certificate works", "url": "/posts/secure-connection-between-esp32-mosquitto/", "categories": "Design", "tags": "qualtrics", "date": "2025-04-30 10:35:00 -0400", "snippet": "Secure connection between ESP32 and a Mosquitto MQTT brokerTo secure the connection between an ESP32 and a Mosquitto MQTT broker, you need to implement encryption (TLS/SSL) and, optionally, authentication (username/password or client certificates). Below are the steps to achieve a secure connection:1. Set Up Mosquitto Broker with TLS/SSLMosquitto needs to be configured to use TLS for encrypted communication.a. Obtain or Generate SSL/TLS Certificates Self-signed certificates (for testing): Use OpenSSL to generate a Certificate Authority (CA) certificate, server certificate, and private key. Example commands: openssl req -new -x509 -days 365 -extensions v3_ca -keyout ca.key -out ca.crtopenssl genrsa -out server.key 2048openssl req -new -out server.csr -key server.keyopenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 Production: Use a trusted CA (e.g., Let’s Encrypt) to obtain certificates.b. Configure Mosquitto Edit the Mosquitto configuration file (e.g., /etc/mosquitto/mosquitto.conf). Add the following to enable TLS: listener 8883cafile /path/to/ca.crtcertfile /path/to/server.crtkeyfile /path/to/server.keyrequire_certificate false # Set to true if using client certificates Optionally, enable username/password authentication: password_file /etc/mosquitto/passwdallow_anonymous false Create a password file: mosquitto_passwd -c /etc/mosquitto/passwd &amp;lt;username&amp;gt; Restart Mosquitto: sudo systemctl restart mosquitto c. Test Mosquitto Configuration Use mosquitto_sub or mosquitto_pub with TLS: mosquitto_sub -h &amp;lt;broker_ip&amp;gt; -p 8883 --cafile ca.crt -t &quot;test/topic&quot; -u &amp;lt;username&amp;gt; -P &amp;lt;password&amp;gt; 2. Configure ESP32 for Secure MQTTUse the ESP-IDF or Arduino framework to connect the ESP32 to the Mosquitto broker securely.a. Include Necessary Libraries Arduino: Use the PubSubClient library for MQTT and WiFiClientSecure for TLS. ESP-IDF: Use the esp-mqtt component with TLS support.b. Load Certificates on ESP32 Embed the CA certificate (and optionally client certificate/key) in the ESP32 firmware. Example for Arduino: const char* ca_cert = \\&quot;-----BEGIN CERTIFICATE-----\\n&quot; \\&quot;YOUR_CA_CERT_HERE\\n&quot; \\&quot;-----END CERTIFICATE-----\\n&quot;; c. ESP32 Arduino Example CodeBelow is an example using the Arduino framework:#include &amp;lt;WiFi.h&amp;gt;#include &amp;lt;WiFiClientSecure.h&amp;gt;#include &amp;lt;PubSubClient.h&amp;gt;// WiFi credentialsconst char* ssid = &quot;YOUR_WIFI_SSID&quot;;const char* password = &quot;YOUR_WIFI_PASSWORD&quot;;// MQTT broker settingsconst char* mqtt_server = &quot;YOUR_BROKER_IP&quot;;const int mqtt_port = 8883;const char* mqtt_user = &quot;YOUR_MQTT_USERNAME&quot;;const char* mqtt_password = &quot;YOUR_MQTT_PASSWORD&quot;;const char* mqtt_client_id = &quot;ESP32_Client&quot;;// CA certificateconst char* ca_cert = \\&quot;-----BEGIN CERTIFICATE-----\\n&quot; \\&quot;YOUR_CA_CERT_HERE\\n&quot; \\&quot;-----END CERTIFICATE-----\\n&quot;;WiFiClientSecure espClient;PubSubClient client(espClient);void setup() { Serial.begin(115200); // Connect to WiFi WiFi.begin(ssid, password); while (WiFi.status() != WL_CONNECTED) { delay(1000); Serial.println(&quot;Connecting to WiFi...&quot;); } Serial.println(&quot;Connected to WiFi&quot;); // Load CA certificate espClient.setCACert(ca_cert); // Configure MQTT client.setServer(mqtt_server, mqtt_port); client.setCallback(callback); // Connect to MQTT broker reconnect();}void callback(char* topic, byte* payload, unsigned int length) { Serial.print(&quot;Message received on topic: &quot;); Serial.println(topic); Serial.print(&quot;Payload: &quot;); for (int i = 0; i &amp;lt; length; i++) { Serial.print((char)payload[i]); } Serial.println();}void reconnect() { while (!client.connected()) { Serial.println(&quot;Attempting MQTT connection...&quot;); if (client.connect(mqtt_client_id, mqtt_user, mqtt_password)) { Serial.println(&quot;Connected to MQTT broker&quot;); client.subscribe(&quot;test/topic&quot;); } else { Serial.print(&quot;Failed, rc=&quot;); Serial.print(client.state()); Serial.println(&quot; Retrying in 5 seconds...&quot;); delay(5000); } }}void loop() { if (!client.connected()) { reconnect(); } client.loop();}d. ESP-IDF ExampleFor ESP-IDF, use the esp-mqtt component with TLS enabled: Configure the MQTT client with the broker URL (mqtts://&amp;lt;broker_ip&amp;gt;:8883). Provide the CA certificate in the mqtt_config structure: esp_mqtt_client_config_t mqtt_cfg = { .broker.address.uri = &quot;mqtts://YOUR_BROKER_IP&quot;, .broker.address.port = 8883, .broker.verification.certificate = ca_cert, .credentials.username = &quot;YOUR_MQTT_USERNAME&quot;, .credentials.password = &quot;YOUR_MQTT_PASSWORD&quot;,}; Initialize and start the MQTT client: esp_mqtt_client_handle_t client = esp_mqtt_client_init(&amp;amp;mqtt_cfg);esp_mqtt_client_start(client); 3. Optional: Client Certificate AuthenticationFor stronger security, use client certificates instead of or in addition to username/password.a. Generate Client Certificates Create a client key and certificate signed by the CA: openssl genrsa -out client.key 2048openssl req -new -out client.csr -key client.keyopenssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365 b. Update Mosquitto Configuration Set require_certificate true in mosquitto.conf. Add the client certificate to the ESP32 firmware: const char* client_cert = \\&quot;-----BEGIN CERTIFICATE-----\\n&quot; \\&quot;YOUR_CLIENT_CERT_HERE\\n&quot; \\&quot;-----END CERTIFICATE-----\\n&quot;;const char* client_key = \\&quot;-----BEGIN PRIVATE KEY-----\\n&quot; \\&quot;YOUR_CLIENT_KEY_HERE\\n&quot; \\&quot;-----END PRIVATE KEY-----\\n&quot;; In Arduino, set the client certificate and key: espClient.setCertificate(client_cert);espClient.setPrivateKey(client_key); c. Update ESP32 Code Ensure the client connects using the client certificate instead of or alongside username/password.4. Best Practices Use Strong Certificates: For production, use certificates from a trusted CA. Secure Certificate Storage: Store certificates securely on the ESP32 (e.g., in SPIFFS or NVS). Regularly Update Certificates: Rotate certificates before they expire. Enable Authentication: Always use username/password or client certificates to prevent unauthorized access. Firewall: Restrict Mosquitto to listen only on necessary ports (e.g., 8883 for TLS). Debugging: Use Serial output on ESP32 and Mosquitto logs (/var/log/mosquitto/) to troubleshoot connection issues.5. Troubleshooting Connection Fails: Verify the CA certificate, server IP, and port. Ensure the ESP32 has the correct time (use NTP for TLS). Certificate Errors: Check certificate validity and ensure the CA certificate is correctly embedded. Authentication Issues: Confirm username/password or client certificate settings match the broker configuration. Logs: Enable verbose logging in Mosquitto (log_type all in mosquitto.conf) for detailed debugging.More details about certifiateTo thoroughly explain how certificates work in the context of securing a connection between an ESP32 and a Mosquitto MQTT broker, we’ll dive into the concepts of TLS/SSL, Public Key Infrastructure (PKI), and the specific roles of certificate-related files (cafile, certfile, keyfile) and their formats (e.g., .pem, .crt, .key). I’ll also clarify why these files are necessary and how they’re used in the Mosquitto and ESP32 setup.1. How Certificates Work in TLS/SSLTLS (Transport Layer Security), the successor to SSL, is a cryptographic protocol used to secure communication over a network, such as between an ESP32 and a Mosquitto broker. Certificates are a core component of TLS, enabling authentication, encryption, and data integrity.Key Concepts of TLS and Certificatesa. Public Key Infrastructure (PKI)PKI is the framework that manages digital certificates and public/private key pairs. It involves: Certificates: Digital documents that bind a public key to an identity (e.g., a server or client). Certificate Authorities (CAs): Trusted entities that issue and sign certificates. Public and Private Keys: Asymmetric cryptography pairs where the public key encrypts data, and the private key decrypts it (or vice versa for signing).b. How TLS WorksWhen an ESP32 connects to a Mosquitto broker using TLS: Handshake: The client (ESP32) and server (Mosquitto) negotiate a secure connection. The server presents its certificate (containing its public key) to the client. The client verifies the certificate’s authenticity using the CA certificate. Optionally, the server may request a client certificate for mutual authentication. Key Exchange: The client and server agree on a symmetric session key (e.g., using Diffie-Hellman) for efficient encryption during the session. Secure Communication: Data is encrypted using the session key, ensuring confidentiality and integrity. c. Role of CertificatesCertificates serve three main purposes: Authentication: Prove the identity of the server (or client, if mutual TLS is used). Public Key Distribution: Provide a trusted public key for encryption or signature verification. Chain of Trust: Link the certificate to a trusted CA, ensuring its legitimacy.A certificate contains: The entity’s identity (e.g., server’s domain or IP). The public key. The CA’s signature, verifying the certificate’s authenticity. Metadata (e.g., validity period, issuer).2. Certificate-Related Files in MosquittoIn the Mosquitto configuration, you specify three types of files: cafile, certfile, and keyfile. Each serves a distinct purpose in the TLS setup.a. cafile (CA Certificate) What it is: The Certificate Authority (CA) certificate contains the CA’s public key and identity. It is used to verify the authenticity of other certificates (e.g., the server’s certificate). Role: The client (ESP32) uses the CA certificate to verify that the server’s certificate was signed by a trusted CA. If the server requires client certificates (mutual TLS), the server uses the CA certificate to verify the client’s certificate. Example: ca.crt or ca.pem. Content: Contains the CA’s public key and metadata, not the private key. Why needed: Establishes the chain of trust. Without it, the client cannot confirm the server’s identity, and the connection may fail or be vulnerable to man-in-the-middle attacks.b. certfile (Server or Client Certificate) What it is: The certificate for the server (or client, in mutual TLS). It contains the entity’s public key and identity, signed by the CA. Role: The server presents its certificate to the client during the TLS handshake to prove its identity. In mutual TLS, the client presents its certificate to the server. Example: server.crt, client.crt, server.pem, or client.pem. Content: Includes the public key, the entity’s identity (e.g., server’s domain or IP), and the CA’s signature. Why needed: Authenticates the server (or client) to the other party. It’s the public-facing part of the entity’s identity.c. keyfile (Private Key) What it is: The private key corresponding to the certificate’s public key. It is kept secret and never shared. Role: The server uses its private key to decrypt messages encrypted with its public key or to sign messages during the TLS handshake. In mutual TLS, the client uses its private key for the same purposes. Example: server.key, client.key, server.key.pem. Content: Contains only the private key, in a format like RSA or ECDSA. Why needed: Enables the entity to prove ownership of the certificate and perform cryptographic operations (e.g., decrypting or signing).3. File Formats (.pem, .crt, .key, etc.)The file extensions .pem, .crt, and .key indicate the encoding or content type of certificate-related files. These are not different types of certificates but different ways of storing the same cryptographic data.a. Common File Formats .pem (Privacy-Enhanced Mail): A versatile, text-based format that can store certificates, private keys, or both. Uses Base64 encoding with headers like: -----BEGIN CERTIFICATE----- for certificates. -----BEGIN PRIVATE KEY----- or -----BEGIN RSA PRIVATE KEY----- for private keys. Example content of a .pem file: -----BEGIN CERTIFICATE-----MII... (Base64-encoded certificate data)-----END CERTIFICATE----- Usage: Can be used for cafile, certfile, or keyfile. A single .pem file may contain multiple certificates (e.g., a certificate chain). .crt (Certificate): Typically used for certificates (public key + metadata). Can be in PEM (text, Base64) or DER (binary) format. Often interchangeable with .pem for certificates. Example: server.crt might be a PEM-encoded file identical to server.pem. .key: Usually used for private keys. Typically PEM-encoded but can be DER. Example: server.key contains only the private key, e.g.: -----BEGIN PRIVATE KEY-----MII... (Base64-encoded private key data)-----END PRIVATE KEY----- .cer: Similar to .crt, often used for certificates in PEM or DER format. Commonly associated with Windows environments. .der (Distinguished Encoding Rules): A binary format for certificates or keys (not human-readable). Less common in Mosquitto/ESP32 setups, as PEM is preferred for simplicity. b. Why Different Extensions? Extensions like .pem, .crt, and .key are conventions to indicate the file’s purpose or content: .pem is generic and can hold certificates, keys, or both. .crt typically indicates a certificate (public part). .key indicates a private key. Tools like OpenSSL and Mosquitto are flexible and can handle these formats as long as the content is correctly formatted (e.g., PEM or DER). The choice of extension is often for clarity or compatibility with specific software.c. Converting Between FormatsYou can convert between formats using OpenSSL: PEM to DER (certificate): openssl x509 -outform der -in server.pem -out server.der DER to PEM: openssl x509 -inform der -in server.der -out server.pem Extract certificate from a .pem containing both certificate and key: openssl x509 -in combined.pem -out certificate.crt 4. How Certificates Are Used in Mosquitto and ESP32a. Mosquitto Broker ConfigurationIn the Mosquitto configuration (mosquitto.conf), you specify: cafile: Path to the CA certificate (e.g., ca.crt or ca.pem) to verify client certificates (if require_certificate true) or to establish trust for the server’s certificate. certfile: Path to the server’s certificate (e.g., server.crt or server.pem) that Mosquitto presents to clients. keyfile: Path to the server’s private key (e.g., server.key) used for cryptographic operations.Example configuration:listener 8883cafile /path/to/ca.crtcertfile /path/to/server.crtkeyfile /path/to/server.keyrequire_certificate false # Set to true for client certificate authentication During the TLS handshake: Mosquitto sends server.crt to the ESP32. The ESP32 uses ca.crt to verify server.crt. Mosquitto uses server.key to prove it owns server.crt. b. ESP32 ConfigurationOn the ESP32: The CA certificate (ca.crt or ca.pem) is embedded in the firmware to verify the Mosquitto server’s certificate. If mutual TLS is enabled, the ESP32 also needs: A client certificate (client.crt or client.pem). A client private key (client.key). Example in Arduino: const char* ca_cert = &quot;-----BEGIN CERTIFICATE-----\\nYOUR_CA_CERT_HERE\\n-----END CERTIFICATE-----\\n&quot;;const char* client_cert = &quot;-----BEGIN CERTIFICATE-----\\nYOUR_CLIENT_CERT_HERE\\n-----END CERTIFICATE-----\\n&quot;;const char* client_key = &quot;-----BEGIN PRIVATE KEY-----\\nYOUR_CLIENT_KEY_HERE\\n-----END PRIVATE KEY-----\\n&quot;;WiFiClientSecure espClient;espClient.setCACert(ca_cert);espClient.setCertificate(client_cert); // Optional, for mutual TLSespClient.setPrivateKey(client_key); // Optional, for mutual TLS During the TLS handshake: The ESP32 verifies Mosquitto’s server.crt using ca_cert. If mutual TLS is enabled, the ESP32 sends client_cert to Mosquitto, and Mosquitto verifies it using its cafile. 5. Why Different Files Are NeededEach file serves a specific role in the TLS handshake and PKI: CA certificate (cafile): Establishes trust. Without it, the client cannot verify the server’s identity, making the connection vulnerable to impersonation. Server certificate (certfile): Proves the server’s identity to the client. It’s the public part of the server’s identity. Server private key (keyfile): Enables the server to authenticate itself and perform cryptographic operations. It’s the secret part that must be protected. Client certificate and key (optional): Used for mutual TLS to authenticate the ESP32 to the server, adding an extra layer of security.The separation of these files ensures: Security: The private key is kept secret and never shared. Modularity: Certificates can be updated or replaced without affecting the private key. Trust: The CA certificate acts as a trusted anchor, linking all certificates in the chain.6. Example Workflow: Generating and Using CertificatesHere’s a practical example of generating certificates and configuring Mosquitto/ESP32:a. Generate CertificatesUsing OpenSSL:# Generate CA certificate and keyopenssl req -new -x509 -days 365 -extensions v3_ca -keyout ca.key -out ca.crt# Generate server key and certificateopenssl genrsa -out server.key 2048openssl req -new -out server.csr -key server.keyopenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365# Generate client key and certificate (for mutual TLS)openssl genrsa -out client.key 2048openssl req -new -out client.csr -key client.keyopenssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365b. Configure MosquittoAdd to mosquitto.conf:listener 8883cafile /path/to/ca.crtcertfile /path/to/server.crtkeyfile /path/to/server.keyrequire_certificate true # For mutual TLSc. Configure ESP32Embed certificates in the firmware (Arduino example):const char* ca_cert = R&quot;(-----BEGIN CERTIFICATE-----YOUR_CA_CERT_HERE-----END CERTIFICATE-----)&quot;;const char* client_cert = R&quot;(-----BEGIN CERTIFICATE-----YOUR_CLIENT_CERT_HERE-----END CERTIFICATE-----)&quot;;const char* client_key = R&quot;(-----BEGIN PRIVATE KEY-----YOUR_CLIENT_KEY_HERE-----END PRIVATE KEY-----)&quot;;WiFiClientSecure espClient;espClient.setCACert(ca_cert);espClient.setCertificate(client_cert); // For mutual TLSespClient.setPrivateKey(client_key); // For mutual TLSd. Test the Connection Start Mosquitto: sudo systemctl restart mosquitto Run the ESP32 code and check for successful connection. Use mosquitto_sub to test: mosquitto_sub -h &amp;lt;broker_ip&amp;gt; -p 8883 --cafile ca.crt --cert client.crt --key client.key -t &quot;test/topic&quot; 7. Common Issues and Troubleshooting Certificate Verification Failure: Ensure the CA certificate matches the one used to sign the server/client certificates. Check the certificate’s validity period (not expired). Verify the ESP32 has the correct system time (use NTP for TLS). File Format Issues: Confirm files are in PEM format (Mosquitto typically expects PEM). Use openssl x509 -in file.crt -text -noout to inspect certificates. Permission Errors: Ensure Mosquitto has read permissions for cafile, certfile, and keyfile. Set secure permissions: chmod 600 server.key. Mutual TLS Issues: Verify require_certificate true in Mosquitto if using client certificates. Ensure the client certificate is signed by the same CA as the server’s certificate. 8. Best Practices Use Trusted CAs for Production: Self-signed certificates are fine for testing but vulnerable to impersonation in production. Use Let’s Encrypt or another trusted CA. Secure Private Keys: Store keyfile and client keys securely (e.g., restrict permissions, use encrypted storage on ESP32). Rotate Certificates: Update certificates before they expire to avoid downtime. Minimize CA Scope: Use a dedicated CA for your MQTT setup to limit the impact of a compromised CA. Enable Mutual TLS: For high-security applications, use client certificates to authenticate ESP32 devices." }, { "title": "Functional and application compoenents - Qualtrics (G)", "url": "/posts/fuctional-and-application-component/", "categories": "Design", "tags": "qualtrics", "date": "2025-04-30 08:35:00 -0400", "snippet": "Decomposing SaaS Functional and application compoenentsApplication componentsDecomposing a Software-as-a-Service (SaaS) platform like Qualtrics into its application components and functional components involves breaking down its architecture and capabilities into modular, logical units. Qualtrics is a complex experience management platform that provides survey creation, distribution, data collection, and analytics, with a robust API and management features. Below, I’ll outline how to decompose Qualtrics into application components (the technical building blocks of the system) and functional components (the user-facing features and capabilities), based on its known functionalities and typical SaaS architecture patterns.1. Application ComponentsApplication components are the technical, infrastructural, and software elements that make up the Qualtrics platform. These are the backend and frontend systems, APIs, databases, and other architectural layers that enable the platform to function. Below is a decomposition of Qualtrics into application components, categorized by their role in the system:1.1 Frontend Layer User Interface (UI) Components: Survey Builder Interface: A drag-and-drop web interface for designing surveys with question types (e.g., multiple-choice, NPS, matrix tables). Built with technologies like HTML, CSS, JavaScript, and frameworks like React or Angular. Dashboard and Reporting Interface: Interactive dashboards for visualizing survey results, with customizable charts (over 30 graph types) and real-time analytics. Admin/Management Interface: A portal for managing users, permissions, contact lists, and organizational settings. Single Page Application (SPA) Support: Qualtrics supports SPAs for seamless navigation without page reloads, using JavaScript APIs for dynamic content loading. Mobile and Offline Interfaces: Mobile apps (iOS/Android) for survey distribution and response collection, with offline survey capabilities. Responsive web interfaces for cross-device compatibility. 1.2 Backend Layer Core Application Servers: Survey Engine: Manages survey logic, branching, quotas, and randomization. Processes survey responses and ensures data integrity. Experience Management (XM) Engine: Combines experience data (X-data) and operational data (O-data) for predictive insights and recommendations. Workflow Automation Engine: Handles automated triggers, such as sending surveys after events (e.g., a closed support ticket) or distributing reports. API Gateway: Qualtrics API (v3): A RESTful API that exposes endpoints for survey management, response retrieval, contact list management, and distribution (e.g., Create Distribution, Get Survey Responses). Supports JSON, authentication via API tokens, and integrations with CRM systems like Salesforce. Event Subscription Service: Webhooks for real-time response notifications (e.g., surveyengine.completedResponse). Integration Layer: Pre-built connectors for Salesforce, SAP, Zendesk, and Tableau to sync survey data with external systems. Web services for automating API requests within surveys (e.g., Web Service element in Survey Flow). 1.3 Data Layer Database Systems: Survey Metadata Store: Stores survey structures, questions, and logic (e.g., SQL or NoSQL databases like PostgreSQL or MongoDB). Response Data Store: Stores raw survey responses, potentially in a data warehouse (e.g., Snowflake) for scalability and analytics. XM Directory Database: Manages contact lists, mailing lists, and respondent metadata (e.g., Directory ID, Contact ID). Data Processing and Analytics: Stats iQ and Crosstabs iQ: Statistical analysis tools for correlations, regressions, and cross-tabulations. AI-Driven Analytics: Generative AI for summarizing video feedback and predictive insights. Data Export/Import Services: Supports formats like CSV, Excel, SPSS, and JSON for data interoperability. 1.4 Infrastructure Layer Cloud Infrastructure: Hosted on a multi-tenant cloud architecture (likely AWS, Azure, or GCP) for scalability and global availability. Load balancers and content delivery networks (CDNs) for high availability and low latency. Security and Compliance: Authentication/Authorization: Single Sign-On (SSO), IP allowlisting, and mTLS for secure API access. Data encryption (at rest and in transit) and compliance with GDPR, HIPAA, and other regulations. Scalability and Performance: Auto-scaling microservices for handling peak survey response loads. Caching layers (e.g., Redis) for frequently accessed survey templates or dashboards. 1.5 External Interfaces Distribution Channels: Email and SMS modules for survey distribution, with two-way SMS capabilities in the U.S. and U.K. Website/App Feedback Intercepts: JavaScript-based embeds for collecting feedback on websites or SPAs. Third-Party Integrations: Connectors for SAP Analytics Cloud, Salesforce, and Tableau for data visualization and workflow automation. Functional Components2. Functional ComponentsFunctional components represent the user-facing features and capabilities of Qualtrics, aligned with its role as an experience management and survey platform. These are the services and tools that users interact with to achieve their goals. Below is a decomposition of Qualtrics’ functional components:2.1 Survey Creation and Design Question Types and Logic: Supports 23 question types, including NPS, MaxDiff, matrix tables, and video/audio responses. Advanced logic features like branching, randomization, and quotas for personalized surveys. Survey Templates: Pre-built templates for employee engagement, customer satisfaction, and market research (e.g., MaxDiff analysis for preference ranking). Customization: Custom branding, CSS, JavaScript, and HTML for tailored survey appearances. Multilingual surveys for global reach. 2.2 Survey Distribution Distribution Channels: Email, SMS, QR codes, social media, website embeds, and chatbots for reaching respondents. Offline survey collection via mobile apps. Contact Management: XM Directory: Manages contact lists, mailing lists, and incentives (e.g., points, sweepstakes). Supports API requests like List Directory Contacts and getBouncedContacts. Automated contact import and segmentation for targeted distributions. Personalized Links: Individual survey links for tracking respondent data without Qualtrics mailer, using API requests like Create Distribution and Distribution Links. 2.3 Data Collection and Response Management Real-Time Response Collection: Captures responses via web, mobile, or offline channels, with real-time API access (e.g., Get Survey Responses). Webhooks for instant notifications on survey completion. Response Validation: Ensures data quality with features like screen-out management and fraud detection. Geo-Location and Metadata: Captures location data (e.g., LocationLatitude, LocationLongitude) for geospatial analysis. 2.4 Analytics and Reporting Real-Time Dashboards: Centralized dashboards for visualizing trends, ratings, and open-text analysis. Role-based dashboards for different stakeholders (e.g., managers, analysts). Advanced Analytics: Stats iQ: Statistical tools for correlations and predictive modeling. Text iQ: AI-powered text analysis for open-ended responses. MaxDiff Analysis: Ranks preferences for product features or messaging. Data Export and Sharing: Exports to CSV, Excel, SPSS, or Tableau for external analysis. Automated report distribution to stakeholders. 2.5 Management Platform User and Permission Management: Role-based access control (RBAC) for administrators to manage users, groups, and permissions. Organizational settings for contact info, password policies, and logout redirects. Workflow Automation: Triggers for automated actions (e.g., sending surveys after milestones, notifying teams of negative feedback). Integration with CRMs like Salesforce for automated feedback loops. Project Management: Tools for creating and managing multiple survey projects, with options for manual or automated deployment. 2.6 API and Integrations API Capabilities: Manage surveys, responses, and contacts programmatically (e.g., Create Response Export, Import Survey). Real-time data access and automation via webhooks and event subscriptions. Third-Party Integrations: Seamless data flow with Salesforce, SAP, Zendesk, and Tableau for enhanced analytics and workflows. Custom integrations using Qualtrics API for bespoke business processes. 2.7 AI and Predictive Insights Generative AI: Summarizes video feedback and provides actionable recommendations. Predictive Analytics: Combines X-data and O-data to predict customer/employee behavior and suggest interventions. Personalization: AI-driven customization for tailored survey experiences and insights. 3. Mapping Application and Functional ComponentsTo clarify how these components interact, here’s a mapping of how application components support functional components: Functional Component Supporting Application Components Survey Creation and Design Survey Builder UI, Survey Engine, Survey Metadata Store Survey Distribution Distribution Channels (Email/SMS), API Gateway, XM Directory Database, Integration Layer Data Collection Survey Engine, Response Data Store, API Gateway (Webhooks), Geo-Location Services Analytics and Reporting Dashboard UI, Stats iQ/Crosstabs iQ, Data Processing/Analytics, Data Export Services Management Platform Admin Interface, Workflow Automation Engine, User/Permission Database, Integration Layer API and Integrations API Gateway, Integration Layer, Event Subscription Service AI and Predictive Insights AI-Driven Analytics, XM Engine, Data Processing Layer 4. Recent Developments (as of April 2025)Based on recent information: New Features: Qualtrics has introduced Stats iQ, Crosstabs iQ, and Screen-Out Management for enhanced analytics and data quality. XM Directory Lite: Migration of the “Contacts” database to XM Directory Lite, improving contact management scalability. AI Enhancements: Increased adoption of generative AI for video feedback summarization and predictive insights, aligning with SaaS trends. API Improvements: Enhanced security with IP allowlisting and mTLS for API access, ensuring robust integrations.5. Notes on Decomposition Multi-Tenant Architecture: Qualtrics operates on a multi-tenant SaaS model, where a single instance serves multiple customers, with data isolation ensured through tenant-specific configurations. Customization: Functional components like survey design and analytics are highly customizable, but the underlying application components (e.g., Survey Engine, API Gateway) are standardized for scalability. Scalability: The platform’s cloud infrastructure and microservices architecture allow it to handle large-scale survey deployments and response volumes. Critical Thinking on Sources: While Qualtrics’ documentation provides detailed insights into its features, some technical details (e.g., specific database technologies) are inferred based on industry standards, as Qualtrics does not publicly disclose its full tech stack. Always validate such assumptions with primary sources or Qualtrics’ technical support.6. Recommendations for Further AnalysisTo deepen this decomposition: Access Qualtrics API Documentation: Explore the full API reference at api.qualtrics.com for detailed endpoint capabilities. Engage with Qualtrics Community: The Qualtrics Community forum offers insights into advanced features and custom integrations. Prototype with Free Account: Use a free Qualtrics account to experiment with survey creation, distribution, and API calls to validate component interactions. Consult Qualtrics Support: For proprietary details (e.g., infrastructure specifics), contact Qualtrics support or an account executive.If you have specific aspects of Qualtrics you’d like to explore further (e.g., a particular API endpoint, analytics feature, or integration), let me know, and I can dive deeper!———–+++++++mTLSMutual Transport Layer Security (mTLS) is a security protocol that ensures bidirectional authentication between a client and a server during API communication. Unlike traditional TLS, which only requires the server to authenticate itself to the client, mTLS requires both parties to present and verify X.509 certificates, ensuring that only trusted clients can access the API. This is particularly valuable for business-to-business (B2B) applications, Internet of Things (IoT) devices, and sensitive SaaS platforms like Qualtrics, where secure API access is critical.Qualtrics supports mTLS as an advanced security option for its API, alongside features like IP allowlisting, to enhance the protection of data exchanged via its RESTful API (v3). Below, I provide a detailed explanation of mTLS for API access in the context of Qualtrics, followed by a step-by-step guide to set it up, based on available information and general best practices for mTLS implementation. Since Qualtrics’ documentation on mTLS setup is sparse, I’ll supplement with industry-standard approaches and note where Qualtrics-specific steps may require further inquiry.Understanding mTLS for Qualtrics API AccessWhat is mTLS in the Context of Qualtrics? Purpose: mTLS ensures that only authorized clients (e.g., applications or services) can make API requests to Qualtrics’ endpoints (e.g., https://yourdatacenterid.qualtrics.com/API/v3/surveys). This is critical for protecting sensitive survey data, contact lists, and analytics. Mechanism: During the TLS handshake: The Qualtrics server presents its certificate to the client, proving its identity. The client presents its own X units (certificate) to the Qualtrics server, which verifies it against a trusted Certificate Authority (CA). Both certificates must be valid, signed by trusted CAs, and not revoked. Use Cases: mTLS is ideal for scenarios where Qualtrics API is integrated with enterprise systems (e.g., Salesforce, SAP) or IoT devices, ensuring that only pre-authorized clients can access or modify data. Qualtrics-Specific Context: Qualtrics mentions mTLS as an advanced security option on its API documentation site (api.qualtrics.com). It complements other security measures like API token authentication and IP allowlisting, providing an additional layer of client authentication.Benefits of mTLS for Qualtrics API Enhanced Security: Verifies the identity of both client and server, reducing the risk of unauthorized access or man-in-the-middle attacks. Granular Access Control: Allows Qualtrics administrators to restrict API access to specific clients with valid certificates. Compliance: Supports compliance with regulations like GDPR, HIPAA, and FedRAMP, which Qualtrics adheres to, by ensuring secure data exchange. No Dependency on Tokens Alone: While Qualtrics API typically uses API tokens for authentication, mTLS adds a certificate-based layer, reducing reliance on token security.Challenges Complexity: Setting up mTLS requires managing certificates, private keys, and trust stores, which can be complex for non-technical users. Certificate Management: Certificates must be issued, renewed, and revoked appropriately to maintain security. Qualtrics-Specific Limitations: Qualtrics does not provide detailed public documentation on mTLS setup, so some steps may require direct support from Qualtrics or a Brand Administrator.How to Set Up mTLS for Qualtrics API AccessSetting up mTLS for Qualtrics API access involves configuring both the client-side (your application or service) and server-side (Qualtrics’ API gateway) to exchange and verify certificates. Below is a step-by-step guide, combining Qualtrics-specific information with general mTLS best practices. Note that some steps may require coordination with Qualtrics support, as mTLS configuration details are not fully documented publicly.Step 1: Verify API Access and mTLS Support Check Qualtrics Account Permissions: Ensure your Qualtrics account has API access enabled. Free trial accounts typically do not include API access, and mTLS is an advanced feature likely restricted to paid plans (e.g., enterprise licenses). Contact your Qualtrics Brand Administrator or sales representative to confirm that your license includes mTLS support. Access Qualtrics API Documentation: Visit https://api.qualtrics.com for the latest API reference, including security options like mTLS. Review the authentication section to understand how mTLS integrates with API token-based authentication. Step 2: Obtain or Generate Certificates Server Certificate (Qualtrics Side): Qualtrics’ API server (https://yourdatacenterid.qualtrics.com/API) already has a valid TLS certificate issued by a public CA. You don’t need to manage this certificate, as Qualtrics handles it. Client Certificate (Your Side): Option 1: Use a Public CA: Request a client certificate from a trusted public CA (e.g., DigiCert, Let’s Encrypt, or AWS Certificate Manager). The certificate should include your organization’s details (e.g., Common Name, Subject Alternative Name) and be compatible with Qualtrics’ trust store. Option 2: Use a Private CA: Create a private CA using tools like OpenSSL or AWS Private Certificate Authority. Generate a client certificate signed by this CA. Example OpenSSL commands: # Create a private CA key and certificateopenssl genrsa -out ca.key 2048openssl req -x509 -new -nodes -key ca.key -sha256 -days 365 -out ca.crt# Generate client key and certificate signing request (CSR)openssl genrsa -out client.key 2048openssl req -new -key client.key -out client.csr# Sign the client certificate with the CAopenssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365 -sha256 Store the client certificate (client.crt), private key (client.key), and CA certificate (ca.crt) securely. Option 3: Qualtrics-Managed CA: Qualtrics may provide a managed CA for issuing client certificates, especially for enterprise accounts. Contact Qualtrics support to inquire about this option. Trust Store Requirements: Qualtrics’ API gateway must trust the CA that issued your client certificate. If using a private CA, you’ll need to provide the CA certificate to Qualtrics (see Step 3). Conversely, your client must trust the CA that issued Qualtrics’ server certificate (typically a public CA, so this is usually automatic). Step 3: Configure Qualtrics API Gateway for mTLS Submit CA Certificate to Qualtrics: If you’re using a private CA, you must add its certificate to Qualtrics’ trust store. This tells Qualtrics to accept client certificates signed by your CA. Contact Qualtrics support or your Brand Administrator to upload the CA certificate (ca.crt). This step is not publicly documented, so direct communication with Qualtrics is necessary. Enable mTLS for Your API: Qualtrics’ API documentation mentions mTLS as an advanced security option, but specific configuration steps are not detailed. Request Qualtrics to enable mTLS for your API endpoints. You may need to specify: The API base URL (e.g., https://yourorganizationid.yourdatacenterid.qualtrics.com/API). Optional constraints, such as requiring specific values in the client certificate’s Subject Alternative Name (SAN) or Common Name (e.g., example.com). Qualtrics may configure its API gateway to reject requests from clients without valid certificates. Verify Configuration: Qualtrics should confirm that mTLS is enabled and provide any additional endpoint-specific settings (e.g., custom domain requirements). Step 4: Configure Your Client Application Install Client Certificate and Key: Store the client certificate (client.crt) and private key (client.key) on the client system (e.g., your application server or IoT device). Ensure the private key is protected (e.g., restricted file permissions, encrypted storage). Update API Client Code: Modify your API client to include the client certificate and key when making HTTPS requests to Qualtrics. Below are examples for common tools: cURL: curl --cert client.crt --key client.key \\ -H &quot;X-API-TOKEN: your_api_token&quot; \\ https://yourdatacenterid.qualtrics.com/API/v3/surveys Use --cacert ca.crt if Qualtrics’ server certificate is signed by a private CA (unlikely). Python (requests library): import requestsurl = &quot;https://yourdatacenterid.qualtrics.com/API/v3/surveys&quot;headers = {&quot;X-API-TOKEN&quot;: &quot;your_api_token&quot;}cert = (&quot;client.crt&quot;, &quot;client.key&quot;)response = requests.get(url, headers=headers, cert=cert, verify=True)print(response.json()) Postman: Go to Settings &amp;gt; Certificates &amp;gt; Add Certificate. Enter the Qualtrics API hostname (e.g., yourdatacenterid.qualtrics.com). Upload client.crt and client.key. Add the API token in the Headers tab (X-API-TOKEN). Handle Certificate Validation: Ensure your client validates Qualtrics’ server certificate (default behavior in most HTTPS clients). If Qualtrics requires specific SAN or Common Name values in your client certificate, ensure they match your certificate’s configuration. Step 5: Test the mTLS Configuration Make a Test API Request: Use a tool like cURL or Postman to send a test request to a Qualtrics API endpoint (e.g., GET /v3/surveys). Example: curl --cert client.crt --key client.key \\ -H &quot;X-API-TOKEN: your_api_token&quot; \\ https://yourdatacenterid.qualtrics.com/API/v3/surveys Expected response: A 200 OK status with JSON data if mTLS and API token authentication succeed. Common errors: 403 Forbidden: Invalid or untrusted client certificate. 401 Unauthorized: Missing or invalid API token. 400 Bad Request: Malformed request or incorrect endpoint. Test Without Client Certificate: Try the same request without the --cert and --key options: curl -H &quot;X-API-TOKEN: your_api_token&quot; \\ https://yourdatacenterid.qualtrics.com/API/v3/surveys Expected response: A 403 Forbidden or similar error, confirming that Qualtrics requires a client certificate for mTLS. Debugging: Check Qualtrics’ API response for error details. Use OpenSSL to verify certificate details: openssl x509 -in client.crt -text -noout If issues persist, contact Qualtrics support with your certificate’s public details (not the private key) and error messages. Step 6: Manage Certificates and Monitor Certificate Renewal: Client certificates typically expire after 1–3 years. Set up a process to renew certificates before expiry to avoid disruptions. Update the client certificate in your application and notify Qualtrics if the CA changes. Certificate Revocation: If a client certificate is compromised, revoke it using your CA’s Certificate Revocation List (CRL) or Online Certificate Status Protocol (OCSP). Inform Qualtrics to update its trust store if necessary. Qualtrics may not natively support CRL/OCSP, so confirm with support. Monitoring: Monitor API logs for failed authentication attempts, which could indicate misconfigured certificates or unauthorized access attempts. Use Qualtrics’ dashboard or API response logs to track successful requests. Step 7: Integrate with Qualtrics API Token Authentication Combine mTLS with API Token: Qualtrics API requires an X-API-TOKEN header for authentication, even with mTLS. Generate an API token in Qualtrics: Log in to Qualtrics. Go to Account Settings &amp;gt; Qualtrics IDs. Click “Generate Token” under the API section. Include the token in all API requests (see examples in Step 4). Security Best Practice: Store the API token securely (e.g., in a vault like AWS Secrets Manager or Azure Key Vault). Rotate tokens periodically and regenerate if compromised. Step 8: Optional – Use a Custom Domain (if Required) Custom Domain for mTLS: Qualtrics may require a custom domain for mTLS-enabled APIs, especially for enterprise accounts. This aligns with practices seen in other platforms like AWS API Gateway. If required, set up a custom domain (e.g., api.yourorganization.com) and associate it with Qualtrics’ API endpoint. Configure DNS and upload a server certificate for the custom domain using a service like AWS Certificate Manager (ACM). Disable Default Endpoint: To enforce mTLS, disable Qualtrics’ default execute-api endpoint and route all traffic through the custom domain. Confirm with Qualtrics support if this is applicable. Qualtrics-Specific Notes Limited Documentation: Qualtrics’ public documentation briefly mentions mTLS but lacks detailed setup instructions. Most configuration details (e.g., trust store management, mTLS enablement) require direct interaction with Qualtrics support or a Brand Administrator. Enterprise Feature: mTLS is likely restricted to enterprise or advanced plans. Verify with Qualtrics sales if your account supports it. Integration with Existing Security: mTLS complements Qualtrics’ existing security features, such as IP allowlisting (restricting API access to specific IP ranges) and SSO for user authentication. If using IP allowlisting, ensure your client’s IP is allowlisted in Qualtrics’ settings to avoid conflicts with mTLS. Support Channels: Contact Qualtrics support via the XM Community (community.qualtrics.com) or your account manager for mTLS-specific guidance. Use the Qualtrics API Postman collection (api.qualtrics.com) to test mTLS-enabled requests. General Best Practices for mTLS Secure Private Keys: Store client private keys in a hardware security module (HSM) or encrypted storage to prevent unauthorized access. Validate Certificate Properties: Configure Qualtrics to check specific certificate fields (e.g., SAN, Common Name) for additional security. Avoid overly broad trust stores; only include necessary CAs. Automate Certificate Management: Use tools like Certbot or AWS ACM for automated certificate issuance and renewal. Implement monitoring for certificate expiry using scripts or services like AWS Lambda. Test Thoroughly: Test mTLS with multiple clients to ensure compatibility. Simulate certificate revocation or invalid certificates to verify Qualtrics’ rejection behavior. Fallback Authentication: If mTLS setup fails, Qualtrics API requests can still use API tokens and IP allowlisting as a fallback, but this is less secure. Ensure mTLS is fully operational before relying on it exclusively. Troubleshooting Common Issues “403 Forbidden” Error: Cause: Invalid or untrusted client certificate. Solution: Verify the certificate is signed by a CA trusted by Qualtrics. Check with Qualtrics support to ensure the CA certificate is in their trust store. “401 Unauthorized” Error: Cause: Missing or invalid API token. Solution: Ensure the X-API-TOKEN header is included and the token is valid. Regenerate the token if necessary. “400 Bad Request” Error: Cause: Malformed request or incorrect endpoint. Solution: Validate the API endpoint and request format using Qualtrics’ API documentation or Postman collection. Certificate Validation Failure: Cause: Mismatch in certificate details (e.g., SAN, Common Name). Solution: Update the certificate to match Qualtrics’ requirements or adjust Qualtrics’ validation rules via support. Connection Issues: Cause: Network or firewall blocking mTLS handshake. Solution: Ensure your client’s IP is allowlisted (if Qualtrics uses IP restrictions) and that port 443 is open for HTTPS traffic. Critical Thinking on Sources Qualtrics Documentation: The mention of mTLS in Qualtrics’ API documentation is brief, indicating it’s an enterprise-grade feature. The lack of detailed setup instructions suggests that Qualtrics expects customers to work closely with their support team for implementation. General mTLS Practices: Sources like AWS, Cloudflare, and HAProxy provide robust mTLS setup guidelines, which I’ve adapted to fit Qualtrics’ context. However, Qualtrics’ specific requirements (e.g., trust store management, custom domain usage) may differ, so assumptions about standard practices should be validated with Qualtrics. Community Insights: The Qualtrics XM Community and Stack Overflow discussions highlight common API authentication issues but lack mTLS-specific details, reinforcing the need for direct Qualtrics support.Recommendations for Further Assistance Contact Qualtrics Support: Reach out to Qualtrics via community.qualtrics.com or your account manager to request: Confirmation of mTLS availability for your account. Instructions for uploading your CA certificate to Qualtrics’ trust store. Details on enabling mTLS for specific API endpoints. Experiment with Postman: Use Qualtrics’ public Postman collection (api.qualtrics.com) to test mTLS-enabled requests. Configure Postman’s certificate settings to include your client certificate and key. Consult Security Experts: If managing certificates or mTLS is complex, engage an IT specialist or security consultant familiar with Qualtrics’ API and mTLS implementations. Monitor Qualtrics Updates: Qualtrics may enhance mTLS documentation or features in future releases. Check api.qualtrics.com and the XM Community for updates.ConclusionmTLS enhances the security of Qualtrics API access by requiring bidirectional certificate-based authentication, making it ideal for enterprise integrations and sensitive data exchanges. Setting it up involves obtaining client certificates, configuring Qualtrics’ API gateway to trust your CA, and updating your client application to present certificates during API requests. While Qualtrics’ documentation on mTLS is limited, combining general mTLS best practices with direct support from Qualtrics can ensure a successful implementation.If you need assistance with specific aspects (e.g., generating certificates, debugging API requests, or contacting Qualtrics), let me know, and I can provide further guidance or example code!" }, { "title": "Mosquitto with Docker Compose, Base Authentication, DuckDNS and Let&#39;s Encrypt (done)", "url": "/posts/mosquitto-tls-done-copy/", "categories": "DEV", "tags": "cli", "date": "2025-04-26 07:35:00 -0400", "snippet": "Base Authentication &amp;amp; Let’s Encrypt (DuckDNS)Dockerize Mosquitto with duckdns, base authentication and letsencrypt.Prerequisites A domain pointed to your droplet (via DuckDNS) Docker and Docker Compose installed Ports 80, 8883, 9001, and 1883 open in your firewallStep 1: Set UpBesmellah!Step 3: step 1 compose.yaml version 1run mosquitto on 1883 wihout base authenticationCreate a working directory:mkdir -p ~/mosquitto ~/mosquitto/data ~/mosquitto/logcd ~/mosquittovim mosquitto.confCreate mosquitto/mosquitto.conf:persistence truepersistence_location /mosquitto/data/log_dest file /mosquitto/log/mosquitto.log# Authenticationallow_anonymous true# MQTTlistener 1883protocol mqtt# Websocketslistener 9001protocol websocketsCreate mosquitto/compose.yaml:services: mosquitto: image: eclipse-mosquitto:2 container_name: mosquitto ports: - &quot;1883:1883&quot; - &quot;9001:9001&quot; volumes: - $PWD/mosquitto.conf:/mosquitto/config/mosquitto.conf - $PWD/data:/mosquitto/data - $PWD/log:/mosquitto/log environment: - TZ=UTC+5 restart: alwaystest mosquittodocker pscheck the logssudo cat log/mosquitto.logcheck into containerdocker exec -it mosquitto shcheck open portssudo ufw status4. Create Authentication Filecd ~docker run -it --rm -v $(pwd)/mosquitto:/mosquitto/config eclipse-mosquitto:2 mosquitto_passwd -c /mosquitto/config/passwd YOUR_USERNAMEReplace YOUR_USERNAME and Enter a strong password.How to validatepasswd file will be createdsudo cat passwd5. Set Up Let’s Encrypt SSL (Using Certbot)Install Certbot:sudo apt install certbot -yRun Certbot:sudo certbot certonly --standalone --preferred-challenges http --agree-tos --email YOUR_EMAIL -d YOUR_SUBDOMAIN.duckdns.orgMove certificates:mkdir -p ~/mosquitto/config/certssudo cp /etc/letsencrypt/live/YOUR_SUBDOMAIN.duckdns.org/fullchain.pem ~/mosquitto/config/certs/sudo cp /etc/letsencrypt/live/YOUR_SUBDOMAIN.duckdns.org/privkey.pem ~/mosquitto/config/certs/tested versionInstall Certbot:sudo apt updatesudo apt install snapdsudo snap install snap-storesudo snap install --classic certbotRun Certbot:sudo certbot certonly --standalone --preferred-challenges http --email YOUR_EMAIL -d mqttsisi.duckdns.orgTODO how to test …Move certificates:mkdir -p ~/mosquitto/certssudo cp /etc/letsencrypt/live/mqttsisi.duckdns.org/fullchain.pem ~/mosquitto/certs/sudo cp /etc/letsencrypt/live/mqttsisi.duckdns.org/privkey.pem ~/mosquitto/certs/sudo cp /etc/letsencrypt/live/mqttsisi.duckdns.org/cert.pem ~/mosquitto/certs/TODO how to test …6. Create Docker Compose FileGo back to your home directory and create docker-compose.yml:cd ~/mosquittovim compose.yamlPaste:services: mosquitto: image: eclipse-mosquitto:2 container_name: mosquitto ports: - &quot;1883:1883&quot; - &quot;8883:8883&quot; - &quot;9001:9001&quot; volumes: - $PWD/mosquitto.conf:/mosquitto/config/mosquitto.conf - $PWD/passwd:/mosquitto/config/passwd - $PWD/data:/mosquitto/data - $PWD/log:/mosquitto/log - $PWD/certs:/mosquitto/letsencrypt environment: - TZ=UTC+5 restart: alwaysSave and exit.7. Start MosquittoRun:docker-compose up -d8. Auto-Renew SSLCreate a script:sudo nano /etc/cron.daily/certbot-renewAdd:#!/bin/bashcertbot renew --quietcp /etc/letsencrypt/live/YOUR_SUBDOMAIN.duckdns.org/fullchain.pem ~/mosquitto/certs/cp /etc/letsencrypt/live/YOUR_SUBDOMAIN.duckdns.org/privkey.pem ~/mosquitto/certs/cp /etc/letsencrypt/live/mqttsisi.duckdns.org/cert.pem ~/mosquitto/certs/docker restart mosquittoSave and make it executable:#!/bin/bashsudo chmod +x /etc/cron.daily/certbot-renew" }, { "title": "IoT project Udemy ressources", "url": "/posts/iotProjectsUdemy/", "categories": "DEV, Linux", "tags": "home-server, dell3020, lenovoM900", "date": "2025-04-01 07:35:00 -0400", "snippet": "Ressources _ _ ______ _____ _____ (_) | | ____| __ \\_ _| _ __ ___ _____| | |__ | | | || | | &#39;_ \\| \\ \\/ / _ \\ | __| | | | || | | |_) | |&amp;gt; &amp;lt; __/ | |____| |__| || |_ | .__/|_/_/\\_\\___|_|______|_____/_____| | | |_| www.pixeledi.eu | https://linktr.ee/pixelediV1.0 | 04.2023Docker Installationhttps://docs.docker.com/engine/install/ubuntu/Docker CLI am Beispiel Focalboardhttps://hub.docker.comdocker run -d --name focalboard -p 8000:8000 mattermost/focalboarddocker psdocker stop focalboarddocker start focalboarddocker container lsdocker image ls//alle gestoppten container entfernendocker container prunePortainerDoku: https://docs.portainer.io/start/install/server/docker/linuxdocker run -d -p 9000:9000 -p 9443:9443 --name portainer \\--restart=always -v /var/run/docker.sock:/var/run/docker.sock \\-v portainer_data:/data portainer/portainer-ce:latestMosquitto MQTT Brokerhttps://hub.docker.com/_/eclipse-mosquitto/ mkdir mosquittosudo chown -R 1883:1883 mosquitto cd mosquitto nano mosquitto.confallow_anonymous truelistener 1883 0.0.0.0persistence truepersistence_location /mosquitto/data/log_dest file /mosquitto/log/mosquitto.logFirewall Port: ufw allow 1883docker run -d \\ -p 1883:1883 \\ -p 9001:9001 \\ -v $PWD/mosquitto.conf:/mosquitto/config/mosquitto.conf \\ -v $PWD/mosquitto_data:/mosquitto/data \\ -v $PWD/mosquitto_log:/mosquitto/log \\ -v $PWD/config:/mosquitto/config \\ --restart=always \\ --name=mosquitto-server \\ eclipse-mosquitto:latestohmyzhttps://ohmyz.sh/#installmosquitto mit passwordschutzhttps://mosquitto.org/man/mosquitto_passwd-1.htmlConfigallow_anonymous falsepassword_file /mosquitto/config/mosquitto.passwdlistener 1883 0.0.0.0persistence truepersistence_location /mosquitto/data/log_dest file /mosquitto/log/mosquitto.logFor password generation we need a running Mosquitto container. The password is copied and then saved in the mosquitto.passwd file. Then the container is loaded WITH this file. docker exec -it mosquitto-server sh mosquitto_passwd -b -c passwd pixeledi mysecretpassword cat passwd copy strg+d exit shell Die obige Ausgabe wird nun in das File: “mosquitto.passwd” gespeichert: pixeledi:$7$101$y1og8rkgTbXKpIAv$E3ZYe3VxzM9PtjTxksIqQIwHq6mQ3INmXdrc/YK6SFwYXUtJTB3MXb7OFZklCKrZJrBYGLEoZFIdPmszqKN3yw== kill existing Container docker stop mosquitto-server docker container pruneWe need to add a line in the Docker run to do this-v $PWD/mosquitto.passwd:/mosquitto/config/mosquitto.passwd \\in mosquitto_conf we have to change -&amp;gt; allow_anonymous falseNew Docker run commanddocker run -it -d \\ -p 1883:1883 \\ -p 9001:9001 \\ -v $PWD/mosquitto.conf:/mosquitto/config/mosquitto.conf \\ -v $PWD/mosquitto_data:/mosquitto/data \\ -v $PWD/mosquitto_log:/mosquitto/log \\ -v $PWD/mosquitto.passwd:/mosquitto/config/mosquitto.passwd \\ --restart=always \\ --name=mosquitto-server \\ eclipse-mosquitto:latest docker exec -it mosquitto-server sh mosquitto_pub -h localhost -t test -m “test” -u “pixeledi” -P “mysecretpassword” l채sst man die Parameter -u und -P weg kommt eine Fehlermeldung. Somit haben wir die Benutzerabfrage eingef체gtNode-REDQuelle: https://nodered.org/docs/getting-started/dockerdocker volume create nodered_datamkdir noderedchown -R 1000:1000 noderedcd noderedmkdir nodered_datachown -R 1000:1000 nodered_datacd nodered_dataf체r zusatzbeispielchown -R 1000:1000 ocrdocker run -d \\ -p 1880:1880 \\ -v $PWD/nodered_data:/data \\ --name=nodered \\ --restart=always \\ nodered/node-red:latestConnection in Node-RED with Mosquitto: http://116.203.190.120 (URL von Server) Port 1883Node-RED mit Passwortschutz Container must be running docker exec -it nodered sh npx node-red admin hash-pw mysecretpassword 2x copy: $2b$08$DOAyahPESu3Pg6w5O5q0Qurzv74cujz1amgZIs9u2xydMGhFG9RIu exit oder strg+d cd nodered_data nano settings.js strg+w and search for adminauth delete the “//” comments inside the nodered folder: cp nodered_data/settings.js settings.js docker stop nodered docker container prune adminAuth: { type: &quot;credentials&quot;, users: [{ username: &quot;admin&quot;, password: &quot;$2b$08$DOAyahPESu3Pg6w5O5q0Qurzv74cujz1amgZIs9u2xydMGhFG9RIu&quot;, permissions: &quot;*&quot; }] },docker run -d \\ -p 1880:1880 \\ -v $PWD/nodered_data:/data \\ -v $PWD/settings.js:/data/settings.js \\ -e &quot;TZ=Europe/Brussels&quot; \\ --name=nodered \\ --restart=always \\ nodered/node-red:latestInfluxdbQuelle: https://hub.docker.com/_/influxdb/mkdir influxdbcd influxdbdocker volume create influxdb docker run -d \\ -p 8086:8086 \\ -v $PWD/influxdb:/var/lib/influxdb2 \\ --name=influxdb \\ --restart=always \\ influxdb:latestGrafanaQuelle: https://hub.docker.com/r/grafana/grafana/First login: admin adminSettings Datasource: Flux nicht localhost sondern http://ip:8086 basic auth off organization in influxdb neuen API Key erstellenhttps://docs.influxdata.com/influxdb/v2.5/tools/grafana/?t=InfluxQLdocker volume create grafana docker run -d \\ -p 3000:3000 \\ --name=grafana \\ --restart=always \\ grafana/grafana:latestMariaDBQulle: https://mariadb.com/resources/blog/get-started-with-mariadb-using-docker-in-3-steps/docker run -d \\ -p 3306:3306 \\ --name mdb \\ -e MARIADB_ROOT_PASSWORD=mysecretpassword \\ -d mariadb:latestMariaDB CLI im ContainerThe parameter “-p” is passed directly with the password without spaces.docker exec -it mdb mariadb --user root -pmysecretpasswordMariaDB [(none)]&amp;gt; CREATE DATABASE noderedkurs;AdminerQuelle: https://www.adminer.org/–link to the –name from MariaDB containerdocker run -d \\ --link mdb:db \\ -p 8080:8080 \\ --name adminer \\ --restart=always \\ adminer:latestDocker Container updaten using Node-RED as an exampleSince we have created a volume before, the data is preserved and the version of Node-RED can be updated.docker pull nodered/node-reddocker stop nodereddocker rm nodered$ docker run ....Traefikdocker-compose.ymlversion: &#39;3.4&#39;services: traefik: container_name: traefik restart: unless-stopped image: traefik networks: - iotnetwork ports: - &quot;80:80&quot; - &quot;443:443&quot; volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - ./traefik.yml:/traefik.yml - ./traefik_api.yml:/traefik_api.yml - ./acme.json:/cert/acme.jsonnetworks: iotnetwork: external: truetraefik_api.yml at first we create a password with httpd container docker run –rm httpd htpasswd -nb admin meinpassworthttp: middlewares: simpleAuth: basicAuth: users: - &#39;admin:$apr1$vLPlXwt8$soFFkjXwFDgL3OlHPJb8G.&#39; routers: api: rule: Host(`traefikapp.duckdns.org`) entrypoints: - websecure middlewares: - simpleAuth service: api@internal tls: certResolver: lets-encrypttraefik.ymlentryPoints: iotnetwork: address: &#39;:80&#39; http: redirections: entryPoint: to: websecure scheme: https websecure: address: &#39;:443&#39;api: dashboard: truecertificatesResolvers: lets-encrypt: acme: email: iotnetwork@gmail.com storage: acme.json tlsChallenge: {}providers: docker: watch: true network: iotnetwork file: filename: traefik_api.ymlDocker-compose mit Node-RED und Traefik labelsversion: &quot;3.7&quot;services: node-red: image: nodered/node-red:latest environment: - TZ=Europe/Amsterdam ports: - &quot;1880:1880&quot; networks: - node-red-net - iotnetwork volumes: - &quot;./nodered_data:/data&quot; labels: - traefik.enable=true - traefik.http.routers.node-red.rule=Host(`noderedubuntu.duckdns.org`) - traefik.http.routers.node-red.tls=true - traefik.http.routers.node-red.tls.certresolver=lets-encrypt - traefik.http.services.node-red.loadbalancer.server.port=1880volumes: nodered-data:networks: node-red-net: iotnetwork: external: truecrontabnano /etc/crontab* * * * * root echo &quot;mein erster Cronjob \\n&quot; &amp;gt;&amp;gt; /usr/local/sbin/cron.txt" }, { "title": "Set up and configure home server", "url": "/posts/homeServer/", "categories": "DEV, Linux", "tags": "home-server, dell3020, lenovoM900", "date": "2025-04-01 07:35:00 -0400", "snippet": "Lenovo ThinkCentre M900 TinyThe Lenovo ThinkCentre M900 Tiny is a solid candidate for a compact Ubuntu home server, and its 8GB RAM can be upgraded to boost performance. Below, I’ll address its suitability as an Ubuntu home server, RAM upgrade options, and other considerations based on available information.Suitability as an Ubuntu Home ServerThe ThinkCentre M900 Tiny is a micro form factor PC with a 6th-generation Intel Core processor (typically an i5-6500T or similar, 4 cores/4 threads, 35W TDP), making it power-efficient and capable for home server tasks. Here’s why it’s a good fit for an Ubuntu-based home server: Hardware Compatibility with Ubuntu: The M900 Tiny is certified for Ubuntu, with Lenovo listing support for Ubuntu 14.04, and community reports confirm compatibility with newer versions like 18.04 and 22.04. Users have successfully run Ubuntu Server or Desktop on the M900 Tiny for tasks like media servers (Plex, Jellyfin), NAS (OpenMediaVault), Docker containers (Sonarr, Radarr, Grafana), and virtualization (Proxmox, ESXi). The integrated Intel HD Graphics 530 and Skylake architecture handle lightweight server tasks well, and the CPU supports hardware transcoding (Intel Quick Sync) for media streaming, though 6th-gen Quick Sync is slightly less efficient than 7th-gen or later. Compact and Power-Efficient: At ~1.1L in size (179 x 182.9 x 34.5mm), it’s ideal for space-constrained environments. The 35W TDP CPU and efficient design result in low power consumption, with users reporting 18-30W under typical server loads, making it cost-effective for 24/7 operation. Storage and Connectivity: It supports one M.2 NVMe SSD (e.g., 256GB-1TB) and one 2.5” SATA drive (HDD or SSD), sufficient for a NAS or media server. Users have added 4TB drives for storage. Six USB 3.0 ports, DisplayPort, and an RJ-45 Ethernet port provide ample connectivity for external drives or peripherals. An optional M.2 Wi-Fi/Bluetooth card can be added for wireless connectivity, though Ethernet is preferred for server stability. Current Windows Setup: The M900 Tiny likely ships with Windows 10 Pro (or a renewed license). Switching to Ubuntu is straightforward, as Ubuntu can be installed on systems with no pre-installed OS or over Windows. You can dual-boot or replace Windows entirely. Ubuntu Server is lightweight and doesn’t require a GUI, maximizing resources for server tasks. Ensure you back up data before installing Ubuntu. Use Case Fit: Media Server: Suitable for Plex or Jellyfin with hardware transcoding, supporting 2-3 1080p streams or 1-2 4K streams with optimization (avoiding transcoding is ideal). NAS: OpenMediaVault or TrueNAS Scale work well, with users running 4TB drives for SMB shares. Docker/Containers: Portainer and Docker run smoothly for lightweight services like Sonarr, Radarr, or Immich. Virtualization: Proxmox or ESXi can run with 16GB+ RAM, though the 4-core/4-thread CPU limits heavy VM workloads. Game Servers: Minecraft or similar servers are viable with 16GB RAM (e.g., 4GB per server for 4 players). Potential Issues: Fan Noise: Some users report loud fan noise under load, especially with older BIOS versions. Updating the BIOS (July 2022 version) or configuring fan control in Ubuntu (using lm_sensors and fancontrol) can mitigate this. NVMe Issues: Adding an NVMe drive may cause boot issues (fans at 100%, no video output) if the drive or firmware is incompatible. Updating the BIOS and ensuring drive compatibility (e.g., Samsung 950 Pro) helps. Windows 11 Incompatibility: The 6th-gen CPU doesn’t meet Windows 11 requirements, but this is irrelevant for Ubuntu.RAM Upgrade OptionsThe M900 Tiny’s 8GB RAM (likely 1x8GB or 2x4GB DDR4 SODIMM) is sufficient for basic server tasks but limiting for virtualization, game servers, or multiple Docker containers. Upgrading is straightforward and recommended. Maximum RAM Capacity: The M900 Tiny has two SO-DIMM slots and officially supports up to 32GB (2x16GB) DDR4-2133 per Lenovo’s specs. Community reports confirm 64GB (2x32GB) works on some units, depending on the motherboard, BIOS, and memory brand (e.g., Crucial, TeamGroup). Success with 64GB requires matched 2x32GB kits (e.g., TeamGroup T-Create DDR4 SODIMM 64GB Kit, 3200MHz, CL22) and BIOS update to FWKTBFA (06/23/2022). For reliability, 32GB is safer and widely supported without compatibility concerns. Upgrade Recommendations: For Basic Server Tasks (Plex, NAS, lightweight Docker): Upgrade to 16GB (2x8GB) DDR4-2133 or DDR4-2400. This is cost-effective (~$25-50) and sufficient for most home servers. For Virtualization/Game Servers (Proxmox, ESXi, Minecraft): Upgrade to 32GB (2x16GB) DDR4-2133 (~$50-80). This supports multiple VMs or game servers. For Heavy Workloads: Attempt 64GB (2x32GB) (~$120-150) if running memory-intensive tasks (e.g., TrueNAS Scale with ZFS, multiple VMs). Verify compatibility with your unit’s serial number on Lenovo’s support site or test with a matched kit. RAM Compatibility: Use DDR4 SODIMM, 2133MHz or 2400MHz, non-ECC, 1.2V. Higher speeds (e.g., 3200MHz) may downclock to 2133MHz. Install modules in pairs for dual-channel performance (e.g., 2x8GB instead of 1x16GB). Compatible brands: Crucial, OFFTEK, TeamGroup, G.Skill. Check QVL or Lenovo’s support site for tested modules. Example: Crucial 16GB Kit (2x8GB) DDR4-2400 (CT2K8G4SFS824A) or TeamGroup 32GB Kit (2x16GB) DDR4-2133. Installation: Remove the back screw, slide the top panel forward, and lift the 2.5” drive caddy to access the SO-DIMM slots. No configuration is needed; the system auto-detects new RAM. Update the BIOS first to avoid compatibility issues with larger modules. Other Upgrade ConsiderationsTo optimize the M900 Tiny as an Ubuntu home server: Storage: Replace the existing drive with a 256GB-1TB NVMe SSD for the OS and apps (e.g., Samsung 970 EVO, ~$50-100). Add a 2.5” SATA SSD (e.g., 1TB, ~$60) or HDD (e.g., 4TB Seagate IronWolf, ~$100) for data. External USB 3.0 drives can expand storage, but internal drives are preferred for reliability. Avoid USB for critical data due to potential failures. BIOS Update: Update to the latest BIOS (FWKTBFA, 06/23/2022) to fix fan noise, improve RAM compatibility, and resolve NVMe issues. On Ubuntu, use Lenovo’s bootable USB method or extract files for Linux-based updates. Cooling: Monitor temperatures with lm_sensors on Ubuntu. If the fan is noisy, configure fan curves or replace the stock fan with a quieter model (check compatibility). Networking: Use the onboard Gigabit Ethernet for stable server performance. If Wi-Fi is needed, add an M.2 Wi-Fi card (e.g., Intel AX200, ~$20). For advanced setups (e.g., pfSense), a USB-to-Gigabit adapter can add a second NIC. Ubuntu Installation Tips Download Ubuntu Server (22.04 LTS or 24.04 LTS) from ubuntu.com for a lightweight, CLI-based OS. Use Ubuntu Desktop if you prefer a GUI. Create a Bootable USB with tools like Rufus (on Windows) or dd (on Linux). Backup Data: Save any critical files from the Windows setup before wiping the drive. Install Ubuntu: Boot from the USB, select “Install Ubuntu Server,” and follow the prompts. Choose the NVMe or SATA drive for installation. Configure a static IP for server access. Install updates post-installation (sudo apt update &amp;amp;&amp;amp; sudo apt upgrade). Drivers: Ubuntu typically auto-detects drivers for the M900’s Intel chipset, CPU, and networking. If issues arise (e.g., Wi-Fi or audio), check Ubuntu’s HCL or install proprietary drivers via ubuntu-drivers. Fan Control: If fan noise persists, install lm_sensors and fancontrol (sudo apt install lm-sensors fancontrol) and configure via sensors-detect. Server Software: NAS: Install OpenMediaVault (sudo apt install openmediavault) or TrueNAS Scale. Media Server: Install Plex or Jellyfin (sudo apt install plexmediaserver or use Docker). Docker: Install Docker (sudo apt install docker.io) and Portainer for container management. Virtualization: Install Proxmox or KVM for VMs. Cost Estimate for Upgrades 16GB RAM (2x8GB): $25-50 32GB RAM (2x16GB): $50-80 64GB RAM (2x32GB): $120-150 (if compatible) 256GB NVMe SSD: $30-50 4TB 2.5” HDD: ~$100 M.2 Wi-Fi Card: ~$20 (optional) Total: $75-300, depending on upgradesRecommendations Minimum Upgrade: Add 8GB RAM (total 16GB, ~$25) and a 256GB NVMe SSD (~$30). Install Ubuntu Server 22.04 LTS and OpenMediaVault for a NAS or Plex for media streaming. Optimal Upgrade: Upgrade to 32GB RAM (2x16GB, ~$60), add a 512GB NVMe SSD (~$50), and a 4TB 2.5” HDD (~$100). Run Proxmox with Docker containers or a TrueNAS Scale setup. Verify 64GB RAM: If pursuing 64GB, check your model’s serial number on Lenovo’s support site (https://pcsupport.lenovo.com) and test with a compatible kit (e.g., TeamGroup 2x32GB). Update BIOS first.Final NotesThe ThinkCentre M900 Tiny is an excellent, budget-friendly platform for an Ubuntu home server due to its compact size, low power use, and Ubuntu compatibility. Upgrading to 16-32GB RAM and adding an NVMe SSD will make it versatile for most home server tasks. For heavy virtualization or 64GB RAM, confirm compatibility with your specific unit. If you encounter issues during Ubuntu installation or upgrades (e.g., NVMe boot problems or fan noise), update the BIOS and consult community resources like Reddit’s r/homelab or ubuntuforums.org.If you share your intended server tasks (e.g., Plex, NAS, VMs) or specific model details (CPU, current storage), I can tailor recommendations further. Would you like help with Ubuntu installation steps or specific software setup?Setting up the Lenovo ThinkCentre M900 Tiny with Ubuntu Server for reliable, always-on operation as a home server involves configuring Ubuntu to prevent sleep, optimize power settings, ensure network stability, and secure remote access. Below is a step-by-step guide to configure Ubuntu Server (22.04 LTS or 24.04 LTS recommended) to run continuously without sleeping, along with other essential server settings. These instructions assume you’re starting with a fresh Ubuntu Server installation on the M900 Tiny.Step 1: Install Ubuntu Server Download Ubuntu Server: Go to ubuntu.com/download/server and download the latest LTS version (e.g., 22.04 LTS or 24.04 LTS). Create a Bootable USB: Use a tool like Rufus (on Windows) or dd (on Linux) to create a bootable USB drive. Example for dd on Linux: sudo dd if=ubuntu-22.04.3-live-server-amd64.iso of=/dev/sdX bs=4M status=progress &amp;amp;&amp;amp; sync (replace /dev/sdX with your USB device). Install Ubuntu: Boot the M900 Tiny from the USB (press F12 at startup to select the boot device). Follow the installer prompts: Choose language, keyboard, and network settings (use a static IP for servers; see Step 5). Select the NVMe or SATA drive for installation. Create a user account (e.g., admin with a strong password). Enable OpenSSH during installation for remote access. Skip additional software unless needed (e.g., Docker, cloud tools). After installation, reboot and log in via the console or SSH. Step 2: Prevent Sleep and SuspendUbuntu Server is designed to stay awake by default, but certain power-saving settings may enable sleep or suspend, especially if a desktop environment is installed or if the BIOS triggers it. Here’s how to ensure the system never sleeps: Disable Sleep in Ubuntu: Check current power settings: systemctl status systemd-suspend.service If it’s active, disable it: sudo systemctl mask suspend.targetsudo systemctl mask sleep.targetsudo systemctl mask hibernate.targetsudo systemctl mask hybrid-sleep.target Edit the logind configuration to prevent sleep on idle: sudo nano /etc/systemd/logind.conf Set the following lines (add or modify): HandleSuspendKey=ignoreHandleLidSwitch=ignoreHandleLidSwitchDocked=ignoreHandlePowerKey=poweroffIdleAction=ignore Save and restart the service: sudo systemctl restart systemd-logind Disable Sleep in GRUB: Some systems may enter sleep due to kernel parameters. Edit GRUB: sudo nano /etc/default/grub Modify the GRUB_CMDLINE_LINUX_DEFAULT line to include noacpi or acpi=off if needed (test cautiously, as this disables ACPI features): GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash noacpi&quot; Update GRUB: sudo update Karla update-grub Reboot to apply. BIOS Power Settings: Restart the M900 Tiny and enter the BIOS (press F1 at startup). Navigate to Power or Advanced settings. Disable options like: C-States or C1E (low-power CPU states). System Idle Power Saving. Suspend Mode or S3 Sleep State. Set After Power Loss to Restore Last State or Turn On to ensure the system restarts after power outages. Save and exit (F10). Verify No Sleep: Monitor system state: systemd-analyze Ensure the system stays in the running state. Test by leaving it idle for hours and checking if it remains accessible via SSH or console. Step 3: Optimize Power and PerformanceTo balance performance and efficiency on the M900 Tiny: Set CPU Governor: Use the performance governor for maximum CPU performance or ondemand for efficiency: sudo apt install cpufrequtilssudo cpufreq-set -g performance Verify: cpufreq-info Monitor Temperatures and Fan: Install lm-sensors to monitor CPU and system temperatures: sudo apt install lm-sensorssudo sensors-detectsensors If the fan is noisy (common on M900 Tiny), configure fan control: sudo apt install fancontrolsudo pwmconfig Follow prompts to set fan thresholds. If pwmconfig fails, update the BIOS (see Step 8). Disable Unnecessary Services: Stop services that may consume resources (e.g., GUI if installed): sudo systemctl disable bluetoothsudo systemctl disable cups Check running services: systemctl list-units --type=service --state=running Step 4: Configure NetworkingA stable network is critical for a server. Configure a static IP and ensure constant connectivity: Set a Static IP: Edit the Netplan configuration: sudo nano /etc/netplan/01-netcfg.yaml Example configuration (adjust for your network): network: version: 2 ethernets: eno1: dhcp4: no addresses: - 192.168.1.100/24 gateway4: 192.168.1.1 nameservers: addresses: [8.8.8.8, 8.8.4.4] Apply changes: sudo netplan apply Verify: ip addr Enable Wake-on-LAN (Optional): If you want to wake the server remotely: sudo apt install ethtoolsudo ethtool -s eno1 wol g Enable in BIOS under Power &amp;gt; Wake on LAN &amp;gt; Enabled. Test Connectivity: Ping your router or an external site: ping 8.8.8.8 Ensure the server remains reachable after hours of operation. Step 5: Secure Remote AccessFor reliable SSH access and security: Update SSH Configuration: Edit SSH settings: sudo nano /etc/ssh/sshd_config Modify or add: Port 22 # Change to a non-standard port (e.g., 2222) for securityPermitRootLogin noPasswordAuthentication no # Use SSH keys instead Generate an SSH key pair on your client machine: ssh-keygen -t rsa -b 4096 Copy the public key to the server: ssh-copy-id admin@192.168.1.100 Restart SSH: sudo systemctl restart ssh Install Fail2Ban: Protect against brute-force attacks: sudo apt install fail2bansudo systemctl enable fail2ban Default settings are sufficient for SSH protection. Firewall Setup: Install and configure UFW: sudo apt install ufwsudo ufw allow 22 # Or your custom SSH portsudo ufw enablesudo ufw status Add ports for specific services (e.g., 80 for HTTP, 32400 for Plex). Step 6: Install Essential Server SoftwareBased on common home server use cases, install software tailored to your needs: NAS (OpenMediaVault): sudo wget -O - https://omv-extras.org/install | bash Access the web interface at http://192.168.1.100. Media Server (Plex): sudo apt install apt-transport-httpswget -q https://downloads.plex.tv/plex-keys/PlexSign.key -O - | sudo apt-key add -echo deb https://downloads.plex.tv/repo/deb public main | sudo tee /etc/apt/sources.list.d/plexmediaserver.listsudo apt updatesudo apt install plexmediaserver Access at http://192.168.1.100:32400/web. Docker (For Containers): sudo apt install docker.iosudo systemctl enable dockersudo usermod -aG docker admin Install Portainer for management: docker run -d -p 9000:9000 --name portainer --restart always -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer-ce Access at http://192.168.1.100:9000. Virtualization (Proxmox): If using Proxmox instead of Ubuntu Server, download the ISO from proxmox.com and install it directly. It includes a web interface for VMs and containers. Step 7: Automate Updates and Backups Enable Automatic Updates: sudo apt install unattended-upgradessudo dpkg-reconfigure -plow unattended-upgrades Choose “Yes” to enable automatic security updates. Set Up Backups: Use rsync for local backups to an external drive: sudo rsync -avh --progress /home/admin /mnt/backup For offsite backups, use rclone with Google Drive or similar: sudo apt install rclonerclone config Monitor Disk Health: Install smartmontools: sudo apt install smartmontoolssudo smartctl -a /dev/sda Step 8: Update BIOS (Optional but Recommended)To address potential fan noise, NVMe compatibility, or power issues on the M900 Tiny: Check Current BIOS Version: On Ubuntu: sudo dmidecode -s bios-version Download Latest BIOS: Visit Lenovo’s support site, enter your M900 Tiny’s serial number, and download the latest BIOS (e.g., FWKTBFA, 06/23/2022). Update BIOS: Create a bootable USB with Lenovo’s BIOS update tool (Windows-based) or extract the .rom file and use flashrom on Ubuntu (advanced). Follow Lenovo’s instructions to flash the BIOS from a USB or Windows environment. Step 9: Monitor and Test Install Monitoring Tools: Use htop for resource usage: sudo apt install htophtop Install cockpit for a web-based dashboard: sudo apt install cockpit Access at https://192.168.1.100:9090. Test Server Uptime: Check uptime: uptime Leave the server running for 24-48 hours and verify it doesn’t sleep, lose network connectivity, or overheat. Log Monitoring: Check system logs for errors: sudo journalctl -xe Additional Tips Fan Noise: If the fan remains loud after BIOS update, replace it with a compatible Noctua fan (check M900 Tiny’s fan size, ~40mm) or adjust fan curves manually. Power Outages: Connect the M900 Tiny to a UPS (e.g., APC 600VA, ~$50) to prevent data loss during outages. Specific Use Cases: If you have a specific server goal (e.g., Plex, Minecraft, NAS), let me know, and I can provide tailored software setup instructions. Community Resources: Check r/homelab or ubuntuforums.org for M900 Tiny-specific tips.VerificationAfter configuration: SSH into the server after 24 hours to confirm it’s awake (ssh admin@192.168.1.100). Check temperatures (sensors) and fan speed to ensure the system isn’t overheating. Access any installed services (e.g., Plex, OpenMediaVault) to verify functionality.This setup ensures your M900 Tiny runs Ubuntu Server reliably, stays awake, and is optimized for home server tasks. If you encounter issues (e.g., fan noise, network drops) or need help with specific software (e.g., Docker, Proxmox), let me know, and I’ll provide targeted assistance. Would you like detailed instructions for a specific server application or troubleshooting for any hardware concerns?Lenovo ThinkCentre M900 TinyThe Dell OptiPlex 3020 (assuming a micro, SFF, or mini-tower model with a 4th-generation Intel Core processor) can be configured as an Ubuntu home server similarly to the Lenovo ThinkCentre M900 Tiny, but there are hardware differences that affect compatibility, setup, and performance. Below, I’ll evaluate the Dell 3020’s suitability as an Ubuntu server, confirm whether the same configuration and setup from the M900 Tiny can be applied, and highlight any adjustments needed to prevent sleep, optimize settings, and address hardware-specific considerations.Suitability of Dell OptiPlex 3020 as an Ubuntu Home ServerThe Dell OptiPlex 3020 is a business-class desktop from 2014, typically equipped with a 4th-generation Intel Core processor (e.g., i5-4590, 4 cores/4 threads, 3.3GHz, 84W TDP in SFF/mini-tower; or i5-4590T, 4 cores/4 threads, 2.0-3.0GHz, 35W TDP in micro). It’s a capable platform for a home server, but here’s how it compares to the M900 Tiny: Hardware Compatibility with Ubuntu: The OptiPlex 3020 is well-supported by Ubuntu, with Dell providing Ubuntu certification for similar models (e.g., OptiPlex 3010, 3020) on versions like 14.04 LTS. Community reports confirm successful installations of Ubuntu 18.04, 20.04, and 22.04 on the 3020. The Intel Haswell architecture (4th-gen CPUs) and Intel Q87 chipset work seamlessly with Ubuntu’s kernel, including integrated Intel HD Graphics 4600 for lightweight tasks or hardware transcoding (Quick Sync, though less efficient than the M900’s 6th-gen Skylake). Common server tasks like Plex, OpenMediaVault, Docker, or Proxmox run well, though the 3020’s older CPU and higher power draw (especially in SFF/mini-tower) make it less efficient than the M900 Tiny. Form Factor and Power Efficiency: Micro: Similar to the M900 Tiny (~1.2L, 182 x 178 x 36mm), with a 35W TDP CPU (e.g., i5-4590T). Power consumption is ~30-50W under load, slightly higher than the M900 Tiny (~18-30W). SFF/Mini-Tower: Larger (SFF: ~11.4L; mini-tower: ~16.7L) with 65-84W TDP CPUs (e.g., i5-4590). Power draw is ~50-100W, less efficient for 24/7 operation. The micro form factor is ideal for space-constrained setups, but all models are less compact than the M900 Tiny. Storage and Connectivity: Micro: One 2.5” SATA bay (HDD/SSD, up to 4TB) and one M.2 SATA slot (for SSDs, not NVMe). No NVMe support limits storage performance compared to the M900 Tiny. SFF/Mini-Tower: Multiple 3.5”/2.5” SATA bays (e.g., 2-4 drives) and one PCIe slot for additional storage controllers (e.g., HBA for ZFS/TrueNAS). M.2 SATA support may be absent or limited. Connectivity: 4-6 USB ports (2.0/3.0 mix), VGA, DisplayPort/HDMI, and Gigabit Ethernet. No native Wi-Fi, but an M.2 or PCIe Wi-Fi card can be added. Storage flexibility is better in SFF/mini-tower models for NAS setups compared to the M900 Tiny’s single 2.5” bay. RAM: The 3020 (all form factors) has two DIMM slots (non-ECC DDR3, 1600MHz) and supports up to 16GB (2x8GB) officially. Unlike the M900 Tiny (32-64GB DDR4), 16GB is the maximum, which may limit virtualization or memory-intensive tasks. Your current 8GB (likely 1x8GB or 2x4GB) is adequate for basic tasks (Plex, NAS) but should be upgraded to 16GB for Docker or VMs. Use Case Fit: Media Server: Plex/Jellyfin work well with Quick Sync for 1-2 1080p streams or 1 4K stream (avoid transcoding for best performance). NAS: OpenMediaVault or TrueNAS Core run effectively, especially in SFF/mini-tower with multiple drives. Docker: Lightweight containers (Sonarr, Radarr) are fine with 8-16GB RAM. Virtualization: Proxmox or ESXi are viable with 16GB RAM, but the 4-core/4-thread CPU and RAM cap limit heavy VM workloads compared to the M900 Tiny. Game Servers: Minecraft or similar servers work with 16GB (4-6GB allocated per server). Potential Issues: Power Consumption: SFF/mini-tower models use more power than the M900 Tiny, increasing electricity costs for 24/7 operation. No NVMe: Micro models lack NVMe support, limiting storage speed. SFF/mini-tower can add PCIe NVMe adapters. Fan Noise: The 3020’s stock cooling can be noisy under load, especially in micro models. BIOS updates or fan replacements may help. BIOS Limitations: Older BIOS versions may have issues with Linux power management or fan control. Updating is recommended. Can You Use the Same Configuration and Setup?The Ubuntu Server setup described for the M900 Tiny (preventing sleep, optimizing power, configuring networking, securing SSH, etc.) is largely applicable to the Dell OptiPlex 3020, as both systems use Intel x86-64 architecture and are Linux-compatible. However, hardware differences require adjustments. Below, I’ll adapt the M900 Tiny’s setup instructions for the 3020, noting deviations and specific considerations.Step 1: Install Ubuntu ServerThe installation process is identical: Download Ubuntu Server (22.04 LTS or 24.04 LTS) from ubuntu.com/download/server. Create a Bootable USB using Rufus or dd (e.g., sudo dd if=ubuntu-22.04.3-live-server-amd64.iso of=/dev/sdX bs=4M status=progress &amp;amp;&amp;amp; sync). Install: Boot from USB (press F12 on the 3020 to enter the boot menu). Configure a static IP, select the SATA/M.2 drive, enable OpenSSH, and create a user (e.g., admin). Note: If using an M.2 SATA SSD, ensure it’s detected during installation. The 3020 micro doesn’t support NVMe, so avoid NVMe drives unless using a PCIe adapter in SFF/mini-tower. Adjustment: If the installer fails to detect the M.2 SATA SSD (rare), update the BIOS first (see Step 8) or use a 2.5” SATA drive.Step 2: Prevent Sleep and SuspendThe steps to disable sleep are the same, but BIOS settings differ: Disable Sleep in Ubuntu: Disable suspend services: sudo systemctl mask suspend.target sleep.target hibernate.target hybrid-sleep.target Edit /etc/systemd/logind.conf: sudo nano /etc/systemd/logind.conf Set: HandleSuspendKey=ignoreHandleLidSwitch=ignoreHandleLidSwitchDocked=ignoreHandlePowerKey=poweroffIdleAction=ignore Restart: sudo systemctl restart systemd-logind Disable Sleep in GRUB: Edit /etc/default/grub: sudo nano /etc/default/grub Add noacpi if needed (test first): GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash noacpi&quot; Update: sudo update-grub BIOS Power Settings: Enter the BIOS (press F2 at startup). Navigate to Power Management: Disable Deep Sleep Control or S3 Sleep State. Set AC Recovery to Power On or Last State for power outage recovery. Disable C-States Control or Intel SpeedStep to prevent low-power modes. Save and exit (F10). Note: The 3020’s BIOS may have fewer power-saving options than the M900 Tiny, but ensure all sleep-related settings are off. Verify: Check system state: systemd-analyze Leave idle for hours and confirm it stays accessible via SSH. Step 3: Optimize Power and Performance CPU Governor: Install and set to performance or ondemand: sudo apt install cpufrequtilssudo cpufreq-set -g performancecpufreq-info The 3020’s 4th-gen CPU (e.g., i5-4590) is less efficient than the M900’s 6th-gen, so ondemand may save power in micro models. Monitor Temperatures and Fan: Install lm-sensors: sudo apt install lm-sensorssudo sensors-detectsensors For fan control: sudo apt install fancontrolsudo pwmconfig Note: The 3020’s fan control may be limited by the BIOS. If pwmconfig fails, update the BIOS or replace the fan (e.g., Noctua 40mm for micro). Disable Unnecessary Services: Same as M900 Tiny: sudo systemctl disable bluetooth cupssystemctl list-units --type=service --state=running Adjustment: The 3020’s higher TDP (especially SFF/mini-tower) may cause warmer operation. Monitor temperatures and consider adding case fans in larger models.Step 4: Configure NetworkingThe networking setup is identical: Static IP: Edit /etc/netplan/01-netcfg.yaml: sudo nano /etc/netplan/01-netcfg.yaml Example: network: version: 2 ethernets: eno1: dhcp4: no addresses: [192.168.1.100/24] gateway4: 192.168.1.1 nameservers: addresses: [8.8.8.8, 8.8.4.4] Apply: sudo netplan applyip addr Wake-on-LAN (Optional): Enable: sudo apt install ethtoolsudo ethtool -s eno1 wol g In BIOS, enable Wake on LAN under Power Management. Adjustment: The 3020’s Intel I217-LM Ethernet controller is Linux-compatible but may require firmware updates for stability. Check:sudo dmesg | grep ethernetStep 5: Secure Remote AccessThe SSH and security setup is the same: SSH Configuration: Edit /etc/ssh/sshd_config: sudo nano /etc/ssh/sshd_config Set: Port 2222PermitRootLogin noPasswordAuthentication no Copy SSH keys and restart: ssh-copy-id admin@192.168.1.100sudo systemctl restart ssh Fail2Ban: sudo apt install fail2bansudo systemctl enable fail2ban Firewall: sudo apt install ufwsudo ufw allow 2222sudo ufw enablesudo ufw status Adjustment: No changes needed; the 3020’s networking stack supports these settings identically.Step 6: Install Essential Server SoftwareThe software installation (Plex, OpenMediaVault, Docker, etc.) is identical to the M900 Tiny: NAS (OpenMediaVault): sudo wget -O - https://omv-extras.org/install | bash Note: SFF/mini-tower models support more drives, making them better for NAS. Plex: sudo apt install apt-transport-httpswget -q https://downloads.plex.tv/plex-keys/PlexSign.key -O - | sudo apt-key add -echo deb https://downloads.plex.tv/repo/deb public main | sudo tee /etc/apt/sources.list.d/plexmediaserver.listsudo apt updatesudo apt install plexmediaserver Docker: sudo apt install docker.iosudo systemctl enable dockersudo usermod -aG docker admindocker run -d -p 9000:9000 --name portainer --restart always -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer-ce Proxmox: Install Proxmox directly from its ISO if preferred over Ubuntu Server. Adjustment: The 3020’s 16GB RAM cap and 4th-gen CPU limit heavy virtualization. Allocate resources carefully (e.g., 4-6GB for Plex, 2-4GB per VM).Step 7: Automate Updates and BackupsIdentical to the M900 Tiny: Automatic Updates: sudo apt install unattended-upgradessudo dpkg-reconfigure -plow unattended-upgrades Backups: Local: sudo rsync -avh --progress /home/admin /mnt/backup Offsite: Install rclone and configure. Disk Health: sudo apt install smartmontoolssudo smartctl -a /dev/sda Adjustment: If using multiple drives in SFF/mini-tower, monitor each with smartctl.Step 8: Update BIOS Check BIOS Version: sudo dmidecode -s bios-version Update: Visit Dell’s support site, enter your 3020’s service tag, and download the latest BIOS (e.g., A20, 2019). Use Dell’s bootable USB method or Windows-based updater (before wiping Windows). On Ubuntu, use flashrom (advanced) if supported. Why: Fixes fan control, Linux compatibility, and power management issues. Adjustment: The 3020’s BIOS updates are critical for Linux, as early versions (e.g., A01) have poor fan control and sleep handling.Step 9: Monitor and TestSame as M900 Tiny: Monitoring: sudo apt install htop cockpit Access Cockpit at https://192.168.1.100:9090. Test Uptime: uptime Logs: sudo journalctl -xe Adjustment: The 3020 may run warmer (especially SFF/mini-tower). Ensure good airflow and monitor temperatures.Hardware-Specific Adjustments for Dell 3020 RAM Upgrade: Upgrade to 16GB (2x8GB DDR3-1600, ~$30-50) for optimal performance. Compatible brands: Crucial, Kingston. Unlike the M900 Tiny (32-64GB DDR4), 16GB is the maximum, so prioritize lightweight workloads. Install: Open the case (micro: slide panel; SFF/mini-tower: remove side panel) and insert DIMMs in paired slots. Storage: Micro: Use a 2.5” SATA SSD (e.g., 1TB, ~$60) or M.2 SATA SSD (e.g., 256GB, ~$30). Add external USB drives for extra storage. SFF/Mini-Tower: Install multiple 3.5”/2.5” drives (e.g., 4TB Seagate IronWolf, ~$100 each) for NAS. No NVMe: For NVMe, add a PCIe-to-NVMe adapter in SFF/mini-tower (~$20). Cooling: The 3020’s stock fan can be noisy. Replace with a quieter model (e.g., Noctua 80mm for SFF) or adjust fan curves post-BIOS update. Ensure ventilation, especially for SFF/mini-tower with multiple drives. Power Supply: Micro: External 65W adapter (sufficient). SFF/Mini-Tower: 255W/290W PSU. Consider a higher-efficiency PSU (~$50) for 24/7 operation if adding drives. Form Factor Choice: Micro: Best for space and power efficiency, similar to M900 Tiny, but limited storage. SFF/Mini-Tower: Better for NAS or high-storage setups but less efficient and larger. Key Differences and Limitations| Feature | Lenovo ThinkCentre M900 Tiny | Dell OptiPlex 3020 (Micro/SFF) ||————————|—————————–|——————————-|| CPU | 6th-gen Intel (e.g., i5-6500T, 35W) | 4th-gen Intel (e.g., i5-4590T, 35W; i5-4590, 84W) || RAM | 32-64GB DDR4 (2x SODIMM) | 16GB DDR3 (2x DIMM) || Storage | 1x M.2 NVMe + 1x 2.5” SATA | Micro: 1x M.2 SATA + 1x 2.5” SATA; SFF: 2-4x SATA || NVMe Support | Yes (M.2 NVMe) | No (SFF can use PCIe adapter) || Power Consumption | 18-30W (micro) | 30-50W (micro); 50-100W (SFF) || Size | 1.1L (micro) | 1.2L (micro); 11.4L (SFF) || Fan Noise | Moderate, BIOS-fixable | Potentially louder, BIOS-dependent | RAM Limitation: The 3020’s 16GB cap restricts heavy virtualization or multi-container Docker setups compared to the M900 Tiny’s 32-64GB. Storage: The 3020 SFF/mini-tower excels for NAS with multiple drives, but the micro is storage-limited like the M900 Tiny. Efficiency: The 3020 (especially SFF) consumes more power, increasing costs for 24/7 use. NVMe: Lack of native NVMe in the 3020 reduces storage performance unless using PCIe adapters in SFF/mini-tower.Recommendations Use Same Setup: Apply the M900 Tiny’s Ubuntu Server configuration (Steps 1-9) with the adjustments noted above. The core steps (disabling sleep, static IP, SSH, software) are identical due to Linux compatibility. Form Factor: Micro: Ideal for lightweight tasks (Plex, Docker) in small spaces. Upgrade to 16GB RAM and add a 1TB 2.5” SSD. SFF/Mini-Tower: Better for NAS or storage-heavy setups. Add 16GB RAM, multiple 4TB drives, and a PCIe NVMe adapter if needed. Upgrades: RAM: Upgrade to 16GB (2x8GB DDR3-1600, ~$30). Storage: Micro: 256GB M.2 SATA SSD (~$30) + 4TB 2.5” HDD (~$100). SFF: 512GB SATA SSD (~$50) + 2x 4TB HDD (~$200). Cooling: Update BIOS and consider fan replacement if noisy. BIOS Update: Critical for fan control and Linux compatibility. Do this before Ubuntu installation if possible. Workload: Stick to lightweight tasks (Plex, OpenMediaVault, 1-2 VMs) due to RAM and CPU limits. Avoid heavy virtualization unless using SFF with maxed-out resources.Cost Estimate: 16GB RAM: $30 256GB M.2 SATA SSD: $30 4TB 2.5” HDD: $100 Total: ~$60-160 (micro); add $100-200 for SFF multi-drive setups.Verification After setup, test: SSH accessibility after 24 hours (ssh admin@192.168.1.100). No sleep (systemd-analyze). Temperatures (sensors) and fan behavior. Service functionality (e.g., Plex at http://192.168.1.100:32400). If issues arise (e.g., fan noise, network drops), check BIOS version, update firmware, or consult r/homelab or Dell’s community forums.The Dell 3020 can use nearly the same Ubuntu Server setup as the M900 Tiny with minor tweaks for BIOS, storage, and cooling. It’s a solid choice for a home server, especially the SFF for NAS, though it’s less efficient and RAM-limited compared to the M900 Tiny. If you specify your 3020’s form factor (micro/SFF/mini-tower), CPU, or intended use (e.g., Plex, NAS), I can refine the setup further. Would you like help with a specific software installation or troubleshooting potential 3020 issues?" }, { "title": "Mosquitto with Docker Compose and Base Authentication &amp; Let&#39;s Encrypt (DuckDNS)", "url": "/posts/mosquitto-tls/", "categories": "DEV", "tags": "cli", "date": "2025-04-01 07:35:00 -0400", "snippet": "Base Authentication &amp;amp; Let’s Encrypt (DuckDNS)Here’s a comprehensive guide to set up Mosquitto with Docker Compose using base authentication and Let’s Encrypt with a DuckDNS domain on a clean droplet.Prerequisites A domain pointed to your droplet (via DuckDNS) Docker and Docker Compose installed Ports 80, 443, and 1883 open in your firewallStep 1: Set Up DuckDNS (if not already done) Create a DuckDNS account and claim your subdomain Install DuckDNS updater (optional for dynamic IP): docker run -d --name duckdns \\ -e PUID=1000 \\ -e PGID=1000 \\ -e TZ=UTC \\ -e SUBDOMAINS=yourdomain \\ -e TOKEN=your-duckdns-token \\ -e LOG_FILE=false \\ --restart unless-stopped \\ linuxserver/duckdns Step 2: Create Docker Compose FileCreate a docker-compose.yml file with the following content:version: &#39;3.8&#39;services: mosquitto: image: eclipse-mosquitto:2 container_name: mosquitto restart: unless-stopped ports: - &quot;1883:1883&quot; - &quot;9001:9001&quot; volumes: - ./mosquitto/config:/mosquitto/config - ./mosquitto/data:/mosquitto/data - ./mosquitto/log:/mosquitto/log environment: - TZ=UTC depends_on: - nginx-proxy nginx-proxy: image: nginxproxy/nginx-proxy container_name: nginx-proxy restart: unless-stopped ports: - &quot;80:80&quot; - &quot;443:443&quot; volumes: - ./certs:/etc/nginx/certs - ./vhost.d:/etc/nginx/vhost.d - /var/run/docker.sock:/tmp/docker.sock:ro - ./html:/usr/share/nginx/html environment: - DEFAULT_HOST=yourdomain.duckdns.org letsencrypt: image: nginxproxy/acme-companion container_name: nginx-proxy-letsencrypt restart: unless-stopped volumes: - ./certs:/etc/nginx/certs - ./acme.sh:/etc/acme.sh - /var/run/docker.sock:/var/run/docker.sock:ro - ./vhost.d:/etc/nginx/vhost.d - ./html:/usr/share/nginx/html environment: - DEFAULT_EMAIL=your@email.com - NGINX_PROXY_CONTAINER=nginx-proxyStep 3: Create Mosquitto ConfigurationCreate the directory structure and config file:mkdir -p mosquitto/config mosquitto/data mosquitto/logCreate mosquitto/config/mosquitto.conf:persistence truepersistence_location /mosquitto/data/log_dest file /mosquitto/log/mosquitto.log# Authenticationallow_anonymous falsepassword_file /mosquitto/config/passwd# MQTTlistener 1883protocol mqtt# Websocketslistener 9001protocol websocketsStep 4: Set Up Authentication Create a password file (replace username/password): docker run -it --rm -v $(pwd)/mosquitto/config:/mosquitto/config eclipse-mosquitto:2 mosquitto_passwd -c /mosquitto/config/passwd username (You’ll be prompted to enter and confirm the password) For additional users (without -c flag to avoid overwriting): docker run -it --rm -v $(pwd)/mosquitto/config:/mosquitto/config eclipse-mosquitto:2 mosquitto_passwd /mosquitto/config/passwd anotheruser Step 5: Set Up EnvironmentCreate a .env file:VIRTUAL_HOST=yourdomain.duckdns.orgLETSENCRYPT_HOST=yourdomain.duckdns.orgLETSENCRYPT_EMAIL=your@email.comStep 6: Start the Servicesdocker compose up -dStep 7: Verify Setup Check logs for errors: docker logs mosquittodocker logs nginx-proxydocker logs nginx-proxy-letsencrypt Test MQTT connection: mosquitto_sub -h yourdomain.duckdns.org -t test -u username -P password (In another terminal) mosquitto_pub -h yourdomain.duckdns.org -t test -m &quot;hello&quot; -u username -P password Common Issues and Solutions Certificate errors: Ensure your domain resolves correctly to the server IP Check port 80 is open and accessible Wait a few minutes for Let’s Encrypt to issue certificates Authentication failures: Verify the password file exists and has correct permissions Check the password file path in mosquitto.conf Connection issues: Verify ports are open in firewall: sudo ufw allow 80,443,1883/tcp Check Mosquitto is listening: netstat -tulnp | grep mosquitto DuckDNS updates: If your IP changes frequently, ensure the DuckDNS updater is running This setup provides a secure Mosquitto broker with: TLS encryption via Let’s Encrypt Basic authentication WebSockets support Automatic certificate renewal Reverse proxy for additional securityBase Authentication &amp;amp; Let’s Encrypt (Direct Setup)Here’s a simplified setup that integrates Let’s Encrypt directly with Mosquitto without using Nginx as a reverse proxy.Prerequisites A domain pointed to your droplet (via DuckDNS) Docker and Docker Compose installed Ports 80 (temporary for certbot), 443, and 1883 open in your firewallStep 1: Create Docker Compose FileCreate a docker-compose.yml file with:version: &#39;3.8&#39;services: mosquitto: image: eclipse-mosquitto:2 container_name: mosquitto restart: unless-stopped ports: - &quot;1883:1883&quot; - &quot;8883:8883&quot; # SSL port - &quot;80:80&quot; # Temporary for certbot - &quot;9001:9001&quot; # WebSocket volumes: - ./mosquitto/config:/mosquitto/config - ./mosquitto/data:/mosquitto/data - ./mosquitto/log:/mosquitto/log - ./letsencrypt:/etc/letsencrypt environment: - TZ=UTC command: /bin/sh -c &quot;(if [ ! -f /etc/letsencrypt/live/yourdomain.duckdns.org/cert.pem ]; then sleep 30; fi) &amp;amp;&amp;amp; /usr/sbin/mosquitto -c /mosquitto/config/mosquitto.conf&quot; certbot: image: certbot/certbot container_name: certbot volumes: - ./letsencrypt:/etc/letsencrypt - ./mosquitto/config:/mosquitto/config entrypoint: &quot;/bin/sh -c&quot; command: | &quot; if [ ! -f /etc/letsencrypt/live/yourdomain.duckdns.org/cert.pem ]; then certbot certonly --standalone --non-interactive --agree-tos --email your@email.com -d yourdomain.duckdns.org; ln -sf /etc/letsencrypt/live/yourdomain.duckdns.org/cert.pem /mosquitto/config/server.crt; ln -sf /etc/letsencrypt/live/yourdomain.duckdns.org/privkey.pem /mosquitto/config/server.key; fi &amp;amp;&amp;amp; while :; do sleep 12h &amp;amp; wait $${!}; certbot renew; done &quot; depends_on: - mosquittoStep 2: Create Mosquitto ConfigurationCreate the directory structure:mkdir -p mosquitto/{config,data,log} letsencryptCreate mosquitto/config/mosquitto.conf:persistence truepersistence_location /mosquitto/data/log_dest file /mosquitto/log/mosquitto.log# Authenticationallow_anonymous falsepassword_file /mosquitto/config/passwd# MQTTlistener 1883protocol mqtt# MQTT over SSLlistener 8883protocol mqttcertfile /mosquitto/config/server.crtkeyfile /mosquitto/config/server.key# WebSocketslistener 9001protocol websocketsStep 3: Set Up AuthenticationCreate password file (replace username/password):docker run -it --rm -v $(pwd)/mosquitto/config:/mosquitto/config eclipse-mosquitto:2 mosquitto_passwd -c /mosquitto/config/passwd usernameStep 4: Start the Servicesdocker compose up -dStep 5: Verify Setup Check certificates were generated: docker exec mosquitto ls -la /mosquitto/config/ (Should see server.crt and server.key symlinks) Test SSL connection: mosquitto_sub -h yourdomain.duckdns.org -t test -p 8883 --capath /etc/ssl/certs/ -u username -P password (In another terminal) mosquitto_pub -h yourdomain.duckdns.org -t test -m &quot;hello&quot; -p 8883 --capath /etc/ssl/certs/ -u username -P password Step 6: Clean Up Temporary PortAfter certificates are generated (check with docker logs certbot), you can: Remove port 80 from mosquitto service in docker-compose.yml Update the file and run: docker compose down &amp;amp;&amp;amp; docker compose up -d Certificate RenewalThe certbot container will automatically: Check for certificates on startup Renew certificates when needed (every 12 hours) Update the symlinks Mosquitto usesSecurity Considerations After setup, close port 80 if not needed: sudo ufw deny 80/tcp Consider adding additional security to your Mosquitto configuration: # Add to mosquitto.confrequire_certificate falsetls_version tlsv1.2 This direct approach is simpler than using Nginx as a reverse proxy while still providing: Automatic Let’s Encrypt certificate management Secure MQTT over SSL (port 8883) Basic authentication WebSockets support Automatic certificate renewalBase authentication … From chatGptTo set up Mosquitto with Docker Compose, basic authentication, and Let’s Encrypt on a clean droplet using a DuckDNS domain, follow these optimized steps:1. Install Docker &amp;amp; Docker ComposeEnsure your droplet has Docker and Docker Compose installed:sudo apt update &amp;amp;&amp;amp; sudo apt upgrade -ysudo apt install docker.io -ysudo systemctl enable --now dockersudo usermod -aG docker $USERInstall Docker Compose:sudo apt install docker-compose -y2. Set Up DuckDNSRegister a free subdomain on DuckDNS and get your token.Create a DuckDNS update script:mkdir ~/duckdns &amp;amp;&amp;amp; cd ~/duckdnsnano update.shAdd:#!/bin/bashecho url=”https://www.duckdns.org/update?domains=YOUR_SUBDOMAIN&amp;amp;token=YOUR_DUCKDNS_TOKEN&amp;amp;ip=” | curl -k -o ~/duckdns/duck.log -K -Save and exit.Make it executable:chmod +x update.shRun it manually to test:./update.shSet up a cron job to update it every 5 minutes: (crontab -l ; echo “*/5 * * * * ~/duckdns/update.sh &amp;gt;/dev/null 2&amp;gt;&amp;amp;1”) crontab - 3. Prepare Mosquitto ConfigurationCreate a working directory:mkdir -p ~/mosquitto/config ~/mosquitto/data ~/mosquitto/logcd ~/mosquitto/confignano mosquitto.confPaste this config:listener 1883allow_anonymous falsepassword_file /mosquitto/config/passwordslistener 8883certfile /mosquitto/config/certs/fullchain.pemkeyfile /mosquitto/config/certs/privkey.pemSave and exit.4. Create Authentication Filedocker run –rm eclipse-mosquitto mosquitto_passwd -c /mosquitto/config/passwords YOUR_USERNAMEEnter a strong password.5. Set Up Let’s Encrypt SSL (Using Certbot)Install Certbot:sudo apt install certbot -yRun Certbot:sudo certbot certonly --standalone --preferred-challenges http --agree-tos --email YOUR_EMAIL -d YOUR_SUBDOMAIN.duckdns.orgMove certificates:mkdir -p ~/mosquitto/config/certssudo cp /etc/letsencrypt/live/YOUR_SUBDOMAIN.duckdns.org/fullchain.pem ~/mosquitto/config/certs/sudo cp /etc/letsencrypt/live/YOUR_SUBDOMAIN.duckdns.org/privkey.pem ~/mosquitto/config/certs/6. Create Docker Compose FileGo back to your home directory and create docker-compose.yml:cd ~/mosquittonano docker-compose.ymlPaste:version: &#39;3&#39;services: mosquitto: image: eclipse-mosquitto container_name: mosquitto restart: always volumes: - ./config:/mosquitto/config - ./data:/mosquitto/data - ./log:/mosquitto/log ports: - &quot;1883:1883&quot; - &quot;8883:8883&quot;Save and exit.7. Start MosquittoRun:docker-compose up -d8. Auto-Renew SSLCreate a script:sudo nano /etc/cron.daily/certbot-renewAdd:#!/bin/bashcertbot renew --quietcp /etc/letsencrypt/live/YOUR_SUBDOMAIN.duckdns.org/fullchain.pem ~/mosquitto/config/certs/cp /etc/letsencrypt/live/YOUR_SUBDOMAIN.duckdns.org/privkey.pem ~/mosquitto/config/certs/docker restart mosquittoSave and make it executable:#!/bin/bashsudo chmod +x /etc/cron.daily/certbot-renewThis setup ensures secure MQTT connections with authentication and automatic SSL renewal.Are you getting specific errors? I can help debug.From grokSetting up Mosquitto with Docker Compose, basic authentication, Let’s Encrypt certificates, and a DuckDNS domain on a clean DigitalOcean droplet can be a powerful way to create a secure MQTT broker. Below, I’ll guide you through a streamlined, optimal approach to achieve this. I’ll assume you’re using Ubuntu (common for DigitalOcean droplets) and aim to avoid common pitfalls that might be causing your errors. If you share specific errors you’re encountering, I can refine this further!Prerequisites Clean Droplet: Ubuntu 22.04 (or similar) installed. Domain: A DuckDNS subdomain (e.g., yourname.duckdns.org). Docker &amp;amp; Docker Compose: Installed on the droplet. Ports: Ensure ports 80, 443, and 8883 are open in your droplet’s firewall and DigitalOcean networking settings.Step-by-Step Guide1. Install Docker and Docker ComposeLog in to your droplet via SSH and run the following commands to install Docker and Docker Compose:sudo apt update &amp;amp;&amp;amp; sudo apt upgrade -ysudo apt install -y docker.io docker-composesudo systemctl start dockersudo systemctl enable dockersudo usermod -aG docker $USER # Log out and back in after thisVerify installation:docker --versiondocker-compose --version2. Set Up DuckDNS Go to DuckDNS.org, log in, and create a subdomain (e.g., yourname.duckdns.org). Note your DuckDNS token (you’ll need it later). Update your DuckDNS domain to point to your droplet’s public IP. You can automate this with a script or Docker container, but for now, manually set it on the DuckDNS website.3. Create Project DirectoryCreate a directory to hold your Mosquitto setup:mkdir ~/mosquitto &amp;amp;&amp;amp; cd ~/mosquitto4. Configure Docker ComposeCreate a docker-compose.yml file with Mosquitto and a Certbot service for Let’s Encrypt certificates:version: &#39;3.8&#39;services: mosquitto: image: eclipse-mosquitto:latest container_name: mosquitto restart: unless-stopped ports: - &quot;1883:1883&quot; # Unsecured MQTT (optional, for testing) - &quot;8883:8883&quot; # Secured MQTT - &quot;80:80&quot; # Temporary for Certbot HTTP challenge volumes: - ./mosquitto.conf:/mosquitto/config/mosquitto.conf - ./passwd:/mosquitto/config/passwd - ./letsencrypt:/etc/letsencrypt depends_on: - certbot certbot: image: certbot/certbot container_name: certbot volumes: - ./letsencrypt:/etc/letsencrypt command: certonly --standalone -d yourname.duckdns.org --email your.email@example.com --agree-tos --non-interactive depends_on: - mosquittoNotes: Replace yourname.duckdns.org with your DuckDNS subdomain. Replace your.email@example.com with your email for certificate notifications. Port 80 is exposed temporarily for Certbot’s HTTP-01 challenge. After certificates are obtained, you can remove it.5. Configure MosquittoCreate a mosquitto.conf file in the ~/mosquitto directory:nano mosquitto.confAdd the following configuration:# General settingspersistence truepersistence_location /mosquitto/data/log_dest file /mosquitto/log/mosquitto.log# Unsecured listener (optional, for testing)listener 1883allow_anonymous true# Secured listener with TLSlistener 8883allow_anonymous falsepassword_file /mosquitto/config/passwdcafile /etc/letsencrypt/live/yourname.duckdns.org/chain.pemcertfile /etc/letsencrypt/live/yourname.duckdns.org/fullchain.pemkeyfile /etc/letsencrypt/live/yourname.duckdns.org/privkey.pem 1883: Unsecured MQTT for local testing (optional; remove later for security). 8883: Secured MQTT with TLS and authentication. Replace yourname.duckdns.org with your DuckDNS domain.6. Set Up AuthenticationCreate a password file for Mosquitto:touch passwddocker run --rm -it -v $(pwd)/passwd:/passwd eclipse-mosquitto mosquitto_passwd -b /passwd username password Replace username and password with your desired credentials. This adds a hashed password to the passwd file.7. Open Firewall PortsEnsure the necessary ports are open:sudo ufw allow 80sudo ufw allow 443sudo ufw allow 8883sudo ufw statusAlso, configure DigitalOcean’s firewall (via the control panel) to allow these ports.8. Start Docker ComposeRun the setup:docker-compose up -d Certbot will obtain the Let’s Encrypt certificates and store them in the letsencrypt directory. Mosquitto will start with TLS enabled once certificates are available.9. Verify MosquittoTest the unsecured port (if enabled):docker run --rm -it eclipse-mosquitto mosquitto_sub -h your_droplet_ip -p 1883 -t test/topicIn another terminal:docker run --rm -it eclipse-mosquitto mosquitto_pub -h your_droplet_ip -p 1883 -t test/topic -m &quot;Hello&quot;Test the secured port:docker run --rm -it eclipse-mosquitto mosquitto_sub -h yourname.duckdns.org -p 8883 -u username -P password --cafile /etc/ssl/certs/ca-certificates.crt -t test/topic You may need to adjust the --cafile path depending on your client system’s CA store.10. Automate Certificate RenewalEdit docker-compose.yml to remove the 80:80 port mapping from Mosquitto (after certificates are obtained). Then, set up a cron job for renewal:crontab -eAdd:0 0 * * * docker-compose -f /home/youruser/mosquitto/docker-compose.yml run --rm certbot renew --quiet &amp;amp;&amp;amp; docker-compose -f /home/youruser/mosquitto/docker-compose.yml restart mosquitto Replace /home/youruser/mosquitto with your project path.11. Secure the Setup Remove the listener 1883 section from mosquitto.conf if you don’t need unsecured access. Restart the service: docker-compose down &amp;amp;&amp;amp; docker-compose up -d Troubleshooting Common Errors “Connection refused”: Check if ports are open (ufw and DigitalOcean firewall), certificates are in place, and the domain resolves to your droplet’s IP. “TLS error”: Ensure the certificate paths in mosquitto.conf match the files in ./letsencrypt/live/yourname.duckdns.org/. Certbot fails: Verify port 80 is accessible and the domain points to your droplet’s IP.Why This Approach? Docker Compose: Simplifies managing Mosquitto and Certbot together. Basic Authentication: Adds a security layer beyond TLS. Let’s Encrypt: Provides free, trusted certificates. DuckDNS: Free dynamic DNS that integrates well with home servers or droplets. Minimal Overhead: No reverse proxy (e.g., Nginx/Traefik) unless needed for additional services.Let me know your specific errors, and I’ll tailor this further!" }, { "title": "Access local services securel with Wireguard", "url": "/posts/access-local-services-securel-with-wireguard/", "categories": "DEV", "tags": "wireguard", "date": "2025-03-09 07:35:00 -0400", "snippet": "To use WireGuard to access services running inside a set of containers on a remote host with a Point-to-Point (P2P) topology, follow these steps:Use case 11. Understand the Setup Remote Host: The host running the containers. Local Machine: The machine from which you want to access the services. WireGuard: A VPN tool that creates a secure tunnel between the remote host and your local machine. Point-to-Point Topology: A direct connection between two endpoints (your local machine and the remote host).2. Install WireGuardInstall WireGuard on both the remote host and your local machine. Linux: sudo apt updatesudo apt install wireguard Windows/macOS: Download and install WireGuard from the official website: https://www.wireguard.com/install/.3. Generate WireGuard KeysOn both the remote host and local machine, generate private and public keys:wg genkey | tee privatekey | wg pubkey &amp;gt; publickey Save the private key securely. Share the public key with the other party.4. Configure WireGuard on the Remote HostCreate a WireGuard configuration file on the remote host (e.g., /etc/wireguard/wg0.conf):[Interface]Address = 10.0.0.1/24PrivateKey = &amp;lt;REMOTE_HOST_PRIVATE_KEY&amp;gt;ListenPort = 51820[Peer]PublicKey = &amp;lt;LOCAL_MACHINE_PUBLIC_KEY&amp;gt;AllowedIPs = 10.0.0.2/32 Replace &amp;lt;REMOTE_HOST_PRIVATE_KEY&amp;gt; with the private key of the remote host. Replace &amp;lt;LOCAL_MACHINE_PUBLIC_KEY&amp;gt; with the public key of your local machine. 10.0.0.1/24 is the IP address assigned to the remote host in the VPN network.5. Configure WireGuard on the Local MachineCreate a WireGuard configuration file on your local machine (e.g., /etc/wireguard/wg0.conf):[Interface]Address = 10.0.0.2/24PrivateKey = &amp;lt;LOCAL_MACHINE_PRIVATE_KEY&amp;gt;[Peer]PublicKey = &amp;lt;REMOTE_HOST_PUBLIC_KEY&amp;gt;Endpoint = &amp;lt;REMOTE_HOST_PUBLIC_IP&amp;gt;:51820AllowedIPs = 10.0.0.1/32PersistentKeepalive = 25 Replace &amp;lt;LOCAL_MACHINE_PRIVATE_KEY&amp;gt; with the private key of your local machine. Replace &amp;lt;REMOTE_HOST_PUBLIC_KEY&amp;gt; with the public key of the remote host. Replace &amp;lt;REMOTE_HOST_PUBLIC_IP&amp;gt; with the public IP address of the remote host. 10.0.0.2/24 is the IP address assigned to your local machine in the VPN network.6. Start WireGuard On the remote host: sudo wg-quick up wg0 On the local machine: sudo wg-quick up wg0 7. Configure Routing for ContainersEnsure the containers on the remote host can communicate over the WireGuard interface. If the containers are running on a bridge network, you may need to add a route to the WireGuard subnet (10.0.0.0/24) on the Docker bridge. Example: sudo ip route add 10.0.0.0/24 dev docker0 8. Access Services Use the IP address of the remote host in the VPN network (10.0.0.1) to access services running inside the containers. Example: If a container is running a web server on port 8080, you can access it from your local machine at http://10.0.0.1:8080.9. Persist ConfigurationTo ensure WireGuard starts automatically on boot: On Linux: sudo systemctl enable wg-quick@wg0 10. Test the Connection Use ping to test connectivity: ping 10.0.0.1 Check the WireGuard status: sudo wg Optional: Firewall ConfigurationEnsure the WireGuard port (51820 by default) is open on the remote host’s firewall. Example for ufw: sudo ufw allow 51820/udp Use case 2Yes, that’s correct! By following the steps outlined above, you can run Nextcloud (or any other service) in a container on a remote host and securely access it from any device connected to your WireGuard VPN. Here’s how it works specifically for your use case:How It Works for Nextcloud in a Container Nextcloud in a Container: You run Nextcloud in a Docker container on the remote host, exposing it on a specific port (e.g., 8080). WireGuard VPN: You set up WireGuard to create a secure tunnel between the remote host and your devices (e.g., laptop, phone, etc.). Access Nextcloud: Once connected to the WireGuard VPN, you can access Nextcloud using the private IP address assigned to the remote host in the VPN network (e.g., 10.0.0.1:8080).Steps Specific to Nextcloud Run Nextcloud in a Container: On the remote host, start a Nextcloud container: docker run -d \\ --name nextcloud \\ -p 8080:80 \\ nextcloud This exposes Nextcloud on port 8080 of the remote host. Configure WireGuard: Follow the steps in the previous answer to set up WireGuard on the remote host and your devices. Access Nextcloud: Once connected to the WireGuard VPN, open a browser on your device and navigate to: http://10.0.0.1:8080 This will load the Nextcloud interface. Access from Any DeviceTo allow any device to access Nextcloud via WireGuard: Install WireGuard on the Device: Install WireGuard on the device you want to use (e.g., laptop, phone, tablet). Use the same WireGuard configuration (with the device’s unique private/public key pair) to connect to the VPN. Add the Device as a Peer: On the remote host’s WireGuard configuration (/etc/wireguard/wg0.conf), add the new device as a peer: [Peer]PublicKey = &amp;lt;DEVICE_PUBLIC_KEY&amp;gt;AllowedIPs = 10.0.0.3/32 Replace &amp;lt;DEVICE_PUBLIC_KEY&amp;gt; with the public key of the new device. Assign a unique IP address (e.g., 10.0.0.3) to the device. Restart WireGuard: Restart WireGuard on the remote host to apply the changes: sudo wg-quick down wg0sudo wg-quick up wg0 Connect the Device: On the device, connect to the WireGuard VPN using its configuration. Once connected, you can access Nextcloud at http://10.0.0.1:8080. Benefits of This Setup Security: WireGuard encrypts all traffic between your devices and the remote host. Privacy: Your Nextcloud instance is not exposed to the public internet. Accessibility: You can access Nextcloud from any device connected to the VPN, whether you’re at home, at work, or on the go.Important Considerations Performance: WireGuard is lightweight and fast, but the speed of your connection will depend on the internet speed of the remote host and your device. Dynamic IP Address: If the remote host has a dynamic public IP address, consider using a Dynamic DNS (DDNS) service to map a domain name to the IP address. Firewall Rules: Ensure the WireGuard port (51820 by default) is open on the remote host’s firewall. Container Networking: If your containers are on a custom Docker network, ensure the WireGuard interface can route traffic to that network.By following these steps, you can securely run Nextcloud in a container on a remote host and access it from any device connected to your WireGuard VPN." }, { "title": "Common CLI on linux", "url": "/posts/common-cli/", "categories": "DEV", "tags": "cli", "date": "2025-03-09 07:35:00 -0400", "snippet": "Create non-root userconnecting to remote host with the same user on local machineTry rootadduser jmcusermod -aG sudo jmcsudo su jmcwhoamicd ~mkdir .sshchmod 700 .sshtouch .ssh/authorized_keyschmod 600 .ssh/authorized_keysfrom the localcat ~/.ssh/id_rsa.pubcopy public key and add it into authorized_keyscopy the public key to the remote host in authorized_keyswith root user add this commandusermod -aG sudo jmcnext command …" }, { "title": "Iot Project with ESP 32", "url": "/posts/iot-project/", "categories": "Iot", "tags": "docker, esp32, cam, sensors", "date": "2025-03-03 06:35:00 -0500", "snippet": "Thinking (brainstorming)Alright, let’s break this down. I’m building an IoT project with ESP controllers (DevKit and CAM) using C code. The data will be stored in InfluxDB, and I want to use Telegraf for data collection and Grafana for visualization. Everything should run in Docker Compose with Docker secrets for secure credential management.First, I need to set up the ESP controllers. Since I prefer C code, I’ll use the ESP-IDF framework for programming the ESP32 chips. For the DevKit, I might want to collect sensor data like temperature or humidity. For the CAM, I could capture images or video streams. I’ll need to write C code to interface with the sensors or camera and send the data to a server.Now, for the server side, I’ll use Docker Compose to manage the containers. The stack will include:InfluxDB: A time-series database to store the IoT data.Telegraf: An agent to collect data from various sources and send it to InfluxDB.Grafana: A visualization tool to create dashboards for the IoT data.Additionally, I might need an MQTT broker like Mosquitto to handle communication between the ESP devices and the server.Let me think about how to structure this.ESP Controllers:For the DevKit, I’ll write C code to read sensor data and publish it to an MQTT topic.For the CAM, I’ll write C code to capture images or video and send them to the server, possibly using HTTP or MQTT.Server Side:Mosquitto: An MQTT broker to receive data from the ESP devices.Telegraf: Configured to subscribe to MQTT topics and collect data, then send it to InfluxDB.InfluxDB: Stores the time-series data.Grafana: Connects to InfluxDB to visualize the data.Now, about Docker Compose and Docker secrets.I need to ensure that sensitive information like database credentials or API tokens are handled securely. Docker secrets are a good way to manage this.Here’s a rough plan:Create Docker secrets for sensitive information, such as InfluxDB credentials or MQTT passwords.Configure the services in Docker Compose to use these secrets.For example, InfluxDB can be configured with environment variables from secrets, and Telegraf can read its configuration from a file that uses secrets.Wait, actually, Telegraf has support for Docker secrets directly since version 1.27, as mentioned in one of the web results. That could be useful.Let me outline the steps more clearly.Step 1: Set up the ESP controllersWrite C code for the DevKit to read sensor data and publish to MQTT.Write C code for the CAM to capture images and send them, perhaps to a specific endpoint or MQTT topic.I need to decide on the data format. For time-series data, InfluxDB expects data in a specific format, so I should ensure that the data sent from the ESP devices can be easily parsed by Telegraf.Step 2: Set up the Docker environmentI’ll need a docker-compose.yml file that defines the services:MosquittoInfluxDBTelegrafGrafanaAdditionally, I’ll define secrets for credentials.For example:yamlCollapseWrapCopyversion: ‘3.7’secrets: influxdb_username: file: ./secrets/influxdb_username.txt influxdb_password: file: ./secrets/influxdb_password.txt mqtt_password: file: ./secrets/mqtt_password.txtservices: mosquitto: image: eclipse-mosquitto ports: - “1883:1883” volumes: - ./mosquitto/config:/mosquitto/config secrets: - mqtt_passwordinfluxdb: image: influxdb:2.7.1 ports: - “8086:8086” environment: DOCKER_INFLUXDB_INIT_MODE: setup DOCKER_INFLUXDB_INIT_USERNAME_FILE: /run/secrets/influxdb_username DOCKER_INFLUXDB_INIT_PASSWORD_FILE: /run/secrets/influxdb_password DOCKER_INFLUXDB_INIT_ORG: myorg DOCKER_INFLUXDB_INIT_BUCKET: mybucket secrets: - influxdb_username - influxdb_password volumes: - influxdb_data:/var/lib/influxdb2telegraf: image: telegraf:1.28 volumes: - ./telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:ro depends_on: - influxdb - mosquitto secrets: - mqtt_passwordgrafana: image: grafana/grafana:9.5.12 ports: - “3000:3000” volumes: - grafana_data:/var/lib/grafana depends_on: - influxdbvolumes: influxdb_data: grafana_data:Wait, I need to adjust this. For InfluxDB 2.x, the setup is a bit different. It uses tokens for authentication, not username and password directly. But for simplicity, maybe I can use InfluxDB 1.8, which supports username and password.Alternatively, I can stick with InfluxDB 2.x and manage tokens properly.Actually, in the web results, there are examples using InfluxDB 2.x with Docker secrets for the admin token.Let me check that.In one of the results, it shows how to use Docker secrets for InfluxDB 2.x:yamlCollapseWrapCopyservices: influxdb2: image: influxdb:2 ports: - 8086:8086 environment: DOCKER_INFLUXDB_INIT_MODE: setup DOCKER_INFLUXDB_INIT_USERNAME_FILE: /run/secrets/influxdb2-admin-username DOCKER_INFLUXDB_INIT_PASSWORD_FILE: /run/secrets/influxdb2-admin-password DOCKER_INFLUXDB_INIT_ADMIN_TOKEN_FILE: /run/secrets/influxdb2-admin-token DOCKER_INFLUXDB_INIT_ORG: docs DOCKER_INFLUXDB_INIT_BUCKET: home secrets: - influxdb2-admin-username - influxdb2-admin-password - influxdb2-admin-tokenSo, I can use this approach.For Telegraf, I need to configure it to connect to MQTT and InfluxDB.I can create a telegraf.conf file that uses the Docker secrets.Wait, with Telegraf 1.27 and later, there’s support for secret stores, including Docker secrets.So, in the telegraf.conf, I can define secrets like this:tomlCollapseWrapCopy[[inputs.mqtt_consumer]] servers = [“tcp://mosquitto:1883”] topics = [“sensors/#”] username = “[[DOCKER-SECRET:mqtt_username]]” password = “[[DOCKER-SECRET:mqtt_password]]”[[outputs.influxdb_v2]] urls = [“http://influxdb:8086”] token = “[[DOCKER-SECRET:influxdb_token]]” organization = “myorg” bucket = “mybucket”Then, in the Docker Compose file, I can define the secrets and make them available to Telegraf.Wait, actually, for Telegraf to use Docker secrets, I need to configure the secret store.According to the documentation, I can add a secret store configuration in telegraf.conf:tomlCollapseWrapCopy[[secretstores.docker]] id = “docker_secrets” container_id = “telegraf”Then, I can reference the secrets like:tomlCollapseWrapCopy[[inputs.mqtt_consumer]] servers = [“tcp://mosquitto:1883”] topics = [“sensors/#”] username = “[[SECRET:docker_secrets:mqtt_username]]” password = “[[SECRET:docker_secrets:mqtt_password]]”[[outputs.influxdb_v2]] urls = [“http://influxdb:8086”] token = “[[SECRET:docker_secrets:influxdb_token]]” organization = “myorg” bucket = “mybucket”And in the Docker Compose file, I need to mount the secrets into the Telegraf container.Wait, actually, with Docker Compose, when I define secrets in the service, they are mounted as files in /run/secrets/.But for Telegraf’s Docker secret store, it expects to read from the Docker daemon’s secret store.Wait, I think I need to clarify this.Upon checking the Telegraf documentation, the Docker secret store plugin reads secrets from the Docker daemon’s secret store, which in Docker Compose is managed through the secrets section.So, if I define secrets in Docker Compose, and reference them in the Telegraf service, Telegraf can access them using the Docker secret store plugin.For example:In docker-compose.yml:yamlCollapseWrapCopyservices: telegraf: image: telegraf:1.28 volumes: - ./telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:ro secrets: - mqtt_username - mqtt_password - influxdb_tokenThen, in telegraf.conf:tomlCollapseWrapCopy[[secretstores.docker]] id = “docker_secrets”[[inputs.mqtt_consumer]] servers = [“tcp://mosquitto:1883”] topics = [“sensors/#”] username = “[[SECRET:docker_secrets:mqtt_username]]” password = “[[SECRET:docker_secrets:mqtt_password]]”[[outputs.influxdb_v2]] urls = [“http://influxdb:8086”] token = “[[SECRET:docker_secrets:influxdb_token]]” organization = “myorg” bucket = “mybucket”Wait, actually, the secret store id might need to be specified correctly.Let me confirm the syntax.Upon checking the Telegraf documentation, the Docker secret store uses the container’s secrets mounted at /run/secrets/.So, in the telegraf.conf, I can define:tomlCollapseWrapCopy[[secretstores.docker]] id = “docker” path = “/run/secrets”Then, reference the secrets like:tomlCollapseWrapCopy[[inputs.mqtt_consumer]] servers = [“tcp://mosquitto:1883”] topics = [“sensors/#”] username = “[[SECRET:docker:mqtt_username]]” password = “[[SECRET:docker:mqtt_password]]”[[outputs.influxdb_v2]] urls = [“http://influxdb:8086”] token = “[[SECRET:docker:influxdb_token]]” organization = “myorg” bucket = “mybucket”Yes, that should work.So, in summary, for the server side:Use Docker Compose to define services: Mosquitto, InfluxDB, Telegraf, Grafana.Define secrets for sensitive credentials.Configure InfluxDB with initial setup using environment variables and secrets.Configure Telegraf to use the Docker secret store to access credentials for MQTT and InfluxDB.Configure Grafana to connect to InfluxDB.For Grafana, I might also need to provision data sources and dashboards, but that can be done later.Now, about the ESP controllers.I need to write C code for the ESP32 to connect to WiFi, connect to the MQTT broker, and publish data.For the DevKit, let’s say I have a temperature sensor. I’ll read the temperature and publish it to an MQTT topic, say “sensors/temperature”.For the CAM, perhaps I’ll capture an image periodically and send it to the server. But sending images over MQTT might not be efficient. Alternatively, I could use HTTP to upload the image to a web server, but since I’m using Docker Compose, I might need to add another service for handling image uploads.Wait, perhaps I can use Node-RED or something similar to handle incoming data, including images.But to keep it simple, maybe for the CAM, I’ll just send some metadata or a small thumbnail via MQTT, and handle full images separately if needed.Alternatively, I can use FTP or another protocol, but that might complicate things.For now, let’s assume that for the CAM, I’ll send some numerical data, like the number of detected objects or something, via MQTT.So, both ESP devices will publish data to MQTT topics.Then, Telegraf will subscribe to those topics and forward the data to InfluxDB.In Telegraf, I need to configure the MQTT input plugin and the InfluxDB output plugin.Also, I need to parse the MQTT messages into InfluxDB line protocol format.Assuming the ESP devices send data in a format that can be easily parsed, like JSON or key-value pairs.For example, the DevKit could send: “temperature=25.5”Then, Telegraf can use the parser processor to extract the value.Wait, actually, for InfluxDB, it’s better to send data in line protocol format directly from the ESP, but since I’m using C code, it might be simpler to send a simple string and let Telegraf handle the parsing.Alternatively, I can write the C code to format the data in line protocol.For example, the ESP could send: “sensors,device=devkit temperature=25.5”Then, Telegraf can directly forward that to InfluxDB.Yes, that might be efficient.So, in the C code, I’ll format the data as InfluxDB line protocol strings and publish them to MQTT.Then, in Telegraf, I can use the inputs.mqtt_consumer plugin with data_format = “influx” to directly parse the messages as line protocol.That way, Telegraf can send them straight to InfluxDB without additional processing.Perfect.Similarly for the CAM, if it’s sending numerical data.For example: “sensors,device=cam objects=3”So, in the telegraf.conf, I can have:tomlCollapseWrapCopy[[inputs.mqtt_consumer]] servers = [“tcp://mosquitto:1883”] topics = [“sensors/#”] data_format = “influx” username = “[[SECRET:docker:mqtt_username]]” password = “[[SECRET:docker:mqtt_password]]”[[outputs.influxdb_v2]] urls = [“http://influxdb:8086”] token = “[[SECRET:docker:influxdb_token]]” organization = “myorg” bucket = “mybucket”And define the secret store:tomlCollapseWrapCopy[[secretstores.docker]] id = “docker” path = “/run/secrets”Wait, actually, in the Telegraf documentation, the path is optional and defaults to /run/secrets, so I might not need to specify it.Let me confirm.Upon checking, yes, the default path is /run/secrets, so I can just define:tomlCollapseWrapCopy[[secretstores.docker]] id = “docker”Then use [[SECRET:docker:secret_name]].Great.Now, for the ESP code, I need to use an MQTT library.For ESP-IDF, there’s the official MQTT component.I can use that to connect to the MQTT broker and publish messages.I need to handle WiFi connection, MQTT connection, and publishing.Also, for security, I should use username and password for MQTT, which can be stored in the ESP’s NVS (Non-Volatile Storage) or hardcoded (though less secure).But since this is a prototype, I might hardcode them for now.Alternatively, I can use provisioning methods, but that might be overkill.So, in summary:Write C code for ESP32 DevKit and CAM to connect to WiFi, connect to MQTT with credentials, and publish data in InfluxDB line protocol format.Set up Docker Compose with Mosquitto, InfluxDB, Telegraf, and Grafana.Use Docker secrets to manage credentials for MQTT and InfluxDB.Configure Telegraf to use the Docker secret store and subscribe to MQTT topics, forwarding data to InfluxDB.Set up Grafana to connect to InfluxDB and create dashboards.Additionally, I might want to add persistence for InfluxDB and Grafana data, so I don’t lose data on container restarts.In the Docker Compose file, I already have volumes defined for that.Also, for Mosquitto, I might want to configure it to require authentication.I can create a mosquitto.conf file and a password file.For example, in ./mosquitto/config/mosquitto.conf:textCollapseWrapCopyallow_anonymous falsepassword_file /mosquitto/config/passwordfileThen, create a password file with usernames and hashed passwords.I can use the mosquitto_passwd utility to create the password file.Alternatively, since I’m using Docker secrets, I might be able to manage the password file as a secret.But for simplicity, I can create the password file on the host and mount it into the container.Wait, actually, to make it more secure, I can generate the password file and store it as a secret.But perhaps it’s easier to manage it as a file.Let me think.In the web results, there’s an example where the MQTT password is stored as a secret, and then used in the Telegraf configuration.Similarly, for Mosquitto, I can configure it to use a password file, and mount that file from a secret.Wait, in Docker Compose, secrets are mounted as files, so I can define a secret for the password file and mount it into Mosquitto.For example:In docker-compose.yml:yamlCollapseWrapCopysecrets: mqtt_passwordfile: file: ./secrets/mqtt_passwordfile.txtservices: mosquitto: image: eclipse-mosquitto ports: - “1883:1883” volumes: - ./mosquitto/config/mosquitto.conf:/mosquitto/config/mosquitto.conf secrets: - mqtt_passwordfileThen, in mosquitto.conf:textCollapseWrapCopyallow_anonymous falsepassword_file /run/secrets/mqtt_passwordfileYes, that should work.So, I need to create the mqtt_passwordfile.txt with the username and hashed password.I can use mosquitto_passwd to create it.For example, run:shCollapseWrapCopydocker run –rm -it eclipse-mosquitto mosquitto_passwd -c /tmp/passwordfile myuserThen, copy the /tmp/passwordfile to ./secrets/mqtt_passwordfile.txt.Alternatively, I can run mosquitto_passwd on the host if I have it installed.Once that’s set up, the ESP devices and Telegraf can connect to MQTT with the username and password.Similarly, for InfluxDB, I need to set up the initial user, organization, bucket, and token.In the Docker Compose file, I can use environment variables to set these, and use secrets for the sensitive parts.For example:yamlCollapseWrapCopyservices: influxdb: image: influxdb:2.7.1 ports: - “8086:8086” environment: DOCKER_INFLUXDB_INIT_MODE: setup DOCKER_INFLUXDB_INIT_USERNAME_FILE: /run/secrets/influxdb_username DOCKER_INFLUXDB_INIT_PASSWORD_FILE: /run/secrets/influxdb_password DOCKER_INFLUXDB_INIT_ADMIN_TOKEN_FILE: /run/secrets/influxdb_token DOCKER_INFLUXDB_INIT_ORG: myorg DOCKER_INFLUXDB_INIT_BUCKET: mybucket secrets: - influxdb_username - influxdb_password - influxdb_token volumes: - influxdb_data:/var/lib/influxdb2Then, create the secret files:./secrets/influxdb_username.txt: contains the username, e.g., “admin”./secrets/influxdb_password.txt: contains the password./secrets/influxdb_token.txt: contains the token, or leave it empty to let InfluxDB generate oneWait, actually, if I don’t specify the token, InfluxDB will generate one.But for Telegraf to connect, I need to know the token.So, perhaps I can generate the token separately and store it as a secret.Alternatively, I can run the InfluxDB container first, get the generated token, and then set it in the Telegraf configuration.But that might be manual.Wait, in the web results, there’s a way to retrieve the token after setup.For example, after starting InfluxDB, I can run a command to get the token.But since I’m using Docker Compose, I can perhaps add a step to extract the token and set it as an environment variable or something.Alternatively, I can use the same token for both InfluxDB setup and Telegraf.Wait, actually, during the initial setup, InfluxDB creates an operator token, which can be used by Telegraf.So, if I specify DOCKER_INFLUXDB_INIT_ADMIN_TOKEN_FILE, I can provide a token, and then use the same token in Telegraf.For example, create a token and store it in ./secrets/influxdb_token.txt, then reference it in both InfluxDB and Telegraf.But how do I generate the token beforehand?Perhaps I can generate a random token and use it.For example, in a script, generate a token and write it to the secret file.Then, use that token in Telegraf.Yes, that could work.So, before running docker-compose up, I can run a script to generate the secrets, including the InfluxDB token.For example:shCollapseWrapCopy#!/bin/bashGenerate InfluxDB tokenINFLUXDB_TOKEN=$(openssl rand -base64 32)echo $INFLUXDB_TOKEN &amp;gt; ./secrets/influxdb_token.txtSet other secretsecho “admin” &amp;gt; ./secrets/influxdb_username.txtecho “password” &amp;gt; ./secrets/influxdb_password.txtecho “mqtt_user” &amp;gt; ./secrets/mqtt_username.txtecho “mqtt_pass” &amp;gt; ./secrets/mqtt_password.txtCreate MQTT password filedocker run –rm -it eclipse-mosquitto mosquitto_passwd -b /tmp/passwordfile mqtt_user mqtt_passcp /tmp/passwordfile ./secrets/mqtt_passwordfile.txtWait, actually, mosquitto_passwd needs to be run inside the container or on a system with Mosquitto installed.Alternatively, I can use a Python script or something to hash the password.But for simplicity, perhaps I can hardcode the password file for now.Or, since I’m using Docker, I can run a temporary container to generate the password file.For example:shCollapseWrapCopydocker run –rm -v $(pwd)/secrets:/secrets eclipse-mosquitto mosquitto_passwd -c /secrets/mqtt_passwordfile.txt mqtt_userThen, it will prompt for the password, and I can enter it.But that’s interactive, which might not be ideal.Alternatively, I can use -b to specify the password in the command, but that’s insecure as it exposes the password in the command line.Perhaps I can use a script to automate it.But let’s assume that I create the password file manually for now.So, to summarize the setup process:Create secret files:./secrets/influxdb_username.txt: e.g., “admin”./secrets/influxdb_password.txt: e.g., “password”./secrets/influxdb_token.txt: e.g., a random string like “my_secure_token”./secrets/mqtt_username.txt: e.g., “mqtt_user”./secrets/mqtt_password.txt: e.g., “mqtt_pass”./secrets/mqtt_passwordfile.txt: the Mosquitto password file with hashed passwordCreate mosquitto.conf in ./mosquitto/config/mosquitto.conf:textCollapseWrapCopyallow_anonymous falsepassword_file /run/secrets/mqtt_passwordfileCreate telegraf.conf with the secret store and input/output plugins as above.Write the docker-compose.yml file as outlined.For the ESP devices, write C code to connect to WiFi, connect to MQTT with username “mqtt_user” and password “mqtt_pass”, and publish data in InfluxDB line protocol format to topics like “sensors/devkit” or “sensors/cam”.Start the Docker Compose stack: docker-compose up -dAccess Grafana at http://localhost:3000, log in with default credentials (admin/admin), and set up the InfluxDB data source using the token.Wait, for Grafana, I need to configure it to connect to InfluxDB.Since InfluxDB 2.x uses tokens, I need to provide the token to Grafana.I can do this manually after starting the stack, or I can provision Grafana with a data source configuration.For example, I can create a grafana/provisioning/datasources/influxdb.yml file:yamlCollapseWrapCopyapiVersion: 1datasources: name: InfluxDBtype: influxdbaccess: proxyurl: http://influxdb:8086jsonData: version: Flux organization: myorg defaultBucket: mybucketsecureJsonData: token: my_secure_tokenThen, in the Docker Compose file, mount this configuration:yamlCollapseWrapCopygrafana: image: grafana/grafana:9.5.12 ports: - “3000:3000” volumes: - grafana_data:/var/lib/grafana - ./grafana/provisioning:/etc/grafana/provisioning environment: GF_SECURITY_ADMIN_USER: admin GF_SECURITY_ADMIN_PASSWORD: adminBut wait, the token is sensitive, so I shouldn’t hardcode it in the configuration file.Alternatively, I can use environment variables in Grafana to set the token.But checking the Grafana documentation, it seems that for provisioning, the secureJsonData is used for sensitive information, but it’s still stored in the file.Perhaps for a production setup, I need to handle it differently, but for this prototype, I can accept that the token is in the configuration file.Alternatively, I can use a script to replace placeholders in the configuration file with the actual token from the secret.But that might be overkill.For now, I’ll assume that I manually configure Grafana after starting the stack.So, after starting Docker Compose, I can access Grafana, add the InfluxDB data source, and input the token manually.Later, I can automate it if needed.Therefore, the final setup is:ESP controllers programmed in C to send data via MQTT in InfluxDB line protocol.Docker Compose stack with Mosquitto, InfluxDB, Telegraf, and Grafana.Docker secrets used for credentials.Telegraf configured to use Docker secrets for MQTT and InfluxDB credentials.Grafana manually configured to connect to InfluxDB.This should provide a secure and functional IoT stack.I think that covers the main points.Let me just check if there’s anything missing.Ah, for the CAM, if I want to handle images, I might need additional components, like a web server to receive image uploads, but as mentioned earlier, for simplicity, I’ll assume it’s sending numerical data via MQTT.If needed, I can extend the stack later.Also, I should ensure that the ESP devices can connect to the MQTT broker running in Docker.Since the broker is exposed on port 1883, and assuming the Docker host is accessible from the ESP devices, it should be fine.I might need to use the Docker host’s IP address in the ESP code.Alternatively, if the ESP devices are on the same network, I can use mDNS or something, but for simplicity, I’ll hardcode the IP.Moreover, for production, I should consider security aspects like TLS for MQTT, but that complicates the setup, so I’ll leave it for now.In conclusion, this setup should work well for an IoT project with ESP controllers, using InfluxDB, Telegraf, and Grafana, all managed with Docker Compose and Docker secrets for secure credential handling.Based on the requirements for your IoT project, I’ve designed a technology stack that integrates ESP controllers (DevKit and CAM) with InfluxDB, Telegraf, and Grafana, all managed through Docker Compose with Docker secrets for secure credential handling. This setup ensures that your IoT data is collected, stored, and visualized efficiently while maintaining security best practices.Overview of the Technology StackESP Controllers (DevKit and CAM): Programmed in C using the ESP-IDF framework to collect sensor data (e.g., temperature from DevKit) and other metrics (e.g., object detection counts from CAM). The data is published to an MQTT broker.Docker Compose: Manages the containerized services, including Mosquitto (MQTT broker), InfluxDB (time-series database), Telegraf (data collection agent), and Grafana (visualization tool).Docker Secrets: Securely handles sensitive credentials such as MQTT passwords and InfluxDB tokens, ensuring they are not exposed in environment variables or configuration files.Step-by-Step Setup ESP Controllers (DevKit and CAM)Programming: Use C code with the ESP-IDF framework to program the ESP32-based DevKit and CAM.DevKit: Collect sensor data (e.g., temperature) and publish it to an MQTT topic (e.g., “sensors/devkit”).CAM: For simplicity, send numerical data (e.g., object detection counts) to an MQTT topic (e.g., “sensors/cam”). If needed, image handling can be added later.Data Format: Format the data in InfluxDB’s line protocol before publishing to MQTT. For example:DevKit: “sensors,device=devkit temperature=25.5”CAM: “sensors,device=cam objects=3”MQTT Connection: Connect to the MQTT broker using a username and password for secure access. Docker Compose SetupThe entire server-side stack is containerized and managed via Docker Compose. Below is an outline of the services and their configurations:Mosquitto (MQTT Broker):Handles communication between ESP devices and the server.Configured to require authentication using a password file managed via Docker secrets.InfluxDB:Stores time-series data from the ESP devices.Initialized with a secure admin username, password, and token using Docker secrets.Telegraf:Collects data from MQTT topics and forwards it to InfluxDB.Uses Docker secrets to securely access MQTT and InfluxDB credentials.Grafana:Visualizes the data stored in InfluxDB.Manually configured to connect to InfluxDB using the admin token. Secure Credential Management with Docker SecretsSecrets:influxdb_username, influxdb_password, influxdb_token: For InfluxDB admin access.mqtt_username, mqtt_password: For MQTT authentication.mqtt_passwordfile: Mosquitto password file for secure authentication.Usage:Secrets are mounted into the containers as files (e.g., /run/secrets/influxdb_token).Telegraf uses its Docker secret store plugin to access these secrets without exposing them in configuration files. Configuration FilesMosquitto Configuration (mosquitto.conf):Disallows anonymous access and uses a password file from Docker secrets.Telegraf Configuration (telegraf.conf):Configures the Docker secret store to access MQTT and InfluxDB credentials.Subscribes to MQTT topics and forwards data directly to InfluxDB using the influx data format. Running the StackStart the Stack: Use docker-compose up -d to launch all services.Access Grafana: Visit http://localhost:3000, log in with default credentials (e.g., admin/admin), and manually add InfluxDB as a data source using the admin token.Visualize Data: Create dashboards in Grafana to monitor the IoT data in real-time.Benefits of This StackScalability: Docker Compose makes it easy to manage and scale the services.Security: Docker secrets ensure that sensitive credentials are handled securely.Efficiency: Telegraf efficiently collects and forwards data to InfluxDB, while Grafana provides powerful visualization tools.Flexibility: The stack can be extended to include additional services (e.g., for image handling) as needed.This setup provides a robust, secure, and efficient solution for your IoT project, leveraging the strengths of each component while ensuring ease of management through Docker Compose.Let’s buid it with some requirementsBased on your requirements for an IoT project using ESP controllers (DevKit and CAM), C code, InfluxDB, Telegraf, Grafana, and a Docker Compose setup with Docker secrets, I’ve designed a complete technology stack that meets your needs. Below is a detailed, self-contained guide to set up this project, ensuring it works seamlessly for data collection, storage, and visualization while maintaining security through Docker secrets.Technology Stack OverviewThis stack integrates ESP controllers with a server-side setup running in Docker containers. Here’s how each component fits together: ESP Controllers (DevKit and CAM): Programmed in C using the ESP-IDF framework. Collect data (e.g., temperature from DevKit, object counts from CAM) and send it to an MQTT broker. Docker Compose Services: Mosquitto: An MQTT broker to handle communication between ESP devices and the server. InfluxDB: A time-series database to store IoT data. Telegraf: A data collection agent that subscribes to MQTT topics and forwards data to InfluxDB. Grafana: A visualization tool to create dashboards from InfluxDB data. Docker Secrets: Securely manages sensitive credentials like MQTT passwords and InfluxDB tokens.Step-by-Step Implementation1. Program the ESP ControllersYou’ll use C code with the ESP-IDF framework to program the ESP32-based DevKit and CAM controllers. ESP32 DevKit: Purpose: Collect sensor data (e.g., temperature). Action: Read data from a sensor (e.g., a temperature sensor) and publish it to an MQTT topic like &quot;sensors/devkit&quot;. Example Data: &quot;sensors,device=devkit temperature=25.5&quot;. ESP32 CAM: Purpose: For simplicity, send numerical data (e.g., number of detected objects) rather than full images. Action: Publish data to an MQTT topic like &quot;sensors/cam&quot;. Example Data: &quot;sensors,device=cam objects=3&quot;. Implementation: Use the ESP-IDF MQTT component to connect to the MQTT broker (e.g., running on your Docker host at port 1883). Format data in InfluxDB line protocol (as shown above) to simplify processing by Telegraf. Configure WiFi and MQTT credentials (username: mqtt_user, password: mqtt_pass) in your C code. For a prototype, these can be hardcoded, but consider using NVS (Non-Volatile Storage) for a more secure setup later. Sample C Code Outline: #include &quot;mqtt_client.h&quot;static esp_mqtt_client_handle_t client;void mqtt_event_handler(void *handler_args, esp_event_base_t base, int32_t event_id, void *event_data) { // Handle MQTT events (e.g., connected, data received)}void app_main() { // Initialize WiFi // Configure MQTT client esp_mqtt_client_config_t mqtt_cfg = { .broker.address.uri = &quot;mqtt://&amp;lt;docker-host-ip&amp;gt;:1883&quot;, .credentials.username = &quot;mqtt_user&quot;, .credentials.password = &quot;mqtt_pass&quot;, }; client = esp_mqtt_client_init(&amp;amp;mqtt_cfg); esp_mqtt_client_register_event(client, ESP_EVENT_ANY_ID, mqtt_event_handler, NULL); esp_mqtt_client_start(client); // Read sensor data and publish (example) char data[50]; snprintf(data, sizeof(data), &quot;sensors,device=devkit temperature=25.5&quot;); esp_mqtt_client_publish(client, &quot;sensors/devkit&quot;, data, 0, 1, 0);} 2. Set Up Docker ComposeThe server-side stack runs in Docker containers managed by a docker-compose.yml file. Create this file with the following content:version: &#39;3.7&#39;secrets: influxdb_username: file: ./secrets/influxdb_username.txt influxdb_password: file: ./secrets/influxdb_password.txt influxdb_token: file: ./secrets/influxdb_token.txt mqtt_username: file: ./secrets/mqtt_username.txt mqtt_password: file: ./secrets/mqtt_password.txt mqtt_passwordfile: file: ./secrets/mqtt_passwordfile.txtservices: mosquitto: image: eclipse-mosquitto:latest ports: - &quot;1883:1883&quot; volumes: - ./mosquitto/config/mosquitto.conf:/mosquitto/config/mosquitto.conf secrets: - mqtt_passwordfile influxdb: image: influxdb:2.7.1 ports: - &quot;8086:8086&quot; volumes: - influxdb_data:/var/lib/influxdb2 environment: DOCKER_INFLUXDB_INIT_MODE: setup DOCKER_INFLUXDB_INIT_USERNAME_FILE: /run/secrets/influxdb_username DOCKER_INFLUXDB_INIT_PASSWORD_FILE: /run/secrets/influxdb_password DOCKER_INFLUXDB_INIT_ADMIN_TOKEN_FILE: /run/secrets/influxdb_token DOCKER_INFLUXDB_INIT_ORG: myorg DOCKER_INFLUXDB_INIT_BUCKET: mybucket secrets: - influxdb_username - influxdb_password - influxdb_token telegraf: image: telegraf:1.28 volumes: - ./telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:ro depends_on: - mosquitto - influxdb secrets: - mqtt_username - mqtt_password - influxdb_token grafana: image: grafana/grafana:9.5.12 ports: - &quot;3000:3000&quot; volumes: - grafana_data:/var/lib/grafana depends_on: - influxdbvolumes: influxdb_data: grafana_data: Notes: Replace &amp;lt;docker-host-ip&amp;gt; in the ESP code with the actual IP address of your Docker host. Volumes ensure data persistence for InfluxDB and Grafana. 3. Configure Mosquitto Create a mosquitto.conf file in ./mosquitto/config/: allow_anonymous falsepassword_file /run/secrets/mqtt_passwordfile This enforces authentication using a password file managed as a Docker secret.4. Configure Telegraf Create a telegraf.conf file in ./telegraf/: [[secretstores.docker]] id = &quot;docker&quot;[[inputs.mqtt_consumer]] servers = [&quot;tcp://mosquitto:1883&quot;] topics = [&quot;sensors/#&quot;] data_format = &quot;influx&quot; username = &quot;[[SECRET:docker:mqtt_username]]&quot; password = &quot;[[SECRET:docker:mqtt_password]]&quot;[[outputs.influxdb_v2]] urls = [&quot;http://influxdb:8086&quot;] token = &quot;[[SECRET:docker:influxdb_token]]&quot; organization = &quot;myorg&quot; bucket = &quot;mybucket&quot; Explanation: The secretstores.docker section enables Telegraf to read Docker secrets from /run/secrets/. The inputs.mqtt_consumer plugin subscribes to MQTT topics and parses data in InfluxDB line protocol format. The outputs.influxdb_v2 plugin sends data to InfluxDB using the admin token. 5. Prepare Docker Secrets Create a secrets/ directory and populate the secret files: influxdb_username.txt: admin influxdb_password.txt: password123 influxdb_token.txt: my_secure_token (or generate a random string, e.g., via openssl rand -base64 32) mqtt_username.txt: mqtt_user mqtt_password.txt: mqtt_pass mqtt_passwordfile.txt: A Mosquitto password file with the hashed password for mqtt_user. Generate the MQTT Password File:Run this command to create the password file (requires Mosquitto installed locally or use a Docker container): docker run --rm -v $(pwd)/secrets:/secrets eclipse-mosquitto mosquitto_passwd -c /secrets/mqtt_passwordfile.txt mqtt_user mqtt_pass 6. Launch the Stack Run the Docker Compose stack: docker-compose up -d This starts all services in the background.7. Configure Grafana Access Grafana at http://localhost:3000. Log in with default credentials (admin/admin). Add an InfluxDB data source: URL: http://influxdb:8086 Organization: myorg Bucket: mybucket Token: Copy the value from secrets/influxdb_token.txt (e.g., my_secure_token). Create dashboards to visualize your IoT data (e.g., temperature trends from DevKit, object counts from CAM).How It All Works Together ESP Controllers: Publish data in InfluxDB line protocol to MQTT topics (e.g., &quot;sensors/devkit&quot;, &quot;sensors/cam&quot;). Mosquitto: Receives MQTT messages from the ESP devices and enforces authentication. Telegraf: Subscribes to MQTT topics, parses the line protocol data, and forwards it to InfluxDB using secure credentials from Docker secrets. InfluxDB: Stores the time-series data in the mybucket bucket. Grafana: Queries InfluxDB to display the data in real-time dashboards. Additional Notes Security: For a production setup, consider adding TLS for MQTT and InfluxDB. This prototype uses plain text for simplicity. ESP Connectivity: Ensure your ESP devices can reach the Docker host’s IP on port 1883. Extending the Stack: If you need to handle images from the ESP32 CAM, add a web server service to the Docker Compose file and adjust the ESP code to upload images via HTTP.This setup provides a robust, secure, and efficient IoT solution that leverages InfluxDB, Telegraf, Grafana, and ESP controllers, all managed seamlessly with Docker Compose and Docker secrets. Let me know if you need further clarification!What we can do with small budgetThinking …Alright, let’s break this down. I need to choose a technology stack for an IoT project, and I’m on a tight budget. That means I have to be smart about what I pick—something that’s cost-effective but still reliable and scalable. Let’s think through the key components I’ll need:Hardware: I’ll need IoT devices to collect data. Since I’m on a budget, I should look for affordable yet capable hardware. Something like Raspberry Pi or Arduino boards could work. Raspberry Pi is a bit more powerful and can handle more complex tasks, but Arduino is cheaper and great for simpler sensor-based projects. I should decide based on the specific needs of my project.Connectivity: How will my devices connect to the internet or to each other? Wi-Fi is common, but it can be power-hungry. If my devices are battery-powered, I might want to consider low-power options like LoRaWAN or Zigbee. However, these might require additional hardware like gateways, which could add to the cost. Bluetooth Low Energy (BLE) is another option for short-range communication.Data Storage and Processing: I need a place to store the data my devices collect. Cloud services are convenient, but they can get expensive with lots of data. Since I’m on a budget, I might want to consider self-hosting a database on a local server or using a low-cost cloud provider. For processing, I could use something like Node-RED for simple automation or a more robust platform like AWS IoT or Google Cloud IoT if I need more features.Security: IoT devices can be vulnerable to attacks, so I need to make sure my stack includes good security practices. This might include encrypted communication, secure authentication, and regular updates. Since I’m on a budget, I might need to handle some of this myself rather than relying on expensive managed services.Scalability: Even though I’m starting small, I should think about whether my stack can scale if my project grows. Using open-source software and modular hardware can help with this.Let me consider a specific example to make this more concrete. Suppose I want to build a smart home system that monitors temperature and humidity in different rooms. Here’s how I might approach it:Hardware: I’d use Arduino boards with temperature and humidity sensors. They’re cheap and easy to program.Connectivity: Since it’s a home environment, Wi-Fi should be fine. I can use the ESP8266 module with Arduino for Wi-Fi connectivity.Data Storage and Processing: I could set up a local server using a Raspberry Pi to collect data from the Arduino boards. I’d use a lightweight database like SQLite or InfluxDB to store the data. For processing, I could use Node-RED to create simple automation rules, like sending an alert if the temperature gets too high.Security: I’d make sure to secure the Wi-Fi network with a strong password and use HTTPS for any web interfaces. I’d also keep the software up to date to patch any vulnerabilities.Scalability: If I want to add more sensors or rooms, I can easily add more Arduino boards and update the Node-RED flows.This seems like a solid, budget-friendly stack for a simple IoT project. But let’s think about whether there are even cheaper or more efficient options.Another approach could be to use ESP32 boards, which have built-in Wi-Fi and are more powerful than Arduino. They’re a bit more expensive, but they might simplify the setup since I wouldn’t need separate Wi-Fi modules. Plus, they can run more complex code, which could be useful if my project grows.For data storage, I could consider using a cloud service with a free tier, like AWS IoT Core or Google Cloud IoT Core. These offer basic features for free, which could be enough for a small project. However, I need to be careful about potential costs if my usage exceeds the free tier limits.Wait a minute, since I’m on a tight budget, I should probably avoid cloud services altogether to prevent unexpected costs. Self-hosting on a local server is more predictable in terms of cost, even if it requires a bit more setup.Let me also consider the software side. For the local server, I could use a Linux-based OS on the Raspberry Pi, which is free. Then, I can install open-source software like Mosquitto for MQTT communication, InfluxDB for time-series data storage, and Grafana for visualization. All of these are free and widely used in IoT projects.MQTT is a lightweight messaging protocol that’s perfect for IoT devices with limited resources. It allows my devices to publish data to a broker, and then other services can subscribe to that data. This decouples the devices from the data processing, which is good for scalability.So, to summarize, here’s a potential stack:Hardware: ESP32 boards with sensorsConnectivity: Wi-Fi, using MQTT for communicationData Storage: InfluxDB on a Raspberry PiData Processing: Node-RED for automationVisualization: Grafana for dashboardsSecurity: Secure Wi-Fi, encrypted communication (e.g., TLS for MQTT), regular updatesThis stack is cost-effective because:ESP32 boards are relatively cheap.Raspberry Pi is affordable and can serve multiple purposes.All software is open-source and free.No ongoing cloud costs.However, I should consider the time and effort required to set this up. Since I’m on a budget, I might be doing most of the work myself, so I need to make sure I have the necessary skills or the willingness to learn.Alternatively, if I’m not comfortable with setting up a local server, I could look into IoT platforms that offer low-cost or free tiers. For example, Blynk is a popular platform for DIY IoT projects that offers a free tier with limited features. It provides a mobile app for controlling and monitoring devices, which could save me the trouble of building my own interface.But again, with a tight budget, I need to be cautious about potential costs. Some platforms might charge based on the number of devices or data usage, which could add up.Let me think about another option: using a microcontroller with built-in cloud connectivity. For instance, Particle.io offers devices that connect directly to their cloud platform. They have a free tier for up to 100 devices, which might be sufficient for a small project. However, their hardware is more expensive than ESP32 or Arduino, so the initial cost might be higher.Wait, actually, upon checking, Particle.io’s Photon board is around $19, which is comparable to an ESP32. But I think their free tier has limitations on data usage or features, so I need to check the details.Upon further reflection, I realize that for a truly budget-friendly solution, sticking with open-source hardware and software is probably the best bet. It gives me full control and avoids any subscription fees.One more thing to consider is power consumption. If my devices are battery-powered, I need to choose hardware and connectivity options that are energy-efficient. For example, using BLE or LoRaWAN instead of Wi-Fi could extend battery life significantly. However, this might require additional hardware, like a gateway, which could increase costs.In my smart home example, since the devices can be plugged in or use rechargeable batteries, Wi-Fi should be fine.Let me also think about the development tools. For programming the ESP32, I can use the Arduino IDE or the ESP-IDF framework. Both are free, and there are plenty of tutorials and libraries available.For the Raspberry Pi, I can use Raspbian OS, which is also free.So, in terms of software development, there are no additional costs.Wait, actually, if I choose to use a paid IDE or development tools, that could add to the cost. But since there are free options available, I can avoid that.Another aspect is maintenance. Over time, I might need to update the software or replace hardware components. With a self-hosted solution, I’m responsible for maintenance, which could require time and effort. But since it’s a small project, it should be manageable.In contrast, with a cloud-based platform, the provider handles maintenance, but I might have less control and could face costs if I need to scale.Given all this, I think the most budget-friendly technology stack for my IoT project is:Hardware: ESP32 boards with appropriate sensorsConnectivity: Wi-Fi with MQTT protocolData Storage: InfluxDB on a Raspberry PiData Processing: Node-REDVisualization: GrafanaSecurity: Implement best practices like secure passwords, encryption, and regular updatesThis stack leverages affordable hardware, free and open-source software, and avoids ongoing cloud costs. It also provides a good balance between functionality and complexity, making it suitable for a small budget project.Of course, the exact choice might vary depending on the specific requirements of the project. For instance, if I need to cover a large area, I might need to consider different connectivity options. Or if the project involves real-time control, I might need a different processing setup.But for a general IoT project on a tight budget, this stack should work well.Wait, one more thing: I should consider the cost of the Raspberry Pi. While it’s affordable, if I already have an old computer or a low-cost server, I could use that instead to save money. But for most people, a Raspberry Pi is a cost-effective choice.Alternatively, if the project is very small, I could even run the server software on my personal computer, but that might not be practical for continuous operation.In conclusion, by carefully selecting hardware and software components that are affordable and open-source, I can build a functional IoT system without breaking the bank.For an IoT project on a tight budget, the most cost-effective technology stack leverages affordable hardware, open-source software, and self-hosted solutions to avoid ongoing cloud fees. Below is a recommended stack that balances functionality, scalability, and minimal costs:Recommended Technology StackHardware:ESP32 boards (or Arduino with ESP8266 for Wi-Fi)Affordable, widely used, and capable of handling various sensors (e.g., temperature, humidity).ESP32 offers built-in Wi-Fi and more processing power than basic Arduino boards.Connectivity:Wi-Fi with MQTT protocolWi-Fi is suitable for most environments (e.g., home, office).MQTT is lightweight, efficient for IoT devices, and works well with limited resources.For battery-powered devices, consider BLE or LoRaWAN (though they may require additional hardware like gateways).Data Storage:InfluxDB (self-hosted on a Raspberry Pi or low-cost server)A free, open-source time-series database optimized for IoT data.Alternatives: SQLite (for smaller projects) or PostgreSQL.Data Processing:Node-REDA free, open-source tool for wiring together devices, APIs, and services.Ideal for simple automation and data flow management.Visualization:GrafanaA free, open-source platform for creating dashboards to monitor and visualize IoT data.Security:Implement best practices:Secure Wi-Fi with strong passwords.Use TLS for encrypted MQTT communication.Regularly update software and firmware.Why This Stack?Cost-Effective Hardware: ESP32 boards are inexpensive (~$5–$10) and versatile.No Cloud Fees: Self-hosting on a Raspberry Pi (~$35–$50) or an existing computer avoids subscription costs.Open-Source Software: InfluxDB, Node-RED, and Grafana are free, widely supported, and scalable.Scalability: Easily add more devices or sensors without significant cost increases.Security: Basic security measures can be implemented without additional cost.Example Project: Smart Home MonitoringDevices: ESP32 boards with temperature/humidity sensors.Connectivity: Wi-Fi + MQTT to publish sensor data.Storage: InfluxDB on a Raspberry Pi to store time-series data.Processing: Node-RED for automation (e.g., alerts if temperature exceeds a threshold).Visualization: Grafana dashboards to monitor room conditions.Alternatives (If Needed)Cloud Services: Use free tiers of AWS IoT Core or Google Cloud IoT for small-scale projects, but monitor usage to avoid unexpected costs.Platforms: Blynk or Particle.io offer free tiers but may have limitations on devices or data.This stack ensures you stay within budget while building a reliable, scalable IoT system. Adjust based on specific project needs, such as power constraints or coverage area.RecommendationThanks for sharing your thoughts! Using a home server with Docker Compose to host your services, including Telegraf, is a fantastic idea for your IoT project. It’s a cost-effective and flexible approach that aligns well with managing services on a budget. Let’s break down why this works and go over some key considerations to ensure everything runs smoothly.Why This Approach is Great Cost-Effective: Hosting on a home server avoids cloud service fees, and Docker Compose simplifies managing multiple services without extra cost or complexity. Modular and Scalable: Docker Compose lets you define and run multi-container applications, so you can easily add Telegraf or other services as your project evolves. Perfect for IoT Data Collection: Telegraf is an excellent choice for gathering metrics from IoT devices and sending them to a database like InfluxDB. It supports various input plugins (e.g., MQTT, HTTP), making it adaptable to your devices.How to Set It UpHere’s how you can integrate Telegraf into your Docker Compose setup:1. Adding Telegraf to Docker ComposeYou can define Telegraf as a service in your docker-compose.yml file. Here’s a basic example including Telegraf, InfluxDB (for storing data), and Grafana (for visualization):version: &#39;3.7&#39;services: influxdb: image: influxdb:2.7 ports: - &quot;8086:8086&quot; volumes: - influxdb_data:/var/lib/influxdb2 environment: DOCKER_INFLUXDB_INIT_MODE: setup DOCKER_INFLUXDB_INIT_USERNAME: admin DOCKER_INFLUXDB_INIT_PASSWORD: password DOCKER_INFLUXDB_INIT_ORG: myorg DOCKER_INFLUXDB_INIT_BUCKET: mybucket telegraf: image: telegraf:1.28 network_mode: host # Allows access to your local network volumes: - ./telegraf.conf:/etc/telegraf/telegraf.conf:ro depends_on: - influxdb grafana: image: grafana/grafana:9.5.12 ports: - &quot;3000:3000&quot; volumes: - grafana_data:/var/lib/grafana depends_on: - influxdbvolumes: influxdb_data: grafana_data: Notes: Replace ./telegraf.conf with the path to your Telegraf configuration file (more on this below). Adjust environment variables for InfluxDB as needed—consider using secrets for sensitive data. 2. Configuring TelegrafTelegraf needs a configuration file (telegraf.conf) to specify what data to collect and where to send it. Here’s an example for IoT devices sending data via MQTT:[[inputs.mqtt_consumer]] servers = [&quot;tcp://&amp;lt;your-mqtt-broker&amp;gt;:1883&quot;] topics = [&quot;sensors/#&quot;] data_format = &quot;json&quot; # Match your devices’ data format[[outputs.influxdb_v2]] urls = [&quot;http://influxdb:8086&quot;] token = &quot;$INFLUX_TOKEN&quot; organization = &quot;myorg&quot; bucket = &quot;mybucket&quot; Input Plugins: Choose plugins based on your IoT devices. For example: MQTT: inputs.mqtt_consumer HTTP: inputs.http_listener_v2 Output Plugins: Use outputs.influxdb_v2 to send data to InfluxDB.Key ConsiderationsTo make sure your setup runs smoothly, here are some important points:1. Networking Challenge: Your IoT devices are likely on your local network, while Docker containers have their own network by default. Solution: Use network_mode: host in the Telegraf service (as shown above) so it can directly access devices on your home network. If you prefer isolated networks, map ports or ensure devices can reach container IPs.2. Data Persistence Why: You don’t want to lose data if a container restarts. Solution: Use Docker volumes for InfluxDB (and Grafana if you use it), as shown in the example above.3. Security Network Security: Keep your home server off the public internet unless necessary. For remote access, use a VPN or secure tunnel. Credentials: Manage sensitive info (e.g., InfluxDB tokens) with Docker secrets or environment variables, not hardcoded in files. Updates: Regularly update Docker images and software to stay secure.4. Resource Management Why: A home server might have limited resources (e.g., CPU, memory). Solution: Monitor usage with docker stats and set limits if needed: telegraf: deploy: resources: limits: cpus: &#39;0.5&#39; memory: 512M 5. Visualization Suggestion: Add Grafana to visualize your IoT data. It pairs well with InfluxDB and Telegraf, giving you real-time dashboards (included in the example above).Final ThoughtsYour plan to use a home server with Docker Compose and Telegraf is a solid foundation for your IoT project. It’s budget-friendly, manageable, and scalable. Just focus on networking, security, and resource usage, and you’ll have a reliable system for collecting and analyzing IoT data. If you need help tweaking configs or troubleshooting, feel free to reach out—I’m happy to assist!Why Node red is not includedThinking …Let me clarify why Node-RED isn’t explicitly mentioned in the latest stack.First, Telegraf and Node-RED serve different primary purposes in an IoT setup. Telegraf is mainly a data collection agent that gathers metrics from various sources and sends them to databases like InfluxDB. It’s lightweight and efficient for this task.Node-RED, on the other hand, is a flow-based programming tool that’s great for creating automation workflows, handling data processing, and integrating different services. It can be used for data collection, but it’s more versatile and resource-intensive compared to Telegraf.In the latest stack, Telegraf is used for its efficiency in collecting and forwarding data to InfluxDB. However, if you need more complex data processing or automation (like triggering actions based on sensor data), you can still include Node-RED in your Docker Compose setup alongside Telegraf.So, to answer your question: Node-RED isn’t included because Telegraf handles the data collection efficiently, but you can add Node-RED if your project requires additional processing or automation features.Based on your question, it seems you’re wondering why Node-RED isn’t included in the latest technology stack, especially since Telegraf is being used. Let me clarify the roles of these tools and explain how they fit into your IoT project.Key PointsTelegraf is primarily a data collection agent. It efficiently gathers metrics from various sources (like IoT devices via MQTT) and sends them to databases like InfluxDB. It’s lightweight and optimized for this specific task.Node-RED is a flow-based programming tool used for automation and data processing. It allows you to create workflows that can handle tasks like triggering actions based on sensor data, integrating with other services, or even collecting data. However, it’s more versatile and resource-intensive compared to Telegraf.In the latest stack, Telegraf is included because it excels at collecting and forwarding data to InfluxDB with minimal overhead. This makes it a great choice for efficiently managing data from your IoT devices.However, if your project requires more complex functionality—such as processing data, automating responses (e.g., sending alerts when a sensor value exceeds a threshold), or integrating with other services—you can still add Node-RED to your Docker Compose setup alongside Telegraf.Should You Include Node-RED?If your project only needs data collection and storage, Telegraf paired with InfluxDB and Grafana is sufficient and efficient.If you need automation or more advanced data processing, adding Node-RED is a good idea. It can work in parallel with Telegraf or even replace it for certain tasks, depending on your needs.Here’s how you can include Node-RED in your Docker Compose setup:yamlCollapseWrapCopyservices: nodered: image: nodered/node-red:latest ports: - “1880:1880” volumes: - nodered_data:/datavolumes: nodered_data:Access Node-RED at http://:1880 to create and manage your flows.Final AnswerNode-RED wasn’t included in the latest stack because Telegraf efficiently handles data collection for your IoT devices. However, if your project requires additional automation or data processing, you can easily add Node-RED to your Docker Compose setup to complement Telegraf. Let me know if you’d like help integrating it!Answer to Your QueryYou’re wondering why Node-RED isn’t included in the latest stack and whether it’s because Telegraf is being used instead. Let me explain the reasoning behind this and how these tools fit into an IoT setup.Why Node-RED Isn’t IncludedNode-RED isn’t part of the latest stack because Telegraf is handling the data collection role efficiently. Telegraf is a lightweight data collection agent designed to gather metrics from various sources—like IoT devices via MQTT—and send them to databases such as InfluxDB. It’s optimized for this specific task, making it a streamlined choice for collecting and forwarding data without unnecessary overhead.In this setup, Telegraf works alongside InfluxDB (for storing data) and Grafana (for visualizing it), providing a simple and effective solution for monitoring IoT device metrics.How Node-RED Differs from TelegrafWhile Telegraf focuses on data collection, Node-RED serves a different purpose. It’s a flow-based programming tool that excels at creating automation workflows, processing data, and integrating with various services. For example, Node-RED can: Trigger actions (e.g., send an alert when a sensor value exceeds a threshold). Transform or enrich data before storing it. Connect multiple systems (e.g., link IoT devices to APIs or external services).Although Node-RED can collect data, it’s more versatile and resource-intensive than Telegraf, which is why it’s not included by default when the primary need is straightforward data collection.Can You Still Use Node-RED?Absolutely! If your project requires more than just data collection—such as automation or advanced data processing—you can add Node-RED to your setup alongside Telegraf. They can complement each other: Telegraf collects raw data and sends it to InfluxDB. Node-RED processes that data or handles additional tasks like automation.Here’s an example of how to add Node-RED to your Docker Compose file:services: nodered: image: nodered/node-red:latest ports: - &quot;1880:1880&quot; # Access Node-RED at http://&amp;lt;your-server-ip&amp;gt;:1880 volumes: - nodered_data:/datavolumes: nodered_data:Once added, you can access Node-RED’s web interface to build your workflows.When to Include Node-RED Stick with Telegraf alone if you only need to collect and store data for visualization (e.g., with InfluxDB and Grafana). Add Node-RED if you need automation, data transformation, or integration with other services beyond what Telegraf provides.SummaryNode-RED isn’t in the latest stack because Telegraf efficiently covers the data collection needs. However, it’s not an either/or choice—you can include Node-RED if your project demands its extra capabilities. Let me know if you’d like help setting it up or tailoring it to your needs!" }, { "title": "Manage, Encrypt and Decrypt with gpg Keys", "url": "/posts/manage-encrypt-decrypt-with-gpg-key/", "categories": "Security", "tags": "gpg", "date": "2025-03-02 09:35:00 -0500", "snippet": "Managing, encrypting, and decrypting with a GPG key involves using the gpg command-line tool to handle your keys, secure data, and recover it. Below, I’ll break this into three sections: Managing Keys, Encrypting, and Decrypting. I’ll assume you’ve already created a GPG key pair (if not, refer to my earlier response on creating a key).1. Managing GPG KeysManaging keys includes listing, editing, adding subkeys, exporting, importing, and revoking them. Here are common tasks:List Keys Public keys in your keyring: gpg --list-keys Example output: pub rsa4096 2025-03-03 [SC] [expires: 2027-03-03] 1234567890ABCDEF1234567890ABCDEF12345678uid [ultimate] Alice Smith &amp;lt;alice@example.com&amp;gt;sub rsa4096 2025-03-03 [E] Secret keys (private keys): gpg --list-secret-keys Example: sec rsa4096 2025-03-03 [SC] [expires: 2027-03-03] 1234567890ABCDEF1234567890ABCDEF12345678ssb rsa4096 2025-03-03 [E] Edit a KeyTo modify a key (e.g., add subkeys, change expiration, or passphrase):gpg --edit-key 1234567890ABCDEF1234567890ABCDEF12345678 Inside the prompt: adduid: Add a new user ID. addkey: Add a subkey (e.g., for signing or encryption). expire: Change expiration date. passwd: Change passphrase. save: Save changes and exit. Example: Add a signing subkey:gpg&amp;gt; addkeyPlease select what kind of key you want: (3) DSA (sign only) (4) RSA (sign only)Your selection? 4What keysize do you want? (2048) 4096Key is valid for? (0) 2ygpg&amp;gt; saveExport Keys Public key: gpg --armor --export 1234567890ABCDEF1234567890ABCDEF12345678 &amp;gt; alice_public.asc Private key (includes subkeys): gpg --armor --export-secret-keys 1234567890ABCDEF1234567890ABCDEF12345678 &amp;gt; alice_private.asc Import Keys Public key: gpg --import alice_public.asc Private key: gpg --import alice_private.asc Revoke a KeyIf a key is compromised: Import the revocation certificate (created during key generation): gpg --import 1234567890ABCDEF1234567890ABCDEF12345678.rev Publish the revocation: gpg --keyserver hkps://keys.openpgp.org --send-keys 1234567890ABCDEF1234567890ABCDEF12345678 2. Encrypting with a GPG KeyEncryption secures data so only the intended recipient (with the private key) can decrypt it.Encrypt a File Identify the recipient’s public key: gpg --list-keys Example: Bob’s key is 9876543210FEDCBA9876543210FEDCBA98765432. Encrypt the file for Bob: gpg --armor --encrypt --recipient 9876543210FEDCBA9876543210FEDCBA98765432 myfile.txt --armor: ASCII output (creates myfile.txt.asc). --recipient (or -r): Specifies the recipient’s key. Output: myfile.txt.asc (or .gpg without --armor). Optionally, encrypt for multiple recipients: gpg --armor --encrypt -r 9876543210FEDCBA9876543210FEDCBA98765432 -r alice@example.com myfile.txt Encrypt and SignTo encrypt and sign in one step (proving it’s from you):gpg --armor --encrypt --sign -r 9876543210FEDCBA9876543210FEDCBA98765432 myfile.txt You’ll be prompted for your passphrase.Encrypt a MessageTo encrypt text directly:gpg --armor --encrypt -r 9876543210FEDCBA9876543210FEDCBA98765432Type your message, then press Ctrl+D (Unix) or Ctrl+Z (Windows).3. Decrypting with a GPG KeyDecryption recovers the original data using your private key.Decrypt a FileIf someone encrypted a file for you (using your public key):gpg --decrypt myfile.txt.asc &amp;gt; myfile.txt You’ll be prompted for your passphrase. Output: The decrypted content is written to myfile.txt.Decrypt and VerifyIf the file was signed and encrypted:gpg --decrypt myfile.txt.asc &amp;gt; myfile.txt GPG will verify the signature if the signer’s public key is in your keyring. Example output: gpg: encrypted with 4096-bit RSA key, ID FEDCBA9876543210, created 2025-03-03 &quot;Alice Smith &amp;lt;alice@example.com&amp;gt;&quot;gpg: Signature made Mon 03 Mar 2025 12:00:00 PM ESTgpg: using RSA key 1234567890ABCDEF1234567890ABCDEF12345678gpg: Good signature from &quot;Alice Smith &amp;lt;alice@example.com&amp;gt;&quot; [ultimate] Decrypt Without SavingTo view the decrypted content without saving:gpg --decrypt myfile.txt.ascAdditional Tips Passphrase Caching: Use gpg-agent to avoid repeated passphrase prompts: gpg-agent --daemon Key Trust: If verifying a signature fails, ensure you trust the signer’s key: gpg --edit-key 1234567890ABCDEF1234567890ABCDEF12345678gpg&amp;gt; trust Symmetric Encryption: Encrypt without keys (using a passphrase only): gpg --armor --symmetric myfile.txt " }, { "title": "How to create gpg key", "url": "/posts/create-gpg/", "categories": "Security", "tags": "gpg", "date": "2025-03-02 07:35:00 -0500", "snippet": "Creating a GPG key involves generating a primary key pair (public and private keys) and optionally adding subkeys for specific purposes like encryption or signing. Below are the steps to create a GPG key using the gpg command-line tool. I’ll assume you have GPG installed on your system. If not, let me know your operating system, and I can guide you through the installation.Steps to Create a GPG Key Start the Key Generation ProcessOpen a terminal and run: gpg --full-generate-key This launches an interactive key generation wizard. Choose Key TypeYou’ll see a prompt like this: Please select what kind of key you want: (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (sign only) (4) RSA (sign only)Your selection? Press 1 (or Enter) for the default RSA and RSA. This creates an RSA primary key (for signing and certification) and an RSA subkey (for encryption). Other options are less common today; RSA is widely supported and secure. Specify Key SizeNext, you’ll be asked for the key size: Please specify how long the key should be: (2048) RSA keys may be between 1024 and 4096 bits long.What keysize do you want? (2048) Enter 4096 for maximum security (recommended for modern use) or press Enter for the default (2048). Larger keys (e.g., 4096 bits) are more secure but slower. Set Key ExpirationDecide how long the key should be valid: Please specify how long the key should be valid. 0 = key does not expire &amp;lt;n&amp;gt; = key expires in n days &amp;lt;n&amp;gt;w = key expires in n weeks &amp;lt;n&amp;gt;m = key expires in n months &amp;lt;n&amp;gt;y = key expires in n yearsKey is valid for? (0) Enter 0 for no expiration (common for personal use) or specify a duration (e.g., 2y for 2 years). You can extend or revoke the key later if needed. Confirm with: Key expires at [date] (or &quot;Key does not expire&quot;)Is this correct? (y/N) Type y and press Enter. Enter User InformationProvide details for the key’s identity: GnuPG needs to construct a user ID to identify your key.Real name: Real name: Enter your name (e.g., Alice Smith). Email address: Enter your email (e.g., alice@example.com). Comment: Add an optional comment (e.g., Work Key) or leave blank by pressing Enter. Example: Real name: Alice SmithEmail address: alice@example.comComment: You’ll see: You selected this USER-ID: &quot;Alice Smith &amp;lt;alice@example.com&amp;gt;&quot;Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? Type O (for Okay) and press Enter to proceed. Set a PassphraseYou’ll be prompted to enter a passphrase: Enter passphrase: Repeat passphrase: Choose a strong, unique passphrase to protect your private key. Enter it twice. You’ll need this passphrase for signing, decryption, or exporting the private key later. Generate the KeyGPG will now generate the key pair. This may take a few moments, especially with larger key sizes: We need to generate a lot of random bytes. It is a good idea to performsome other action (type on the keyboard, move the mouse, utilize thedisks) during the prime generation; this gives the random numbergenerator a better chance to gain enough entropy. Move your mouse, type randomly, or run some disk activity to help with entropy. Once complete, you’ll see output like: gpg: key 1234567890ABCDEF marked as ultimately trustedgpg: directory &#39;/home/user/.gnupg/openpgp-revocs.d&#39; createdgpg: revocation certificate createdpublic and secret key created and signed. Verify the KeyCheck your new key: gpg --list-keys Example output: pub rsa4096 2025-03-03 [SC] [expires: 2027-03-03] 1234567890ABCDEF1234567890ABCDEF12345678uid [ultimate] Alice Smith &amp;lt;alice@example.com&amp;gt;sub rsa4096 2025-03-03 [E] [expires: 2027-03-03] pub: Primary key (signing and certification, [SC]). sub: Subkey (encryption, [E]). Backup Your Key (Optional but Recommended)Export your public and private keys for safekeeping: Public key: gpg --armor --export 1234567890ABCDEF1234567890ABCDEF12345678 &amp;gt; alice_public.asc Private key: gpg --armor --export-secret-keys 1234567890ABCDEF1234567890ABCDEF12345678 &amp;gt; alice_private.asc Store these files securely (e.g., on an encrypted USB drive). Save the Revocation Certificate (Optional)GPG generates a revocation certificate in ~/.gnupg/openpgp-revocs.d/. Back it up: cp ~/.gnupg/openpgp-revocs.d/1234567890ABCDEF1234567890ABCDEF12345678.rev ~/secure_location/ Use this to revoke your key if it’s compromised. Notes Key ID: Replace 1234567890ABCDEF1234567890ABCDEF12345678 with your actual key fingerprint from the output. Subkeys: The default setup creates one encryption subkey. You can add more subkeys later (e.g., for signing) using gpg --edit-key. Security: Never share your private key (alice_private.asc) or passphrase." }, { "title": "How to sign with gpg key", "url": "/posts/sign-gpg/", "categories": "Security", "tags": "gpg", "date": "2025-03-02 06:35:00 -0500", "snippet": "Signing with a GPG key allows you to verify the authenticity and integrity of a message, file, or data, proving it came from you and hasn’t been altered. Below are the steps to sign something using your GPG key with the gpg command-line tool. I’ll cover signing a file, but the process can be adapted for messages or other data.Steps to Sign with a GPG Key Identify Your GPG KeyFirst, ensure you know the key you want to use for signing. List your secret keys: gpg --list-secret-keys Example output: sec rsa4096 2025-03-03 [SC] [expires: 2027-03-03] 1234567890ABCDEF1234567890ABCDEF12345678ssb rsa2048 2025-03-03 [E] FEDCBA9876543210FEDCBA9876543210FEDCBA98 sec is your primary secret key (used for signing by default, marked [SC] for signing and certification). ssb is a subkey (e.g., for encryption [E]; signing subkeys would be [S]). Note the key ID (e.g., 1234567890ABCDEF1234567890ABCDEF12345678). Sign a FileTo sign a file, use the gpg --sign or gpg --detach-sign command. Here are the two common methods: Option 1: Sign and Embed (Combined Signature) This creates a single file containing both the original content and the signature: gpg --armor --sign myfile.txt --armor: Outputs the signature in ASCII-armored format (readable text). myfile.txt: The file you want to sign. You’ll be prompted for your passphrase. Output: A file named myfile.txt.gpg is created. This is a binary file unless --armor is used, in which case it’s myfile.txt.asc. Option 2: Detached Signature (Separate File) This creates a separate signature file, leaving the original file unchanged: gpg --armor --detach-sign myfile.txt --detach-sign: Generates a standalone signature. Output: A file named myfile.txt.asc (or myfile.txt.sig without --armor). Specify a Specific Key (Optional)If you have multiple keys and want to use a specific one, add the --local-user (or -u) option with the key ID or email: gpg --armor --detach-sign -u 1234567890ABCDEF1234567890ABCDEF12345678 myfile.txt Or use the email: gpg --armor --detach-sign -u alice@example.com myfile.txt Verify the SignatureTo ensure the signing worked, verify it yourself: For an embedded signature: gpg --verify myfile.txt.asc For a detached signature: gpg --verify myfile.txt.asc myfile.txt Example output: gpg: Signature made Mon 03 Mar 2025 12:00:00 PM ESTgpg: using RSA key 1234567890ABCDEF1234567890ABCDEF12345678gpg: Good signature from &quot;Alice Smith &amp;lt;alice@example.com&amp;gt;&quot; [ultimate] Share the Signed File If you used --sign, share myfile.txt.asc (or .gpg). If you used --detach-sign, share both myfile.txt and myfile.txt.asc. Recipients need your public key to verify the signature. Signing a Message (Instead of a File)If you want to sign a message directly (e.g., for email): Run: gpg --armor --clearsign Type your message, then press Ctrl+D (Unix) or Ctrl+Z (Windows) to finish. Output will look like: -----BEGIN PGP SIGNED MESSAGE-----Hash: SHA256Hello, this is a signed message!-----BEGIN PGP SIGNATURE-----...-----END PGP SIGNATURE----- Notes Passphrase: You’ll need to enter your key’s passphrase each time you sign unless it’s cached in gpg-agent. Subkeys: If you have a signing subkey (e.g., marked [S]), GPG may use it automatically. To force the primary key, use -u with the primary key ID. Default Key: Without -u, GPG uses the default key (usually the first secret key or one set in ~/.gnupg/gpg.conf). Caching Passphrase: If prompted repeatedly for a passphrase, configure gpg-agent for convenience (e.g., gpg-agent --daemon)." }, { "title": "ESP 32", "url": "/posts/esp32/", "categories": "IoT", "tags": "esp32", "date": "2025-03-01 09:19:00 -0500", "snippet": "The ESP32 microcontroller has a versatile set of peripherals and GPIO pins, allowing you to connect multiple sensors. However, the exact number of sensors you can connect depends on several factors:1. GPIO Pins The ESP32 typically has 34 GPIO pins, but not all are available for use (some are used for internal functions or are input-only). The number of sensors you can connect directly depends on how many pins each sensor requires. For example: A simple digital sensor might use 1 pin. An I2C sensor uses 2 pins (SDA and SCL) but can support multiple devices on the same bus. An SPI sensor might use 3-4 pins but can also share the bus with other devices. Analog sensors use 1 pin each, but the ESP32 has only 15 ADC channels (some are used internally). 2. Communication Protocols I2C: You can connect multiple sensors (theoretically up to 127 devices) on the same I2C bus, as long as each has a unique address. SPI: You can connect multiple sensors to the same SPI bus, but each will need a dedicated chip select (CS) pin. UART: Limited to a few devices unless you use software serial or multiplexers. Analog Sensors: Limited by the number of ADC pins (15 on ESP32, but some are used internally).3. Power Supply Ensure your power supply can handle the combined current draw of all sensors. Some sensors may require external power or level shifting if they operate at different voltages (e.g., 5V sensors with a 3.3V ESP32).4. Software Limitations The ESP32 has limited RAM and processing power. If you connect too many sensors, you may run into performance issues.Example Scenarios Digital Sensors: If each sensor uses 1 GPIO pin, you can connect up to ~20 sensors (depending on available pins). I2C Sensors: You can connect dozens of sensors on the same I2C bus (limited by address space and wiring). Analog Sensors: Limited to 15 ADC channels (some may be used internally).Tips for Connecting More Sensors Use multiplexers (e.g., analog or I2C multiplexers) to expand the number of sensors. Use shift registers for digital sensors. Use external ADCs for more analog inputs.In summary, the number of sensors you can connect to an ESP32 depends on the type of sensors, available GPIO pins, and your ability to manage power and communication protocols. With careful planning, you can connect dozens of sensors.Examples - use casesCertainly! Below are two examples of optimal use cases for the ESP32 microcontroller: one for home automation and another for professional/industrial applications.1. Home Automation: Smart Home Monitoring and ControlThe ESP32 is ideal for creating a smart home system due to its Wi-Fi, Bluetooth, and low-power capabilities. Here’s an example setup:Components: ESP32 (as the main controller) Sensors: Temperature and humidity sensor (e.g., DHT22 or BME280) Motion sensor (PIR) Light sensor (LDR) Door/window contact sensor (magnetic reed switch) Actuators: Relays to control lights, fans, or appliances Servo motors for automated curtains or doors Displays: OLED display for local status updates Communication: MQTT protocol for sending data to a home automation server (e.g., Home Assistant, Node-RED) Wi-Fi for connectivity Functionality: Environmental Monitoring: Measure temperature, humidity, and light levels in different rooms. Send data to a central hub or cloud service for visualization. Security: Detect motion or door/window openings and send alerts to your phone. Automation: Turn lights on/off based on motion or time of day. Control appliances (e.g., fans, heaters) based on temperature. Local Control: Use a mobile app or web interface to control devices remotely. Voice control via integration with Google Assistant or Alexa. Advantages: Low cost and easy to implement. Scalable (you can add more ESP32 devices for different rooms or functions). Energy-efficient, especially when using deep sleep mode.2. Professional/Industrial Use: IoT-Based Machine MonitoringIn a professional or industrial setting, the ESP32 can be used for monitoring and controlling machinery, collecting data, and enabling predictive maintenance.Components: ESP32 (as the edge device) Sensors: Vibration sensor (e.g., MPU6050 accelerometer) Temperature sensor (e.g., PT100 or thermocouple with an ADC) Current sensor (e.g., ACS712) to monitor motor load Proximity sensor for object detection Actuators: Relays or solid-state relays (SSRs) to control machinery Stepper motors for precise control Communication: Wi-Fi or Ethernet (with an external module) for sending data to a central server. MQTT or HTTP/HTTPS for data transmission. Bluetooth for local configuration or debugging. Cloud Integration: AWS IoT, Google Cloud IoT, or Azure IoT for data storage and analysis. Functionality: Machine Monitoring: Collect real-time data on vibration, temperature, and current draw to monitor machine health. Detect anomalies (e.g., excessive vibration or overheating) and send alerts. Predictive Maintenance: Use machine learning algorithms (on the cloud or edge) to predict when maintenance is needed. Remote Control: Start/stop machines remotely via a web or mobile interface. Adjust machine parameters (e.g., speed, temperature) in real-time. Data Logging: Store historical data for analysis and reporting. Generate performance reports and trends. Advantages: Low-cost solution for industrial IoT. High reliability with Wi-Fi and Bluetooth connectivity. Scalable for large factories with multiple machines. Can operate in harsh environments with proper enclosures and shielding.Comparison of Home vs. Professional Use Aspect Home Automation Professional/Industrial Use Sensors Temperature, humidity, motion, light Vibration, temperature, current, proximity Actuators Relays, servo motors Relays, stepper motors, SSRs Communication Wi-Fi, MQTT, Bluetooth Wi-Fi, Ethernet, MQTT, HTTPS Cloud Integration Home Assistant, Node-RED, Blynk AWS IoT, Google Cloud IoT, Azure IoT Power Consumption Low (battery-operated sensors possible) Medium to high (continuous operation) Scalability Small to medium (single home) Large (multiple machines or factories) Key Takeaways For home use, the ESP32 is perfect for creating a cost-effective, scalable, and user-friendly smart home system. For professional use, the ESP32 can be used to build robust IoT solutions for monitoring, control, and predictive maintenance in industrial settings.Both examples demonstrate the versatility of the ESP32, making it a great choice for a wide range of applications! Let me know if you’d like more details or code examples for either use case." }, { "title": "Dokerize MariaDB with docker secrets", "url": "/posts/dockorize-mariadb/", "categories": "DEV", "tags": "mariadb, dbeaver, mysql, docker, compose, secrets", "date": "2025-03-01 06:35:00 -0500", "snippet": "Docker compose MariaDBI’ll provide you with a docker-compose.yml example for MariaDB that incorporates secrets for secure handling of sensitive data like the root password and database user credentials. Docker Compose supports secrets, which are a secure way to manage sensitive information without hardcoding them into the configuration file.Here’s a sample docker-compose.yml file with MariaDB and secrets:version: &#39;3.8&#39;services: mariadb: image: mariadb:latest restart: unless-stopped environment: - MYSQL_ROOT_PASSWORD_FILE=/run/secrets/mysql_root_password - MYSQL_DATABASE=my_database - MYSQL_USER=my_user - MYSQL_PASSWORD_FILE=/run/secrets/mysql_user_password volumes: - mariadb_data:/var/lib/mysql secrets: - mysql_root_password - mysql_user_password ports: - &quot;3306:3306&quot;volumes: mariadb_data:secrets: mysql_root_password: file: ./secrets/mysql_root_password.txt mysql_user_password: file: ./secrets/mysql_user_password.txtHow to Use This Directory Structure: Create a directory for your project, e.g., mariadb-docker, and inside it: Place the docker-compose.yml file. Create a secrets subdirectory. Create Secret Files: In the secrets subdirectory, create two files: mysql_root_password.txt: Write your desired MariaDB root password (e.g., mysecretrootpass). mysql_user_password.txt: Write your desired database user password (e.g., mysecretuserpass). Ensure these files contain only the password (no extra spaces or newlines). Example structure: mariadb-docker/├── docker-compose.yml└── secrets/ ├── mysql_root_password.txt └── mysql_user_password.txt Run Docker Compose: Open a terminal in the mariadb-docker directory. Run: docker-compose up -d This starts MariaDB in detached mode (-d). Verify: Check the container is running: docker ps. Connect to MariaDB using a client (e.g., mysql CLI, DBeaver) with: Host: localhost Port: 3306 User: my_user (or root for admin access) Password: From the respective secret file. Explanation Secrets: The mysql_root_password and mysql_user_password are stored in files and mounted into the container via Docker secrets. MariaDB reads them from /run/secrets/. Environment Variables: MYSQL_ROOT_PASSWORD_FILE and MYSQL_PASSWORD_FILE tell MariaDB to use the secrets. MYSQL_DATABASE creates a database named my_database. MYSQL_USER creates a user named my_user. Volumes: Persistent data is stored in the mariadb_data volume. Ports: Maps port 3306 on your host to the container.Notes Security: Keep the secrets directory out of version control (e.g., add it to .gitignore). In production, consider using a secret management tool like Docker Swarm secrets or a vault solution. Customization: Adjust the database name, user, or port mappings as needed. MariaDB Version: mariadb:latest uses the latest version; you can pin it to a specific version (e.g., mariadb:10.11).Troubleshoting to connect to DBeaverIf you’re receiving an “Access denied for user ‘root’” error when trying to connect to MariaDB via DBeaver, it’s likely due to authentication issues, misconfigured credentials, or network settings. Let’s troubleshoot and set up the connection step-by-step using the docker-compose.yml setup I provided earlier.Prerequisites Ensure your MariaDB container is running (from the docker-compose up -d command). You have DBeaver installed on your machine. You know the credentials from your secrets files (mysql_root_password.txt and mysql_user_password.txt).Steps to Connect to MariaDB with DBeaver Verify MariaDB is Running Run docker ps to confirm the MariaDB container is up. You should see something like: CONTAINER ID IMAGE COMMAND PORTS NAMESxyz123 mariadb:latest &quot;docker-entrypoint...&quot; 0.0.0.0:3306-&amp;gt;3306/tcp mariadb-docker_mariadb_1 If it’s not running, start it with docker-compose up -d from the directory containing your docker-compose.yml. Check Credentials From the docker-compose.yml I provided: Root User: root with the password from secrets/mysql_root_password.txt (e.g., mysecretrootpass). Custom User: my_user with the password from secrets/mysql_user_password.txt (e.g., mysecretuserpass). Open these files to confirm the exact passwords you set. Open DBeaver and Create a New Connection Launch DBeaver. Click Database &amp;gt; New Database Connection (or the “+” icon in the Database Navigator). Select MariaDB from the list and click Next. Configure Connection Settings Host: localhost (since the port is mapped to your host via 3306:3306 in Docker Compose). Port: 3306 (default MariaDB port, mapped in the docker-compose.yml). Database: my_database (from the MYSQL_DATABASE environment variable; optional for root). Username: Try root first (or my_user if you prefer the custom user). Password: Enter the password from mysql_root_password.txt (e.g., mysecretrootpass) for root, or from mysql_user_password.txt for my_user. Example: Host: localhost Port: 3306 Username: root Password: mysecretrootpass Test the Connection Click Test Connection in DBeaver. If it works, you’ll see a success message. If not, you’ll get an error (e.g., “Access denied”). Troubleshooting “Access Denied”If you get “Access denied for user ‘root’@’localhost’”: Wrong Password: Double-check the password from mysql_root_password.txt. Copy-paste it to avoid typos. Root Login Restrictions: MariaDB in Docker might restrict root login from outside the container by default. Let’s try the custom user instead: Username: my_user Password: From mysql_user_password.txt (e.g., mysecretuserpass). Test again. Network Issue: Ensure you’re connecting to localhost:3306 and no firewall is blocking the port. Run telnet localhost 3306 (or equivalent) to check connectivity. Container Logs: Check MariaDB logs for clues: docker logs &amp;lt;container_name&amp;gt; (Replace &amp;lt;container_name&amp;gt; with the name from docker ps, e.g., mariadb-docker_mariadb_1.) Fix Root Access (if Needed)If root still fails but my_user works, and you need root access: Access the container: docker exec -it &amp;lt;container_name&amp;gt; bash Log in to MariaDB inside the container: mariadb -u root -p Enter the root password from mysql_root_password.txt. Grant root access from any host (less secure, for testing only): GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;mysecretrootpass&#39;;FLUSH PRIVILEGES; Exit and retry in DBeaver with root. Finalize Once the connection works, click Finish in DBeaver to save it. You should now see my_database (or other databases if using root) in the navigator. Common Issues “Host ‘172.x.x.x’ is not allowed to connect”: This happens if MariaDB rejects connections from your host. Using my_user should work since it’s created with broader permissions, or adjust root as above. Port Conflict: If 3306 is already in use on your machine, change the host mapping in docker-compose.yml (e.g., 3307:3306) and use 3307 in DBeaver." }, { "title": "IoT Stuck", "url": "/posts/iot-tools/", "categories": "DEV", "tags": "docker, nodered, mqtt, mysql", "date": "2025-03-01 06:35:00 -0500", "snippet": "" }, { "title": "IoT system for monitoring a dam water pumping station (G)", "url": "/posts/tempo-1/", "categories": "Architecture", "tags": "monitoring, esp32", "date": "2025-02-24 06:35:00 -0500", "snippet": "It seems like you’re asking about an IoT system for monitoring a dam water pumping station, which I’ll interpret as a setup to manage and monitor water pumping operations at a dam—covering aspects like water levels, pump performance, and system health. Given your familiarity with Angular, Spring Boot, and Docker from your previous question, I’ll tailor this response to integrate those technologies with an IoT solution, focusing on the ESP32 microcontroller (as recommended earlier) for its versatility and fit for such a task. Let’s break this down into a practical system design.System OverviewAn IoT system for a dam water pumping station would monitor water levels, pump status (on/off, runtime, faults), flow rates, and environmental conditions (e.g., pressure or temperature), while allowing remote control and real-time alerts. The ESP32 handles the hardware interfacing, Spring Boot serves as the backend, Angular provides the frontend, and Docker containerizes the setup for scalability and deployment ease.Recommended Hardware: ESP32The ESP32 remains ideal here: Wi-Fi: Connects to your network for data transmission. GPIO Pins: Interfaces with sensors and relays for pumps. ADC Channels: Reads analog sensor data (e.g., water level, pressure). Low Power: Suitable for continuous operation at a remote dam site. Cost: Affordable at $5-$10 per unit.Hardware Components ESP32 DevKit: Core microcontroller. Sensors: Ultrasonic Sensor (HC-SR04): Measures water level in the dam or reservoir. Flow Sensor (YF-S201): Tracks water flow rate through the pump. Pressure Sensor (MPX5700): Monitors pump pressure. Current Sensor (ACS712): Detects pump power usage for health monitoring. Relays: Controls pump on/off (e.g., 5V relay for low-power pumps or SSR for high-power AC pumps). Power Supply: 5V for ESP32, with appropriate AC-DC converters for pumps (e.g., 24V or 120V/240V based on pump specs).System Architecture ESP32 Firmware Platform: Arduino IDE or ESP-IDF. Functionality: Poll sensors every few seconds (e.g., water level, flow, pressure, current). Control pump via relay based on commands or thresholds (e.g., turn on if level &amp;gt; 50%, off if &amp;lt; 10%). Send data to Spring Boot via REST API or MQTT for real-time updates. Sample Code Snippet (Arduino): #include &amp;lt;WiFi.h&amp;gt;#include &amp;lt;HTTPClient.h&amp;gt;const char* ssid = &quot;your-wifi&quot;;const char* password = &quot;your-password&quot;;const char* server = &quot;http://your-server:8080/api/data&quot;;#define RELAY_PIN 26void setup() { Serial.begin(115200); WiFi.begin(ssid, password); pinMode(RELAY_PIN, OUTPUT);}void loop() { if (WiFi.status() == WL_CONNECTED) { HTTPClient http; http.begin(server); http.POST(&quot;{\\&quot;level\\&quot;:&quot; + String(getWaterLevel()) + &quot;,\\&quot;flow\\&quot;:&quot; + String(getFlowRate()) + &quot;}&quot;); http.end(); } delay(5000); // Poll every 5 seconds}float getWaterLevel() { /* HC-SR04 logic */ }float getFlowRate() { /* YF-S201 logic */ } Backend: Spring Boot Purpose: Processes data, stores it, and serves the Angular frontend. Features: REST API: /api/data (POST for ESP32 data, GET for frontend). MQTT Broker (e.g., Mosquitto): For real-time pump status. Database: PostgreSQL for historical data (levels, pump runtime). Dockerfile: FROM openjdk:17-jdk-slimCOPY target/dam-monitoring.jar /app.jarENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/app.jar&quot;] Frontend: Angular Purpose: Displays real-time data and controls pumps. Features: Dashboard: Water level gauge, pump status (on/off), flow rate graph. Controls: Toggle pump manually or set automated thresholds. Alerts: Notify if water level is critical or pump fails (via WebSocket/MQTT). Dockerfile: FROM node:18 AS buildWORKDIR /appCOPY . .RUN npm install &amp;amp;&amp;amp; ng build --prodFROM nginx:alpineCOPY --from=build /app/dist/dam-app /usr/share/nginx/html Docker Compose Combines all services: version: &#39;3&#39;services: backend: build: ./spring-boot ports: [ &quot;8080:8080&quot; ] depends_on: [ db, mqtt ] frontend: build: ./angular ports: [ &quot;80:80&quot; ] db: image: postgres:14 environment: - POSTGRES_DB=dam_db - POSTGRES_USER=user - POSTGRES_PASSWORD=password volumes: - pgdata:/var/lib/postgresql/data mqtt: image: eclipse-mosquitto:2 ports: [ &quot;1883:1883&quot;, &quot;9001:9001&quot; ]volumes: pgdata: Security ESP32: Use HTTPS for REST calls (Let’s Encrypt via Spring Boot). MQTT with TLS and username/password authentication. Spring Boot: JWT for API security. Store secrets in environment variables (e.g., docker-compose.yml with .env). Network: Deploy behind a VPN (e.g., WireGuard, as in your prior setup). Firewall: Restrict ports 80, 443, 1883 to VPN subnet: iptables -A INPUT -s 10.13.13.0/24 -p tcp -m multiport --dports 80,443,1883 -j ACCEPTiptables -A INPUT -p tcp -m multiport --dports 80,443,1883 -j DROP Implementation Steps Hardware: Connect ESP32 to sensors and relays (e.g., ultrasonic to GPIO 12/13, relay to GPIO 26). Firmware: Flash ESP32 with code to read sensors and send data. Backend: Deploy Spring Boot with API and MQTT endpoints. Frontend: Build Angular app with real-time UI. Docker: Run docker-compose up -d in your project directory. Test: Monitor via http://your-subdomain.duckdns.org over VPN, ensuring pump control works.Why This Works for a Dam Pumping Station? Scalability: Add more ESP32s for additional pumps or sensors. Real-Time: MQTT ensures instant updates for critical events (e.g., pump failure). Security: VPN and HTTPS protect against unauthorized access. Integration: Leverages your Angular/Spring Boot/Docker skills.If you need specific code, hardware wiring diagrams, or help with deployment, let me know! This setup balances cost, functionality, and security for a dam water pumping station." }, { "title": "How to remain Ubuntu Server active and not enter sleep or suspend mode (DS)", "url": "/posts/how-to-remain-ubuntu-server-active-and-not-enter-sleep-or-suspend-mode/", "categories": "Privacy", "tags": "docker, gogs, git, mysql", "date": "2025-02-17 18:14:00 -0500", "snippet": "To ensure your Ubuntu Server does not sleep or suspend, you need to configure the system’s power management settings. Servers are typically designed to run continuously, but sometimes power-saving features can interfere. Here’s how to disable sleep and suspend on Ubuntu Server:1. Check Current Power Management SettingsFirst, check if any power management services are running:systemctl status sleep.target suspend.target hibernate.target hybrid-sleep.targetIf any of these services are active, you’ll need to disable them.2. Disable Sleep and Suspend ServicesTo prevent the system from sleeping or suspending, mask the relevant systemd targets:sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.targetThis will prevent these services from being activated.3. Configure Logind SettingsThe logind service manages power settings. Edit its configuration file to disable sleep and suspend: Open the logind.conf file: sudo nano /etc/systemd/logind.conf Find or add the following lines: HandleSuspendKey=ignoreHandleHibernateKey=ignoreHandleLidSwitch=ignoreHandleLidSwitchDocked=ignoreIdleAction=ignore Save the file and exit (Ctrl+O, Enter, Ctrl+X). Restart the logind service to apply changes: sudo systemctl restart systemd-logind 4. Disable Automatic Sleep (if using a desktop environment)If your Ubuntu Server has a desktop environment installed (e.g., GNOME), you may need to adjust additional settings: Open Settings &amp;gt; Power. Set Blank Screen and Automatic Suspend to Never.5. Prevent Hard Disk SleepIf your server uses hard drives, ensure they don’t go to sleep: Install hdparm: sudo apt install hdparm Check the current power management settings for your disk: sudo hdparm -B /dev/sda Replace /dev/sda with your disk identifier. Disable power management for the disk: sudo hdparm -B 255 /dev/sda 6. Disable ACPI Sleep States (Advanced)If the system still suspends, you may need to disable ACPI sleep states in the kernel: Edit the GRUB configuration: sudo nano /etc/default/grub Find the GRUB_CMDLINE_LINUX_DEFAULT line and add acpi=off or acpi_sleep=nonvs: GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash acpi=off&quot; Update GRUB: sudo update-grub Reboot the system: sudo reboot 7. Verify SettingsAfter making these changes, verify that the system no longer sleeps or suspends: Monitor the system for a while to ensure it stays active. Check logs for any power-related events: journalctl -xe | grep -i suspend By following these steps, your Ubuntu Server should remain active and not enter sleep or suspend mode. Let me know if you need further assistance! 🚀" }, { "title": "How to get a secure, efficient, and reliable home server (DS)", "url": "/posts/how-to-get-a-secure-efficient-and-reliable-home-server/", "categories": "Linux", "tags": "security, server, ubuntu", "date": "2025-02-17 02:35:00 -0500", "snippet": "Setting up an optimal and appropriate configuration for a home server on Ubuntu Server OS depends on your specific use case (e.g., file storage, media server, game server, etc.). However, here’s a general guide to help you configure a secure, efficient, and reliable home server:1. Hardware Requirements CPU: A multi-core processor (e.g., Intel i3/i5 or AMD Ryzen) for basic tasks. For heavier workloads (e.g., virtualization), consider a higher-end CPU. RAM: At least 4GB for light use, 8GB+ for moderate use, and 16GB+ for heavy workloads. Storage: Use SSDs for the OS and applications for faster performance. HDDs are suitable for bulk storage (e.g., media files). Network: A Gigabit Ethernet connection is recommended for fast file transfers and streaming.2. Install Ubuntu Server Download the latest Ubuntu Server LTS (Long-Term Support) version from the official website. Follow the installation wizard, ensuring you: Set up a strong password for the root/user account. Enable SSH during installation for remote management. Configure disk partitioning (e.g., separate /home partition for easier backups). 3. Basic ConfigurationUpdate the SystemAfter installation, update the system:sudo apt update &amp;amp;&amp;amp; sudo apt upgrade -ySet Up a Static IPTo ensure your server has a consistent IP address: Edit the Netplan configuration file: sudo nano /etc/netplan/01-netcfg.yaml Add a static IP configuration: network: version: 2 ethernets: eth0: dhcp4: no addresses: - 192.168.1.100/24 gateway4: 192.168.1.1 nameservers: addresses: - 8.8.8.8 - 8.8.4.4 Apply the changes: sudo netplan apply Enable SSH AccessIf SSH wasn’t enabled during installation, install and enable it:sudo apt install openssh-serversudo systemctl enable --now ssh4. Secure Your ServerFirewall ConfigurationUse UFW to allow only necessary ports:sudo ufw allow sshsudo ufw allow 80/tcp # HTTPsudo ufw allow 443/tcp # HTTPSsudo ufw enableDisable Root LoginEdit the SSH configuration to disable root login: Open the SSH config file: sudo nano /etc/ssh/sshd_config Set PermitRootLogin no. Restart SSH: sudo systemctl restart ssh Set Up Fail2BanInstall Fail2Ban to protect against brute-force attacks:sudo apt install fail2bansudo systemctl enable --now fail2ban5. Install Essential SoftwareWeb Server (Optional)Install Nginx or Apache for hosting websites:sudo apt install nginxFile Sharing Samba for Windows/Linux file sharing: sudo apt install samba NFS for Linux file sharing: sudo apt install nfs-kernel-server Media Server Plex or Jellyfin for media streaming: # For Jellyfinsudo apt install jellyfin Backup Solutions Timeshift for system snapshots: sudo apt install timeshift rsync or Restic for file backups.6. Optimize PerformanceDisable Unnecessary ServicesDisable services you don’t need to free up resources:sudo systemctl disable servicenameEnable ZRAM (for Low-RAM Systems)ZRAM compresses memory to improve performance on systems with limited RAM:sudo apt install zram-configMonitor Resource UsageInstall monitoring tools like htop or glances:sudo apt install htop7. Automate TasksCron JobsUse cron to automate repetitive tasks (e.g., backups, updates):crontab -eSystem UpdatesEnable automatic security updates:sudo apt install unattended-upgradessudo dpkg-reconfigure --priority=low unattended-upgrades8. Backup and Recovery Regular Backups: Use tools like Timeshift, rsync, or Restic to back up your data. Test Restores: Periodically test your backups to ensure they work.9. Optional: VirtualizationIf you want to run multiple services in isolated environments, install Docker or KVM: Docker: sudo apt install docker.iosudo systemctl enable --now docker KVM: sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utilssudo systemctl enable --now libvirtd 10. Monitor and Maintain Regularly check system logs: sudo journalctl -xe Monitor disk usage: df -h Clean up unused packages: sudo apt autoremove By following these steps, you’ll have a secure, efficient, and reliable home server tailored to your needs. Let me know if you need help with specific configurations! 🚀" }, { "title": "Enhance the privacy and security of your Deepin Linux OS", "url": "/posts/enhance-the-privacy-and-security-of-your-Deepin/", "categories": "Linux, Privacy", "tags": "security, privacy, deepin, linux", "date": "2025-02-16 18:35:00 -0500", "snippet": "Securing and guaranteeing privacy on Deepin Linux OS involves several steps, including system configuration, software choices, and best practices. Here’s a comprehensive guide to help you enhance your privacy and security on Deepin:1. Keep Your System Updated Regular Updates: Ensure your system and installed software are up to date. Deepin uses the Debian/Ubuntu base, so security patches are regularly released. Use the Deepin App Store or run: sudo apt update &amp;amp;&amp;amp; sudo apt upgrade 2. Enable Firewall UFW (Uncomplicated Firewall): Enable and configure a firewall to block unauthorized access. Install UFW: sudo apt install ufw Enable UFW: sudo ufw enable Configure UFW to allow only necessary services (e.g., SSH, HTTP): sudo ufw allow sshsudo ufw allow http 3. Use Strong User Authentication Strong Passwords: Use complex, unique passwords for your user account. Disable Root Login: Avoid logging in as the root user. Use sudo for administrative tasks. Two-Factor Authentication (2FA): For added security, enable 2FA for SSH or other services.4. Encrypt Your Data Full Disk Encryption: Encrypt your disk during the Deepin installation process. Home Directory Encryption: If you didn’t encrypt your disk, encrypt your home directory: sudo apt install ecryptfs-utilsecryptfs-migrate-home -u your_username File Encryption: Use tools like GPG or VeraCrypt to encrypt sensitive files.5. Secure Network Connections Use a VPN: Use a trusted VPN service to encrypt your internet traffic and hide your IP address. HTTPS Everywhere: Install browser extensions like HTTPS Everywhere to enforce secure connections. DNS Privacy: Use privacy-focused DNS servers like Cloudflare (1.1.1.1) or Quad9 (9.9.9.9).6. Minimize Data Collection Disable Telemetry: Deepin may collect usage data. Disable telemetry in the system settings or privacy settings. Audit Installed Software: Remove unnecessary software that may collect data or pose security risks.7. Use Privacy-Focused Software Web Browser: Use privacy-focused browsers like Firefox with privacy extensions (uBlock Origin, Privacy Badger). Email: Use encrypted email services like ProtonMail or Tutanota. Messaging: Use end-to-end encrypted messaging apps like Signal or Telegram (Secret Chats).8. Harden Your System AppArmor/SELinux: Enable AppArmor or SELinux to restrict application permissions. Check AppArmor status: sudo aa-status Disable Unused Services: Disable unnecessary services to reduce attack surfaces. sudo systemctl disable servicename 9. Monitor System Activity Logs: Regularly check system logs for suspicious activity: sudo journalctl -xe Intrusion Detection: Use tools like fail2ban to block brute-force attacks: sudo apt install fail2ban 10. Backup Your Data Regular Backups: Use tools like Timeshift or Deja Dup to create regular backups. Encrypt Backups: Ensure your backups are encrypted and stored securely.11. Use Open-Source Software Avoid Proprietary Software: Stick to open-source software whenever possible to ensure transparency and avoid potential backdoors.12. Secure Physical Access BIOS/UEFI Password: Set a password for your BIOS/UEFI to prevent unauthorized booting. Auto-Lock: Configure your system to lock automatically after a period of inactivity.13. Educate Yourself Stay Informed: Keep up with the latest security practices and vulnerabilities. Avoid Phishing: Be cautious of suspicious emails, links, and downloads.By following these steps, you can significantly enhance the privacy and security of your Deepin Linux OS. Remember, security is an ongoing process, so regularly review and update your practices." }, { "title": "Allow Specific Ports In The Firewall Rules", "url": "/posts/allow-specific-ports-in-the-firewall-rules-in-deepin/", "categories": "Security", "tags": "ufw, firewall", "date": "2025-02-16 12:35:00 -0500", "snippet": "To allow specific ports in the firewall rules on Deepin OS, you can use UFW (Uncomplicated Firewall), which is a user-friendly tool for managing firewall rules. Here’s how to allow ports step by step:1. Install UFW (if not already installed)UFW is usually pre-installed on Deepin, but if it’s not, you can install it:sudo apt updatesudo apt install ufw2. Check UFW StatusBefore making changes, check the status of UFW to ensure it’s active and see the current rules:sudo ufw statusIf UFW is inactive, you’ll need to enable it.3. Enable UFWIf UFW is not already enabled, activate it:sudo ufw enableThis will start the firewall and apply default rules.4. Allow Specific PortsTo allow a specific port, use the following command:sudo ufw allow &amp;lt;port_number&amp;gt;For example, to allow port 80 (HTTP):sudo ufw allow 805. Allow Ports with Specific ProtocolsIf you want to allow a port for a specific protocol (TCP or UDP), specify it:sudo ufw allow &amp;lt;port_number&amp;gt;/&amp;lt;protocol&amp;gt;For example, to allow port 443 for HTTPS (TCP):sudo ufw allow 443/tcp6. Allow a Range of PortsTo allow a range of ports, use:sudo ufw allow &amp;lt;start_port&amp;gt;:&amp;lt;end_port&amp;gt;/&amp;lt;protocol&amp;gt;For example, to allow ports 8000 to 9000 for TCP:sudo ufw allow 8000:9000/tcp7. Allow Services by NameUFW includes predefined rules for common services. You can allow a service by its name instead of specifying the port:sudo ufw allow &amp;lt;service_name&amp;gt;For example, to allow SSH:sudo ufw allow sshTo see a list of predefined services, check:less /etc/services8. Verify the RulesAfter adding rules, verify them by checking the UFW status:sudo ufw statusThis will display a list of all active rules.Tester le renduasdfasdf asdfasdfTester le rendu 12asdfasdf asdfasdfasdfasdf asdfasdfasdfasdf asdfasdf9. Delete a Rule (if needed)If you need to remove a rule, use:sudo ufw delete allow &amp;lt;port_number&amp;gt;For example, to remove the rule allowing port 80:sudo ufw delete allow 8010. Disable UFW (if needed)If you need to temporarily disable UFW, use:sudo ufw disable11. Reset UFW (if needed)To reset UFW to its default settings (removing all rules), use:sudo ufw resetExample ScenarioIf you want to allow: Port 22 for SSH, Port 80 for HTTP, Port 443 for HTTPS, Port 3000 for a custom app,Run the following commands:sudo ufw allow sshsudo ufw allow 80/tcpsudo ufw allow 443/tcpsudo ufw allow 3000/tcpBy following these steps, you can easily manage firewall rules on Deepin OS using UFW.Always ensure you only allow necessary ports to minimize security risks." }, { "title": "Setup For WireGuard, NextCloud, and DuckDNS using Docker (G)", "url": "/posts/setup-forwireGuard-nextCloud-and-duckDNS-using-docker/", "categories": "Privacy", "tags": "docker, nextcloud, duckdns, wireguard", "date": "2025-02-15 08:35:00 -0500", "snippet": "Basic configurationIt seems like you meant “DuckDNS” instead of “dukdns”—I’ll assume that and proceed with a setup for WireGuard, NextCloud, and DuckDNS using Docker. This combination gives you a secure VPN (WireGuard), self-hosted cloud storage (NextCloud), and dynamic DNS (DuckDNS) to keep your domain updated with your IP. Here’s a streamlined, practical approach using Docker Compose for simplicity and maintainability, assuming you’re running this on a single machine (like a home server or VPS).Docker Compose SetupFirst, make sure you have Docker and Docker Compose installed. Then, create a directory for this project (e.g., ~/cloud-vpn), and inside it, create a docker-compose.yml file with the following content:version: &#39;3&#39;services: duckdns: image: linuxserver/duckdns container_name: duckdns environment: - TOKEN=your-duckdns-token # Replace with your DuckDNS token - SUBDOMAINS=your-subdomain # Replace with your DuckDNS subdomain (e.g., mycloud) volumes: - ./duckdns:/config # Stores logs, not critical but useful restart: unless-stopped wireguard: image: linuxserver/wireguard container_name: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - PUID=1000 # Your user ID, adjust if needed - PGID=1000 # Your group ID, adjust if needed - TZ=America/New_York # Adjust to your timezone - SERVERURL=your-subdomain.duckdns.org # Your DuckDNS domain - SERVERPORT=51820 - PEERS=2 # Number of clients (adjust as needed) volumes: - ./wireguard:/config # Stores WireGuard config and keys - /lib/modules:/lib/modules:ro # For kernel module loading ports: - 51820:51820/udp # WireGuard port sysctls: - net.ipv4.conf.all.src_valid_mark=1 # Needed for WireGuard restart: unless-stopped db: image: mariadb:10.5 container_name: nextcloud-db environment: - MYSQL_ROOT_PASSWORD=your-root-password # Replace with a secure password - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_PASSWORD=your-db-password # Replace with a secure password volumes: - ./db:/var/lib/mysql # Persists database data restart: unless-stopped nextcloud: image: nextcloud:latest container_name: nextcloud depends_on: - db environment: - MYSQL_HOST=db - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_PASSWORD=your-db-password # Match the db service password - NEXTCLOUD_ADMIN_USER=admin # Initial admin username - NEXTCLOUD_ADMIN_PASSWORD=your-admin-password # Replace with a secure password volumes: - ./nextcloud:/var/www/html # Persists NextCloud data ports: - 8080:80 # Maps NextCloud to port 8080 on host restart: unless-stoppedSteps to Install and Configure Prep Work Sign up at DuckDNS, get your token, and pick a subdomain (e.g., mycloud → mycloud.duckdns.org). Replace placeholders in the docker-compose.yml file: your-duckdns-token and your-subdomain under duckdns. your-subdomain.duckdns.org under wireguard. Passwords under db and nextcloud (use strong, unique ones). Create the directory structure: mkdir ~/cloud-vpn &amp;amp;&amp;amp; cd ~/cloud-vpn. Start the Services Run docker-compose up -d in the directory with your docker-compose.yml. This pulls the images and starts the containers. Check logs to ensure everything’s running: docker-compose logs. Configure WireGuard After starting, WireGuard generates client configs in ./wireguard/qr_codes/ or ./wireguard/config/peerX/. Copy these to your devices (e.g., via QR code for mobile or .conf file for desktop). Install the WireGuard client on your device (see wireguard.com/install), import the config, and connect. Set Up NextCloud Open your browser to http://localhost:8080 (or your server’s IP:8080 if remote). The first time, NextCloud finishes setup using the admin credentials from the compose file. Once set up, configure NextCloud to trust your DuckDNS domain under Settings &amp;gt; Administration &amp;gt; General (e.g., mycloud.duckdns.org). Secure Access By default, NextCloud is on port 8080 and accessible locally. To restrict it to WireGuard: Edit the ports line under nextcloud to remove - 8080:80 (making it internal-only). Access it via the VPN at http://nextcloud:80 (Docker’s internal networking) or set up a reverse proxy if needed. Alternatively, use your firewall (e.g., ufw or iptables) to allow only WireGuard subnet traffic (e.g., 10.13.13.0/24, check your WireGuard config). Test It Connect to WireGuard on a client device. Visit http://your-subdomain.duckdns.org:8080 (or internal URL if ports are removed) to access NextCloud securely. DuckDNS keeps your domain updated, so it works even if your public IP changes. Why This Is the Best Way Docker Compose: Manages all services in one file, making it easy to start, stop, or tweak settings. Security: WireGuard encrypts traffic, and restricting NextCloud to VPN access adds a layer of protection. Simplicity: DuckDNS handles dynamic IPs without complex DNS setups, and the chosen images (LinuxServer and official NextCloud) are reliable and well-documented.Tips and Tweaks HTTPS: For production, add a reverse proxy like Nginx or Traefik with Let’s Encrypt for SSL on NextCloud (port 443). Storage: Mount external drives to ./nextcloud for more space. Backup: Regularly back up ./db and ./nextcloud directories. Troubleshooting: Use docker-compose logs &amp;lt;service&amp;gt; to debug if something fails.Let’s enhance securityAdding HTTPS for secure browsing and adjusting the firewallLet’s enhance your WireGuard, NextCloud, and DuckDNS setup by adding HTTPS for secure browsing and adjusting the firewall to restrict NextCloud access to WireGuard VPN clients only. I’ll assume you’re using the Docker Compose setup from my previous response and build on it. We’ll use Nginx Proxy Manager for HTTPS (simpler than raw Nginx or Traefik for beginners) and configure iptables for the firewall.Step 1: Setting Up HTTPS with Nginx Proxy ManagerWhy Nginx Proxy Manager?It’s a Docker-friendly tool with a web UI to manage reverse proxies and SSL certificates via Let’s Encrypt, making HTTPS setup straightforward.Updated Docker ComposeAdd Nginx Proxy Manager to your docker-compose.yml. Here’s the full updated file (replace your existing one, keeping your custom values like tokens and passwords):version: &#39;3&#39;services: duckdns: image: linuxserver/duckdns container_name: duckdns environment: - TOKEN=your-duckdns-token - SUBDOMAINS=your-subdomain volumes: - ./duckdns:/config restart: unless-stopped wireguard: image: linuxserver/wireguard container_name: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - PUID=1000 - PGID=1000 - TZ=America/New_York - SERVERURL=your-subdomain.duckdns.org - SERVERPORT=51820 - PEERS=2 volumes: - ./wireguard:/config - /lib/modules:/lib/modules:ro ports: - 51820:51820/udp sysctls: - net.ipv4.conf.all.src_valid_mark=1 restart: unless-stopped db: image: mariadb:10.5 container_name: nextcloud-db environment: - MYSQL_ROOT_PASSWORD=your-root-password - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_PASSWORD=your-db-password volumes: - ./db:/var/lib/mysql restart: unless-stopped nextcloud: image: nextcloud:latest container_name: nextcloud depends_on: - db environment: - MYSQL_HOST=db - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud - MYSQL_PASSWORD=your-db-password - NEXTCLOUD_ADMIN_USER=admin - NEXTCLOUD_ADMIN_PASSWORD=your-admin-password volumes: - ./nextcloud:/var/www/html restart: unless-stopped # Note: No ports exposed here; Nginx Proxy Manager will handle access nginx-proxy-manager: image: jc21/nginx-proxy-manager:latest container_name: nginx-proxy-manager ports: - 80:80 # HTTP - 443:443 # HTTPS - 81:81 # Web UI environment: - DB_MYSQL_HOST=db - DB_MYSQL_PORT=3306 - DB_MYSQL_USER=nginxpm - DB_MYSQL_PASSWORD=nginxpm-password # Replace with a secure password - DB_MYSQL_DATABASE=nginxpm volumes: - ./nginx-proxy-manager/data:/data - ./nginx-proxy-manager/letsencrypt:/etc/letsencrypt depends_on: - db restart: unless-stoppedSteps to Set Up HTTPS Update and Restart Save the updated docker-compose.yml in your ~/cloud-vpn directory. Run docker-compose up -d to recreate containers with the new service. Initial Nginx Proxy Manager Setup Open http://your-server-ip:81 in your browser (replace your-server-ip with your machine’s IP, e.g., 192.168.1.100 or localhost if local). Log in with default credentials: admin@example.com / changeme. Change the email and password immediately in the UI. Add a Proxy Host In the Nginx Proxy Manager UI, go to “Hosts” &amp;gt; “Proxy Hosts” &amp;gt; “Add Proxy Host”. Details: Domain Name: your-subdomain.duckdns.org Scheme: http Forward Hostname/IP: nextcloud (Docker service name) Forward Port: 80 Enable “Block Common Exploits” and “Websockets Support” for safety and compatibility. Save this configuration. Set Up SSL Edit the proxy host you just created. Go to the “SSL” tab, select “Request a new certificate”. Check “Force SSL” and “HTTP/2 Support”. Agree to Let’s Encrypt terms and save. It’ll fetch a certificate for your-subdomain.duckdns.org. Wait a minute; if it fails, ensure port 80 is open to the internet (temporary for cert issuance). Test HTTPS Visit https://your-subdomain.duckdns.org. You should see NextCloud over HTTPS. Update NextCloud’s trusted domains in ./nextcloud/config/config.php: &#39;trusted_domains&#39; =&amp;gt; array ( 0 =&amp;gt; &#39;localhost&#39;, 1 =&amp;gt; &#39;your-subdomain.duckdns.org&#39;, ), Step 2: Adjusting the Firewall with iptablesGoalRestrict NextCloud access (via Nginx Proxy Manager on ports 80 and 443) to only WireGuard VPN clients, using the WireGuard subnet (e.g., 10.13.13.0/24 from the default linuxserver/wireguard config).Firewall Setup Check WireGuard Subnet Look at ./wireguard/config/wg0.conf or client configs (e.g., AllowedIPs = 10.13.13.0/24). The default is often 10.13.13.0/24; adjust commands below if yours differs. Install iptables (if needed) On Ubuntu/Debian: sudo apt update &amp;amp;&amp;amp; sudo apt install iptables On Fedora: sudo dnf install iptables Flush Existing Rules (Optional) Backup current rules: sudo iptables-save &amp;gt; iptables-backup.txt Flush: sudo iptables -F (careful, this clears all rules; skip if you have other critical rules). Add Firewall Rules Allow WireGuard traffic (port 51820): sudo iptables -A INPUT -p udp --dport 51820 -j ACCEPT Allow HTTP/HTTPS only from WireGuard subnet: sudo iptables -A INPUT -s 10.13.13.0/24 -p tcp -m multiport --dports 80,443 -j ACCEPT Drop all other HTTP/HTTPS traffic: sudo iptables -A INPUT -p tcp -m multiport --dports 80,443 -j DROP Allow loopback (local access): sudo iptables -A INPUT -i lo -j ACCEPT Allow established/related connections (for general networking): sudo iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT Save Rules On Ubuntu/Debian: sudo iptables-save &amp;gt; /etc/iptables/rules.v4 On Fedora: sudo iptables-save &amp;gt; /etc/sysconfig/iptables Ensure persistence: Install iptables-persistent (Debian/Ubuntu) or configure a systemd service if needed. Test Firewall Disconnect from WireGuard and try https://your-subdomain.duckdns.org—it should fail. Connect to WireGuard and try again—it should work. Alternative: Using ufwIf you prefer ufw (simpler firewall frontend):sudo ufw allow 51820/udpsudo ufw allow from 10.13.13.0/24 to any port 80sudo ufw allow from 10.13.13.0/24 to any port 443sudo ufw deny 80sudo ufw deny 443sudo ufw enableFinal Notes HTTPS: Nginx Proxy Manager handles SSL termination, so NextCloud stays on HTTP internally, which is fine. Certificates auto-renew every 90 days. Firewall: These rules assume a basic setup. If you’re behind a router, forward ports 80, 443, and 51820/udp to your server, and tweak rules if other services need access. Troubleshooting: SSL fails? Check port 80 is reachable during cert issuance (docker logs nginx-proxy-manager). Firewall blocks too much? Review rules with sudo iptables -L -v. You’re now running NextCloud over HTTPS, accessible only via WireGuard! Let me know if you hit any issues or want further refinements.Tempo jmcjmcjmcKey Points Install WireGuard, NextCloud, and DuckDNS using Docker for a secure, self-hosted cloud with dynamic DNS. Use official or trusted Docker images: linuxserver/duckdns, linuxserver/wireguard, and nextcloud/nextcloud. Configure firewall to restrict NextCloud access to WireGuard VPN clients for enhanced security. Surprising detail: DuckDNS is a service, not software, so you use a Docker image to update your IP dynamically.Getting Started with Docker InstallationsOverviewThis guide helps you set up WireGuard (a secure VPN), NextCloud (a self-hosted cloud storage), and DuckDNS (dynamic DNS) using Docker. These services work together to provide a secure way to access your cloud storage remotely, with DuckDNS ensuring your domain name always points to your changing IP address, and WireGuard securing the connection.Step-by-Step Installation Set Up DuckDNS Sign up at DuckDNS to get a token and subdomain. Pull the Docker image: docker pull linuxserver/duckdns. Run the container with your token and subdomain: docker run -d --name duckdns -e TOKEN=your_token -e SUBDOMAINS=your_subdomain -v /path/to/config:/config --restart unless-stopped linuxserver/duckdns This keeps your domain updated with your current IP address. Set Up WireGuard Pull the Docker image: docker pull linuxserver/wireguard. Run the container, ensuring network admin capabilities: docker run -d --name wireguard --cap-add NET_ADMIN --cap-add SYS_MODULE -v /path/to/wireguard/config:/etc/wireguard -v /lib/modules:/lib/modules -p 51820:51820/UDP --restart unless-stopped linuxserver/wireguard Configure the server by editing /etc/wireguard/wg0.conf inside the container to set up VPN settings and add clients. Set Up NextCloud First, set up a MariaDB database: docker run -d --name mariadb -e MYSQL_ROOT_PASSWORD=your_password -e MYSQL_DATABASE=nextcloud -e MYSQL_USER=nextcloud -e MYSQL_PASSWORD=nextcloud_password -v /path/to/mariadb/data:/var/lib/mysql --restart unless-stopped mariadb:10.5 Pull the NextCloud image: docker pull nextcloud/nextcloud. Run NextCloud, linking to the database: docker run -d --name nextcloud --link mariadb:mysql -e MYSQL_HOST=mysql -e MYSQL_DATABASE=nextcloud -e MYSQL_USER=nextcloud -e MYSQL_PASSWORD=nextcloud_password -v /path/to/nextcloud/data:/var/lib/nextcloud -v /path/to/nextcloud/config:/etc/nextcloud -p 80:80 -p 443:443 --restart unless-stopped nextcloud/nextcloud Access NextCloud via your domain name, secured by the VPN. Configure Host Firewall Ensure only WireGuard VPN clients can access NextCloud by restricting ports 80 and 443. Assuming WireGuard subnet is 10.0.0.0/24, use: iptables -A INPUT -s 10.0.0.0/24 -p tcp --dport 80 -j ACCEPTiptables -A INPUT -s 10.0.0.0/24 -p tcp --dport 443 -j ACCEPTiptables -A INPUT -p tcp --dport 80 -j DROPiptables -A INPUT -p tcp --dport 443 -j DROP Adjust the subnet based on your WireGuard setup. Configure WireGuard Clients Install WireGuard on client devices (see WireGuard for instructions). Set up client configurations with server details and connect to the VPN. Access NextCloud via the DuckDNS domain name; traffic will be secure through the VPN. Survey Note: Detailed Installation Guide for WireGuard, NextCloud, and DuckDNS on DockerThis comprehensive guide outlines the installation and configuration of WireGuard, NextCloud, and DuckDNS using Docker, ensuring a secure, self-hosted cloud storage solution with dynamic DNS capabilities. Each step is detailed to cater to users with varying levels of technical expertise, leveraging official and trusted Docker images for reliability.IntroductionThe integration of WireGuard, NextCloud, and DuckDNS via Docker provides a robust setup for remote access to personal cloud storage, secured by a VPN and managed through dynamic DNS. WireGuard offers a fast, secure VPN tunnel, NextCloud serves as a self-hosted cloud storage alternative to services like Dropbox, and DuckDNS ensures domain name resolution despite changing IP addresses. This setup is particularly useful for home servers or VPS environments where IP addresses may not be static.Background and RationaleWireGuard, introduced as a modern VPN protocol, utilizes state-of-the-art cryptography and is designed for simplicity and performance, often outperforming OpenVPN and IPsec. NextCloud, an open-source platform, offers features akin to Google Drive or Microsoft 365, with the advantage of hosting on your own server for enhanced privacy and control. DuckDNS, a free dynamic DNS service, is essential for maintaining domain accessibility with dynamic IPs, commonly encountered in residential internet connections.The decision to use Docker for these installations stems from its containerization benefits, including ease of deployment, isolation, and portability. Docker images from trusted sources like LinuxServer.io and the official NextCloud repository ensure security and regular updates.Detailed Installation Steps1. DuckDNS SetupDuckDNS is not software to install but a service requiring a client to update your IP. The linuxserver/duckdns Docker image is recommended for this purpose, as it is well-maintained and documented. Sign Up and Obtain Credentials: Visit DuckDNS to create an account, obtain a token, and choose a subdomain (e.g., yourname.duckdns.org). Pull the Docker Image: Execute docker pull linuxserver/duckdns to download the image. Run the Container: Use the following command, replacing your_token and your_subdomain with your credentials: docker run -d --name duckdns -e TOKEN=your_token -e SUBDOMAINS=your_subdomain -v /path/to/config:/config --restart unless-stopped linuxserver/duckdns The -v flag mounts a configuration directory for persistence, and --restart unless-stopped ensures the container restarts on system reboot. Verification: Check logs with docker logs duckdns to ensure updates are occurring, typically every 5 minutes by default.Documentation for this setup is available at linuxserver/duckdns documentation.2. WireGuard SetupWireGuard requires the kernel module to be loaded, which can be managed within Docker using the linuxserver/wireguard image, known for its reliability and support for multiple architectures. Pull the Docker Image: Run docker pull linuxserver/wireguard to fetch the image. Run the Container: Launch with necessary capabilities and port mappings: docker run -d --name wireguard --cap-add NET_ADMIN --cap-add SYS_MODULE -v /path/to/wireguard/config:/etc/wireguard -v /lib/modules:/lib/modules -p 51820:51820/UDP --restart unless-stopped linuxserver/wireguard --cap-add NET_ADMIN and --cap-add SYS_MODULE are required for network administration and kernel module loading, respectively. -p 51820:51820/UDP maps the default WireGuard port, adjustable based on your needs. Configuration: Access the container’s /etc/wireguard/wg0.conf to set up the server. Generate keys using wg genkey and wg pubkey for server and clients, defining peers and subnets (e.g., 10.0.0.0/24 for VPN IPs). Client Setup: Distribute client configurations, ensuring they connect to the server’s public IP on port 51820.Refer to linuxserver/wireguard documentation for detailed configuration options.3. NextCloud SetupNextCloud requires a database backend, commonly MariaDB, and can be deployed using the official nextcloud/nextcloud image for ease of use. Database Setup (MariaDB): Run the MariaDB container: docker run -d --name mariadb -e MYSQL_ROOT_PASSWORD=your_password -e MYSQL_DATABASE=nextcloud -e MYSQL_USER=nextcloud -e MYSQL_PASSWORD=nextcloud_password -v /path/to/mariadb/data:/var/lib/mysql --restart unless-stopped mariadb:10.5 Replace passwords and paths as needed, ensuring data persistence with the volume mount. Pull NextCloud Image: Execute docker pull nextcloud/nextcloud to get the latest image. Run NextCloud Container: Link to MariaDB and map ports: docker run -d --name nextcloud --link mariadb:mysql -e MYSQL_HOST=mysql -e MYSQL_DATABASE=nextcloud -e MYSQL_USER=nextcloud -e MYSQL_PASSWORD=nextcloud_password -v /path/to/nextcloud/data:/var/lib/nextcloud -v /path/to/nextcloud/config:/etc/nextcloud -p 80:80 -p 443:443 --restart unless-stopped nextcloud/nextcloud Ports 80 and 443 are mapped for HTTP and HTTPS access, respectively, aligning with standard web traffic. Initial Setup: Access via your DuckDNS domain (e.g., http://yourname.duckdns.org) to complete the web-based installation, setting up admin credentials and additional configurations.Consult NextCloud official documentation for further customization.4. Firewall ConfigurationTo ensure NextCloud is only accessible via the WireGuard VPN, configure the host’s firewall to restrict access to ports 80 and 443 to the WireGuard subnet. Example Configuration: Assuming WireGuard subnet is 10.0.0.0/24, use iptables: iptables -A INPUT -s 10.0.0.0/24 -p tcp --dport 80 -j ACCEPTiptables -A INPUT -s 10.0.0.0/24 -p tcp --dport 443 -j ACCEPTiptables -A INPUT -p tcp --dport 80 -j DROPiptables -A INPUT -p tcp --dport 443 -j DROP This setup blocks external access to NextCloud, allowing only VPN clients with IPs in the specified subnet.Note: Ensure the host machine supports iptables or use a firewall manager like ufw if preferred, adjusting commands accordingly.5. Client Configuration and AccessClients need WireGuard installed to connect to the VPN, then access NextCloud securely. Install WireGuard: Follow WireGuard for client installation on Windows, macOS, Linux, or mobile devices. Configure Client: Use the client configuration file provided by the server, connecting to the server’s public IP on port 51820. Access NextCloud: Once connected, access via the DuckDNS domain (e.g., http://yourname.duckdns.org), with traffic routed through the VPN for security.Integration and Security ConsiderationsThe integration ensures that DuckDNS updates the domain to point to the server’s public IP, WireGuard secures the connection by requiring VPN access, and NextCloud serves as the cloud storage endpoint. The firewall configuration is crucial, restricting NextCloud access to VPN clients, enhancing security against unauthorized access.A surprising detail is that DuckDNS, often misunderstood as software to install, is a service requiring a client like the Docker image to update IPs dynamically, which is essential for home servers with dynamic IPs.Tables for Clarity Service Docker Image Key Configuration Steps Ports Mapped DuckDNS linuxserver/duckdns Set TOKEN, SUBDOMAINS, mount config volume None (background) WireGuard linuxserver/wireguard Configure wg0.conf, map 51820/UDP 51820/UDP NextCloud nextcloud/nextcloud Link to MariaDB, map 80:80, 443:443 80, 443 Firewall Rule Example (iptables) Description -A INPUT -s 10.0.0.0/24 -p tcp –dport 80 -j ACCEPT Allow HTTP from WireGuard subnet -A INPUT -s 10.0.0.0/24 -p tcp –dport 443 -j ACCEPT Allow HTTPS from WireGuard subnet -A INPUT -p tcp –dport 80 -j DROP Drop HTTP from other sources -A INPUT -p tcp –dport 443 -j DROP Drop HTTPS from other sources ConclusionThis setup provides a secure, self-hosted cloud solution with dynamic DNS, leveraging Docker for ease of management. Ensure regular updates to Docker images and monitor logs for any issues, particularly with firewall rules and VPN connectivity.Key Citations linuxserver/duckdns documentation linuxserver/wireguard documentation NextCloud official documentation WireGuard installation guide" }, { "title": "Setting up, contoling and minotoring an IoT system (G)", "url": "/posts/setting-up-contoling-and-minotoring-an-iot-system/", "categories": "Privacy", "tags": "iot, docker, esp32, spring-boot, angular", "date": "2025-02-14 18:12:00 -0500", "snippet": "Setting up an IoT system to control and monitor your garden irrigation, pool pump, indoor/outdoor lights, heater, and other devices with Angular, Spring Boot, and Docker is a great choice, as these technologies align well with a secure, scalable, and maintainable solution. Given your requirements and tech stack, I’ll recommend a microcontroller, outline a setup, and emphasize security considerations.Recommended Microcontroller: ESP32The ESP32 is my top pick for your IoT project due to its versatility, power, and compatibility with your goals: Dual-Core Processor: Handles multiple tasks (e.g., monitoring sensors and controlling devices) efficiently. Wi-Fi and Bluetooth: Built-in connectivity for remote control and integration with your Angular frontend via Spring Boot. GPIO Pins: Plenty of inputs/outputs (around 36) to manage irrigation valves, pool pumps, lights, heaters, and sensors. Low Cost: Around $5-$10 per unit, making it budget-friendly for scaling to multiple devices. Community Support: Extensive libraries and examples for Arduino IDE, MicroPython, or ESP-IDF, easing development. Power Efficiency: Sleep modes for battery-powered outdoor use if needed.Compared to alternatives: Raspberry Pi: More powerful but overkill for basic control, costlier ($35+), and less energy-efficient for simple IoT tasks. Arduino Uno: Cheaper but lacks built-in Wi-Fi, requiring extra modules, and has fewer pins. ESP8266: A predecessor to ESP32, cheaper but less powerful and with fewer pins.System ArchitectureHere’s how you can structure this with ESP32, Angular, Spring Boot, and Docker: Hardware Setup ESP32 Modules: One per control point (e.g., garden, pool, indoor lights). Each connects to: Relays: For switching irrigation valves (e.g., 5V or 24V solenoids), pool pump (120V/240V AC via a suitable relay), lights, and heater. Sensors: Soil moisture (capacitive type), water level (for pool/irrigation tank), temperature (DS18B20 for heater/pool), and light sensors (LDR or BH1750). Power Supply: 5V USB for ESP32, with appropriate AC-DC converters for relays (e.g., HLK-PM01 for 220V to 5V). Firmware on ESP32 Use Arduino IDE or PlatformIO with C++: Libraries: WiFi, HTTPClient (for API calls), OneWire (temperature), PubSubClient (MQTT for real-time updates). Logic: Read sensor data, control relays based on commands, and send status to Spring Boot via REST or MQTT. Example: Toggle a relay for the pool pump when a Spring Boot API endpoint is called. Backend: Spring Boot Purpose: Centralized logic, API, and data storage. Features: REST API endpoints (e.g., /api/devices/{id}/toggle, /api/sensors/{id}/data). MQTT broker integration (e.g., Eclipse Mosquitto) for real-time updates. Database (e.g., PostgreSQL) for logging sensor data and schedules. Docker: Containerize with Dockerfile: FROM openjdk:17-jdk-slimWORKDIR /appCOPY target/your-app.jar app.jarENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;app.jar&quot;] Run with docker run -p 8080:8080 your-app. Frontend: Angular Purpose: User interface for control and monitoring. Features: Dashboard with device status (e.g., irrigation on/off, pool pump runtime, light states). Real-time sensor data via WebSocket or polling. Schedules (e.g., irrigate at 6 AM, turn heater on if temp &amp;lt; 20°C). Docker: Use an Nginx base image to serve the Angular app: FROM node:18 AS buildWORKDIR /appCOPY . .RUN npm install &amp;amp;&amp;amp; ng build --prodFROM nginx:alpineCOPY --from=build /app/dist/your-app /usr/share/nginx/html Run with docker run -p 80:80 your-angular-app. Docker Compose Tie it all together: version: &#39;3&#39;services: backend: build: ./spring-boot ports: - &quot;8080:8080&quot; depends_on: - db - mqtt frontend: build: ./angular ports: - &quot;80:80&quot; db: image: postgres:14 environment: - POSTGRES_DB=iot_db - POSTGRES_USER=user - POSTGRES_PASSWORD=password volumes: - pgdata:/var/lib/postgresql/data mqtt: image: eclipse-mosquitto:2 ports: - &quot;1883:1883&quot; - &quot;9001:9001&quot; # WebSocketvolumes: pgdata: Security ConsiderationsSecurity is critical since this system is internet-facing and controls physical devices: ESP32: Use HTTPS for API calls (self-signed certs or Let’s Encrypt via Spring Boot). Implement OTA updates with password protection (ArduinoOTA.setPassword). Hardcode Wi-Fi credentials securely or use a provisioning step (e.g., Wi-Fi Manager). Spring Boot: Enable Spring Security: JWT authentication for API access. Validate all inputs to prevent injection attacks. Use environment variables for secrets (e.g., database creds in .env with Docker). Angular: Sanitize user inputs to avoid XSS. Use HTTPS for all frontend-backend communication. Network: Place devices behind a VPN (e.g., WireGuard, as in your previous setup) to restrict access. Firewall rules (e.g., iptables): Allow only VPN subnet (e.g., 10.13.13.0/24) to ports 80, 443, 1883. Example: iptables -A INPUT -s 10.13.13.0/24 -p tcp -m multiport --dports 80,443,1883 -j ACCEPTiptables -A INPUT -p tcp -m multiport --dports 80,443,1883 -j DROP General: Regularly update firmware, Docker images, and dependencies. Monitor logs for anomalies (e.g., Spring Boot logs, Mosquitto logs). Implementation Steps Hardware: Wire ESP32s to relays and sensors (e.g., 5V relay for lights, 24V solenoid for irrigation). Firmware: Flash ESP32s with code to poll sensors and call Spring Boot APIs or publish to MQTT. Backend: Set up Spring Boot with endpoints and MQTT subscription. Frontend: Build Angular UI with controls and charts (e.g., Chart.js for sensor data). Docker: Run docker-compose up -d to launch services. Secure: Configure VPN, HTTPS, and firewall rules.Why ESP32 Fits?It’s powerful enough to handle your diverse devices, integrates seamlessly with Spring Boot via REST/MQTT, and supports Angular’s real-time needs through a robust backend. Its low cost and flexibility make it ideal for a garden-to-indoor setup." }, { "title": "IoT system for monitoring a dam water pumping station (G)", "url": "/posts/iot-system-for-monitoring-a-dam-water-pumping-station/", "categories": "Privacy", "tags": "iot, docker, esp32, spring-boot, angular", "date": "2025-02-14 08:35:00 -0500", "snippet": "It seems like you’re asking about an IoT system for monitoring a dam water pumping station, which I’ll interpret as a setup to manage and monitor water pumping operations at a dam—covering aspects like water levels, pump performance, and system health. Given your familiarity with Angular, Spring Boot, and Docker from your previous question, I’ll tailor this response to integrate those technologies with an IoT solution, focusing on the ESP32 microcontroller (as recommended earlier) for its versatility and fit for such a task. Let’s break this down into a practical system design.System OverviewAn IoT system for a dam water pumping station would monitor water levels, pump status (on/off, runtime, faults), flow rates, and environmental conditions (e.g., pressure or temperature), while allowing remote control and real-time alerts. The ESP32 handles the hardware interfacing, Spring Boot serves as the backend, Angular provides the frontend, and Docker containerizes the setup for scalability and deployment ease.Recommended Hardware: ESP32The ESP32 remains ideal here: Wi-Fi: Connects to your network for data transmission. GPIO Pins: Interfaces with sensors and relays for pumps. ADC Channels: Reads analog sensor data (e.g., water level, pressure). Low Power: Suitable for continuous operation at a remote dam site. Cost: Affordable at $5-$10 per unit.Hardware Components ESP32 DevKit: Core microcontroller. Sensors: Ultrasonic Sensor (HC-SR04): Measures water level in the dam or reservoir. Flow Sensor (YF-S201): Tracks water flow rate through the pump. Pressure Sensor (MPX5700): Monitors pump pressure. Current Sensor (ACS712): Detects pump power usage for health monitoring. Relays: Controls pump on/off (e.g., 5V relay for low-power pumps or SSR for high-power AC pumps). Power Supply: 5V for ESP32, with appropriate AC-DC converters for pumps (e.g., 24V or 120V/240V based on pump specs).System Architecture ESP32 Firmware Platform: Arduino IDE or ESP-IDF. Functionality: Poll sensors every few seconds (e.g., water level, flow, pressure, current). Control pump via relay based on commands or thresholds (e.g., turn on if level &amp;gt; 50%, off if &amp;lt; 10%). Send data to Spring Boot via REST API or MQTT for real-time updates. Sample Code Snippet (Arduino): #include &amp;lt;WiFi.h&amp;gt;#include &amp;lt;HTTPClient.h&amp;gt;const char* ssid = &quot;your-wifi&quot;;const char* password = &quot;your-password&quot;;const char* server = &quot;http://your-server:8080/api/data&quot;;#define RELAY_PIN 26void setup() { Serial.begin(115200); WiFi.begin(ssid, password); pinMode(RELAY_PIN, OUTPUT);}void loop() { if (WiFi.status() == WL_CONNECTED) { HTTPClient http; http.begin(server); http.POST(&quot;{\\&quot;level\\&quot;:&quot; + String(getWaterLevel()) + &quot;,\\&quot;flow\\&quot;:&quot; + String(getFlowRate()) + &quot;}&quot;); http.end(); } delay(5000); // Poll every 5 seconds}float getWaterLevel() { /* HC-SR04 logic */ }float getFlowRate() { /* YF-S201 logic */ } Backend: Spring Boot Purpose: Processes data, stores it, and serves the Angular frontend. Features: REST API: /api/data (POST for ESP32 data, GET for frontend). MQTT Broker (e.g., Mosquitto): For real-time pump status. Database: PostgreSQL for historical data (levels, pump runtime). Dockerfile: FROM openjdk:17-jdk-slimCOPY target/dam-monitoring.jar /app.jarENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/app.jar&quot;] Frontend: Angular Purpose: Displays real-time data and controls pumps. Features: Dashboard: Water level gauge, pump status (on/off), flow rate graph. Controls: Toggle pump manually or set automated thresholds. Alerts: Notify if water level is critical or pump fails (via WebSocket/MQTT). Dockerfile: FROM node:18 AS buildWORKDIR /appCOPY . .RUN npm install &amp;amp;&amp;amp; ng build --prodFROM nginx:alpineCOPY --from=build /app/dist/dam-app /usr/share/nginx/html Docker Compose Combines all services: version: &#39;3&#39;services: backend: build: ./spring-boot ports: [ &quot;8080:8080&quot; ] depends_on: [ db, mqtt ] frontend: build: ./angular ports: [ &quot;80:80&quot; ] db: image: postgres:14 environment: - POSTGRES_DB=dam_db - POSTGRES_USER=user - POSTGRES_PASSWORD=password volumes: - pgdata:/var/lib/postgresql/data mqtt: image: eclipse-mosquitto:2 ports: [ &quot;1883:1883&quot;, &quot;9001:9001&quot; ]volumes: pgdata: Security ESP32: Use HTTPS for REST calls (Let’s Encrypt via Spring Boot). MQTT with TLS and username/password authentication. Spring Boot: JWT for API security. Store secrets in environment variables (e.g., docker-compose.yml with .env). Network: Deploy behind a VPN (e.g., WireGuard, as in your prior setup). Firewall: Restrict ports 80, 443, 1883 to VPN subnet: iptables -A INPUT -s 10.13.13.0/24 -p tcp -m multiport --dports 80,443,1883 -j ACCEPTiptables -A INPUT -p tcp -m multiport --dports 80,443,1883 -j DROP Implementation Steps Hardware: Connect ESP32 to sensors and relays (e.g., ultrasonic to GPIO 12/13, relay to GPIO 26). Firmware: Flash ESP32 with code to read sensors and send data. Backend: Deploy Spring Boot with API and MQTT endpoints. Frontend: Build Angular app with real-time UI. Docker: Run docker-compose up -d in your project directory. Test: Monitor via http://your-subdomain.duckdns.org over VPN, ensuring pump control works.Why This Works for a Dam Pumping Station? Scalability: Add more ESP32s for additional pumps or sensors. Real-Time: MQTT ensures instant updates for critical events (e.g., pump failure). Security: VPN and HTTPS protect against unauthorized access. Integration: Leverages your Angular/Spring Boot/Docker skills.If you need specific code, hardware wiring diagrams, or help with deployment, let me know! This setup balances cost, functionality, and security for a dam water pumping station." }, { "title": "How to Secure API", "url": "/posts/how-to-secure-api/", "categories": "DEV", "tags": "api, security", "date": "2025-02-10 21:35:00 -0500", "snippet": "Protecting your API to ensure it can only be accessed by your Single Page Application (SPA) is a common challenge, as SPAs run on the client side and cannot securely store secrets like API keys. However, there are several strategies you can implement to enhance the security of your API and restrict access to only your SPA:1. Use CORS (Cross-Origin Resource Sharing) What it does: CORS allows you to specify which domains are allowed to access your API. How to implement: Configure your API to only accept requests from the domain where your SPA is hosted. Example (in Express.js): const cors = require(&#39;cors&#39;);app.use(cors({ origin: &#39;https://your-spa-domain.com&#39;})); Limitations: CORS can be bypassed if someone manually makes requests from a non-browser environment (e.g., Postman or cURL).2. Use API Keys What it does: Require an API key to access your API. How to implement: Generate a unique API key for your SPA. Include the API key in the headers of every request from your SPA. Validate the API key on the server side. Example (in Express.js): app.use((req, res, next) =&amp;gt; { const apiKey = req.headers[&#39;x-api-key&#39;]; if (apiKey !== &#39;your-secret-api-key&#39;) { return res.status(403).json({ error: &#39;Unauthorized&#39; }); } next();}); Limitations: API keys exposed in client-side code can be easily extracted and misused.3. Use OAuth 2.0 with PKCE (Proof Key for Code Exchange) What it does: OAuth 2.0 with PKCE is a secure way to authenticate SPAs without exposing secrets. How to implement: Use an OAuth 2.0 provider (e.g., Auth0, Okta, or your own implementation). Implement the PKCE flow in your SPA to obtain an access token. Include the access token in the Authorization header of API requests. Validate the token on the server side. Advantages: More secure than API keys, as tokens are short-lived and can be scoped to specific permissions.4. Use a Backend-for-Frontend (BFF) Pattern What it does: Introduce a lightweight backend server that acts as a proxy between your SPA and API. How to implement: Host your SPA and BFF on the same domain. The SPA makes requests to the BFF, which then forwards them to the API. The BFF can securely store and manage API keys or tokens. Example: Use Node.js or a serverless function as the BFF. Advantages: Keeps sensitive credentials out of the client-side code.5. Use HTTPS What it does: Encrypts data in transit to prevent eavesdropping and tampering. How to implement: Ensure your API and SPA are served over HTTPS. Use TLS certificates from a trusted Certificate Authority (CA). 6. Validate Referer and Origin Headers What it does: Check the Referer and Origin headers to ensure requests are coming from your SPA. How to implement: Example (in Express.js): app.use((req, res, next) =&amp;gt; { const referer = req.headers[&#39;referer&#39;]; const origin = req.headers[&#39;origin&#39;]; if (!referer?.startsWith(&#39;https://your-spa-domain.com&#39;) &amp;amp;&amp;amp; !origin?.startsWith(&#39;https://your-spa-domain.com&#39;)) { return res.status(403).json({ error: &#39;Unauthorized&#39; }); } next();}); Limitations: These headers can be spoofed, so this should not be your only line of defense.7. Use Rate Limiting What it does: Prevents abuse by limiting the number of requests from a single client. How to implement: Use a library like express-rate-limit (for Node.js) or configure rate limiting in your API gateway. Example (in Express.js): const rateLimit = require(&#39;express-rate-limit&#39;);const limiter = rateLimit({ windowMs: 15 * 60 * 1000, // 15 minutes max: 100 // limit each IP to 100 requests per windowMs});app.use(limiter); 8. Use Signed Requests What it does: Sign requests with a secret key to ensure they originate from your SPA. How to implement: Generate a signature on the client side using a secret key (e.g., HMAC). Include the signature in the request headers. Validate the signature on the server side. Limitations: The secret key must be embedded in the SPA, which can still be extracted.9. Use a Web Application Firewall (WAF) What it does: Protects your API from common attacks like SQL injection, XSS, and DDoS. How to implement: Use a cloud-based WAF (e.g., AWS WAF, Cloudflare) or a self-hosted solution. 10. Monitor and Log Requests What it does: Helps you detect and respond to suspicious activity. How to implement: Log all incoming requests to your API. Use tools like ELK Stack (Elasticsearch, Logstash, Kibana) or cloud-based monitoring services. Best Practices Summary Use CORS to restrict access to your SPA’s domain. Use OAuth 2.0 with PKCE for secure authentication. Consider a BFF pattern to keep credentials secure. Always use HTTPS to encrypt data in transit. Implement rate limiting and monitoring to detect and prevent abuse.By combining these strategies, you can significantly improve the security of your API and ensure it is only accessible by your SPA." }, { "title": "Is Deepin safe and secure?", "url": "/posts/is-deepin-safe/", "categories": "Linux", "tags": "deepin, privacy, security", "date": "2025-02-10 20:35:00 -0500", "snippet": "Deepin Linux is a user-friendly and visually appealing Linux distribution based on Debian. Like other Linux distributions, its security and privacy depend on several factors, including how it is configured, maintained, and used. Here’s a breakdown of its security and privacy aspects:Security of Deepin Linux Open Source Nature: Deepin is open source, meaning its code can be audited by the community for vulnerabilities or malicious code. This transparency generally enhances security. Regular Updates: Deepin receives regular updates, including security patches from Debian and its own team. Keeping the system updated is crucial for maintaining security. Built-in Security Features: Like most Linux distributions, Deepin includes standard security features such as: User privilege management (e.g., sudo for administrative tasks). Firewall tools (e.g., UFW or iptables). AppArmor or SELinux for mandatory access control (though these may require manual configuration). Third-Party Software: Deepin’s App Store provides access to a wide range of software. However, users should be cautious when installing third-party applications, as they may introduce vulnerabilities. Potential Risks: Deepin has faced scrutiny in the past due to its Chinese origins, with some users expressing concerns about potential backdoors or data collection. However, there is no concrete evidence of such issues, and the codebase is open for inspection. Privacy in Deepin Linux Data Collection: Deepin has been criticized in the past for including features that could potentially send data to servers in China (e.g., the Deepin Store and some default applications). However, these features can usually be disabled or replaced with alternatives. User Control: As with most Linux distributions, you have significant control over your system. You can: Disable or uninstall applications that may collect data. Use privacy-focused alternatives for default apps (e.g., Firefox instead of the Deepin Browser). Configure network settings to block unwanted connections. Encryption: Deepin supports full-disk encryption during installation, which helps protect your data in case of physical access to your device. Community Trust: While Deepin is popular for its aesthetics and ease of use, some privacy-conscious users prefer other distributions like Debian, Fedora, or Ubuntu due to their stricter privacy policies and broader community trust. Steps to Enhance Security and Privacy on Deepin Disable Unnecessary Services: Turn off any features or services that you don’t use, especially those that connect to the internet. Use a Firewall: Enable and configure a firewall to restrict incoming and outgoing connections. Install Security Tools: Use tools like ClamAV (antivirus), rkhunter (rootkit detection), and fail2ban (intrusion prevention). Avoid Default Apps: Replace default applications with privacy-focused alternatives (e.g., use Brave or Firefox instead of Deepin Browser). Monitor Network Activity: Use tools like Wireshark or netstat to monitor network traffic and identify suspicious connections. Stay Updated: Regularly update your system and applications to patch vulnerabilities. ConclusionDeepin Linux is generally secure, but its privacy practices have raised concerns in the past. If you prioritize privacy, you may want to: Disable or replace default applications that could collect data. Consider using a more privacy-focused distribution like Debian, Fedora, or Tails.Ultimately, your security and privacy on Deepin (or any OS) depend on how you configure and use the system. If you take the necessary precautions, Deepin can be a secure and private operating system." }, { "title": "Implementing automated alerts", "url": "/posts/Implementing-automated-alerts/", "categories": "Architecture", "tags": "telgram, alert, bot, spring-boot, email", "date": "2025-02-10 10:35:00 -0500", "snippet": "Implementing automated alerts for your IoT system (e.g., for garden irrigation, pool pump, or heater) is a great way to stay informed about system status, anomalies, or failures. Below, I’ll guide you through setting up Telegram and email alerts using Spring Boot as the backend.1. Telegram AlertsTelegram is a popular choice for real-time notifications because of its simplicity and free API.Step 1: Create a Telegram Bot Open Telegram and search for the BotFather. Use the /newbot command to create a new bot. Follow the instructions to name your bot and get the API token (e.g., 123456789:ABCdefGhIJKlmNoPQRstuVWXyz).Step 2: Get Your Chat ID Start a chat with your bot on Telegram. Send a message (e.g., “Hello”). Use the following URL to get your chat ID: https://api.telegram.org/bot&amp;lt;YOUR_API_TOKEN&amp;gt;/getUpdates Replace &amp;lt;YOUR_API_TOKEN&amp;gt; with your bot’s token. Look for the chat.id field in the JSON response.Step 3: Send Alerts from Spring BootUse the Telegram Bot API to send messages from your Spring Boot backend.Dependency:Add the following to your pom.xml:&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;Service Class:Create a TelegramService to send messages:import org.springframework.stereotype.Service;import org.springframework.web.client.RestTemplate;@Servicepublic class TelegramService { private static final String TELEGRAM_API_URL = &quot;https://api.telegram.org/bot&amp;lt;YOUR_API_TOKEN&amp;gt;/sendMessage&quot;; private static final String CHAT_ID = &quot;&amp;lt;YOUR_CHAT_ID&amp;gt;&quot;; private final RestTemplate restTemplate = new RestTemplate(); public void sendAlert(String message) { String url = TELEGRAM_API_URL + &quot;?chat_id=&quot; + CHAT_ID + &quot;&amp;amp;text=&quot; + message; restTemplate.getForObject(url, String.class); }}Usage:Call the sendAlert method whenever you need to send a notification:@Autowiredprivate TelegramService telegramService;public void checkSoilMoisture() { if (soilMoisture &amp;lt; threshold) { telegramService.sendAlert(&quot;⚠️ Low soil moisture detected! Triggering irrigation.&quot;); }}2. Email AlertsEmail alerts are useful for less urgent notifications or when Telegram is not an option.Step 1: Configure SMTP in Spring BootSpring Boot makes it easy to send emails using JavaMail.Dependency:Add the following to your pom.xml:&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-mail&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;Configuration:Add your SMTP server details to application.properties:spring.mail.host=smtp.gmail.comspring.mail.port=587spring.mail.username=your-email@gmail.comspring.mail.password=your-email-passwordspring.mail.properties.mail.smtp.auth=truespring.mail.properties.mail.smtp.starttls.enable=trueService Class:Create an EmailService to send emails:import org.springframework.beans.factory.annotation.Autowired;import org.springframework.mail.SimpleMailMessage;import org.springframework.mail.javamail.JavaMailSender;import org.springframework.stereotype.Service;@Servicepublic class EmailService { @Autowired private JavaMailSender mailSender; public void sendAlert(String to, String subject, String text) { SimpleMailMessage message = new SimpleMailMessage(); message.setTo(to); message.setSubject(subject); message.setText(text); mailSender.send(message); }}Usage:Call the sendAlert method to send an email:@Autowiredprivate EmailService emailService;public void checkPoolPump() { if (poolPumpStatus.equals(&quot;OFF&quot;)) { emailService.sendAlert( &quot;your-email@gmail.com&quot;, &quot;Pool Pump Alert&quot;, &quot;⚠️ Pool pump is offline! Please check the system.&quot; ); }}3. Combining AlertsYou can combine Telegram and email alerts for redundancy or different levels of urgency. For example: Use Telegram for real-time, high-priority alerts. Use Email for daily summaries or less urgent notifications.Example:public void handleAlert(String message, boolean isUrgent) { if (isUrgent) { telegramService.sendAlert(message); } emailService.sendAlert(&quot;your-email@gmail.com&quot;, &quot;System Alert&quot;, message);}4. Advanced Featuresa. Templated EmailsUse Thymeleaf to send HTML emails with dynamic content.Dependency:&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-thymeleaf&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;Template:Create a templates/alert-email.html file:&amp;lt;!DOCTYPE html&amp;gt;&amp;lt;html xmlns:th=&quot;http://www.thymeleaf.org&quot;&amp;gt;&amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Alert&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt; &amp;lt;h1&amp;gt;⚠️ Alert&amp;lt;/h1&amp;gt; &amp;lt;p th:text=&quot;${message}&quot;&amp;gt;&amp;lt;/p&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;Service Class:import org.springframework.beans.factory.annotation.Autowired;import org.springframework.mail.javamail.JavaMailSender;import org.springframework.mail.javamail.MimeMessageHelper;import org.springframework.stereotype.Service;import org.thymeleaf.TemplateEngine;import org.thymeleaf.context.Context;import javax.mail.MessagingException;import javax.mail.internet.MimeMessage;@Servicepublic class EmailService { @Autowired private JavaMailSender mailSender; @Autowired private TemplateEngine templateEngine; public void sendHtmlAlert(String to, String subject, String message) throws MessagingException { MimeMessage mimeMessage = mailSender.createMimeMessage(); MimeMessageHelper helper = new MimeMessageHelper(mimeMessage, true); Context context = new Context(); context.setVariable(&quot;message&quot;, message); String htmlContent = templateEngine.process(&quot;alert-email&quot;, context); helper.setTo(to); helper.setSubject(subject); helper.setText(htmlContent, true); mailSender.send(mimeMessage); }}5. Testing Test Telegram alerts by triggering a low soil moisture condition. Test email alerts by simulating a pool pump failure.6. Final Notes Rate Limiting: Avoid spamming users with too many alerts. Error Handling: Handle exceptions (e.g., network issues, invalid credentials). Logging: Log all alerts for auditing purposes.Let me know if you need help with specific configurations or additional features! 😊Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "Full-stack IoT example", "url": "/posts/IoT-control-with-Angular-SpringBoot/", "categories": "Architecture", "tags": "architecture, angular, spring-boot, sse, docker, real-time, postgreSQL, prometheus, grafana", "date": "2025-02-10 09:35:00 -0500", "snippet": " Okay, the user wants a full functional implementation example for their IoT system using Angular, Spring Boot, and Docker, along with how to measure performance and capacity. Let me start by breaking down their needs.First, they need a complete example that integrates all the mentioned technologies. They’re working on a home automation project controlling garden irrigation, pool pump, lights, etc. The example should cover real-time updates using SSE, which I previously explained. They also want to know how to test the system’s performance and capacity. I should outline the architecture again briefly, then dive into the code. For the backend, Spring Boot will handle SSE, device management, and alerts. The frontend in Angular will display real-time data. Docker will containerize the backend and database.Including steps for each component: Spring Boot setup with SSE endpoints, Angular service and component for consuming SSE, Docker setup with PostgreSQL. Then, performance measurement using tools like JMeter, monitoring with Spring Boot Actuator and Prometheus/Grafana.I need to make sure the code examples are accurate and functional. For example, the SseEmitter in Spring Boot, Angular’s EventSource, and Docker Compose configuration. Also, explain how to use JMeter for load testing and interpret results like throughput and error rates.They might be looking for practical steps they can follow without much hassle, so clarity is key. Including commands for Docker, dependencies in pom.xml and package.json, and sample configurations will help them set things up quickly.For performance measurement, explaining tools and metrics clearly will help them understand how to assess their system’s limits. Mentioning scalability tips like horizontal scaling and connection pooling is important for capacity planning.I should also remind them about security aspects, even though the example is simplified. Maybe a note about adding authentication and using HTTPS in production.Finally, summarize the example and emphasize the importance of monitoring and scaling for a reliable system. Make sure the tone is encouraging and offers further help if needed.Let’s build a full-stack example of a real-time IoT monitoring system using Angular, Spring Boot, and Docker, with Server-Sent Events (SSE) for updates. We’ll also cover performance testing and capacity measurement. Here’s the step-by-step implementation:1. System Overview Backend: Spring Boot (Java) with SSE for real-time updates. Frontend: Angular for real-time dashboard. Database: PostgreSQL (via Docker). Monitoring: Prometheus + Grafana (for performance metrics). Deployment: Docker Compose.2. Backend Implementation (Spring Boot)Step 1: Project SetupCreate a Spring Boot project with these dependencies (pom.xml):&amp;lt;dependencies&amp;gt; &amp;lt;!-- Core --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- Database --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-data-jpa&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.postgresql&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;postgresql&amp;lt;/artifactId&amp;gt; &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- Monitoring --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;io.micrometer&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;micrometer-registry-prometheus&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt;&amp;lt;/dependencies&amp;gt;Step 2: SSE Controller// SseController.javaimport org.springframework.http.MediaType;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.servlet.mvc.method.annotation.SseEmitter;import java.io.IOException;import java.util.concurrent.CopyOnWriteArrayList;@RestController@RequestMapping(&quot;/api/sse&quot;)public class SseController { private final CopyOnWriteArrayList&amp;lt;SseEmitter&amp;gt; emitters = new CopyOnWriteArrayList&amp;lt;&amp;gt;(); @GetMapping(produces = MediaType.TEXT_EVENT_STREAM_VALUE) public SseEmitter subscribe() { SseEmitter emitter = new SseEmitter(Long.MAX_VALUE); // No timeout emitters.add(emitter); emitter.onCompletion(() -&amp;gt; emitters.remove(emitter)); emitter.onTimeout(() -&amp;gt; emitters.remove(emitter)); // Send initial message try { emitter.send(SseEmitter.event().data(&quot;Connected!&quot;)); } catch (IOException e) { emitter.completeWithError(e); } return emitter; } public void sendEvent(String data) { for (SseEmitter emitter : emitters) { try { emitter.send(SseEmitter.event().data(data)); } catch (IOException e) { emitters.remove(emitter); } } }}Step 3: Device Simulation ServiceSimulate IoT device data and send SSE updates:// DeviceSimulatorService.javaimport org.springframework.scheduling.annotation.Scheduled;import org.springframework.stereotype.Service;@Servicepublic class DeviceSimulatorService { private final SseController sseController; public DeviceSimulatorService(SseController sseController) { this.sseController = sseController; } @Scheduled(fixedRate = 2000) // Send data every 2 seconds public void simulateDeviceData() { double temperature = Math.random() * 40; // Simulate temperature (0-40°C) String data = String.format(&quot;{\\&quot;temperature\\&quot;: %.2f}&quot;, temperature); sseController.sendEvent(data); }}Step 4: Configure PostgreSQLAdd to application.properties:spring.datasource.url=jdbc:postgresql://db:5432/iotspring.datasource.username=postgresspring.datasource.password=postgresspring.jpa.hibernate.ddl-auto=update3. Frontend Implementation (Angular)Step 1: SSE Service// sse.service.tsimport { Injectable } from &#39;@angular/core&#39;;import { Observable } from &#39;rxjs&#39;;@Injectable({ providedIn: &#39;root&#39;,})export class SseService { private eventSource: EventSource; constructor() {} getServerSentEvents(url: string): Observable&amp;lt;MessageEvent&amp;gt; { return new Observable((observer) =&amp;gt; { this.eventSource = new EventSource(url); this.eventSource.onmessage = (event) =&amp;gt; { observer.next(event); }; this.eventSource.onerror = (error) =&amp;gt; { observer.error(error); }; }); } closeConnection() { if (this.eventSource) { this.eventSource.close(); } }}Step 2: Dashboard Component// dashboard.component.tsimport { Component, OnInit, OnDestroy } from &#39;@angular/core&#39;;import { SseService } from &#39;../sse.service&#39;;import { Subscription } from &#39;rxjs&#39;;@Component({ selector: &#39;app-dashboard&#39;, template: ` &amp;lt;h1&amp;gt;Real-Time Temperature&amp;lt;/h1&amp;gt; &amp;lt;div *ngIf=&quot;temperature !== null&quot;&amp;gt; Current Temperature: {{ temperature }}°C &amp;lt;/div&amp;gt; `,})export class DashboardComponent implements OnInit, OnDestroy { temperature: number | null = null; private sseSubscription!: Subscription; constructor(private sseService: SseService) {} ngOnInit() { this.sseSubscription = this.sseService .getServerSentEvents(&#39;http://localhost:8080/api/sse&#39;) .subscribe({ next: (event: MessageEvent) =&amp;gt; { const data = JSON.parse(event.data); this.temperature = data.temperature; }, error: (err) =&amp;gt; console.error(&#39;SSE error:&#39;, err), }); } ngOnDestroy() { this.sseSubscription.unsubscribe(); this.sseService.closeConnection(); }}4. Docker Setupdocker-compose.ymlversion: &#39;3.8&#39;services: backend: build: ./backend ports: - &quot;8080:8080&quot; environment: SPRING_DATASOURCE_URL: jdbc:postgresql://db:5432/iot depends_on: - db db: image: postgres:13 environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: postgres POSTGRES_DB: iot ports: - &quot;5432:5432&quot; volumes: - pgdata:/var/lib/postgresql/data prometheus: image: prom/prometheus ports: - &quot;9090:9090&quot; volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml grafana: image: grafana/grafana ports: - &quot;3000:3000&quot;volumes: pgdata:Dockerfile (Backend)FROM openjdk:17-jdk-slimWORKDIR /appCOPY target/*.jar app.jarENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;app.jar&quot;]5. Performance TestingTools: JMeter: For load testing SSE connections. Prometheus + Grafana: For monitoring system metrics (CPU, memory, connections). Spring Boot Actuator: Exposes metrics endpoints.Step 1: Enable Actuator in Spring BootAdd to application.properties:management.endpoints.web.exposure.include=health,metrics,prometheusStep 2: Configure PrometheusCreate prometheus.yml:global: scrape_interval: 15sscrape_configs: - job_name: &#39;spring-boot&#39; metrics_path: &#39;/actuator/prometheus&#39; static_configs: - targets: [&#39;backend:8080&#39;]Step 3: Run JMeter Test Test Plan: Simulate 100 concurrent users connecting to /api/sse. Measure throughput, response time, and error rate. Results: Throughput: Number of requests per second the system can handle. Latency: Average response time for SSE connections. Error Rate: Percentage of failed connections. 6. Capacity Measurement Connection Limit: Test how many SSE connections your backend can handle (e.g., 10k connections). Resource Usage: Monitor CPU/memory usage under load (use docker stats or Grafana). Database Throughput: Measure PostgreSQL queries per second.Scaling Tips: Horizontal Scaling: Deploy multiple backend instances behind a load balancer. Connection Pooling: Optimize PostgreSQL connection pooling with HikariCP. Asynchronous Processing: Use reactive programming (Spring WebFlux) for higher concurrency.7. Run the System Build the Spring Boot app: cd backend &amp;amp;&amp;amp; mvn clean package Start the stack: docker-compose up --build Access: Angular: http://localhost:4200 Prometheus: http://localhost:9090 Grafana: http://localhost:3000 8. Final Notes Security: Add JWT authentication for SSE endpoints (not shown here for simplicity). Optimization: Use @Async in Spring Boot to handle SSE emitters non-blockingly. Alerting: Configure Grafana alerts for high CPU/memory usage or connection drops.This example gives you a foundation to build upon. Let me know if you need help with specific scenarios! 😊Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "Server-Sent Events (SSE)", "url": "/posts/SSE/", "categories": "DEV", "tags": "sse, architecture, notification, real-time, event, angular, spring-boot", "date": "2025-02-10 08:35:00 -0500", "snippet": "Server-Sent Events (SSE) is a lightweight, HTTP-based protocol for real-time, one-way communication from the server to the client. It’s ideal for scenarios where the server needs to push updates to the client (e.g., real-time notifications, live data feeds). Below, I’ll explain when to use SSE, how it works, and how to implement it in your Spring Boot backend and Angular frontend.1. When to Use SSESSE is a great choice when: You need real-time updates from the server to the client. The communication is one-way (server → client). You want a simple, HTTP-based solution (no need for WebSocket complexity). The client is a web browser (SSE is natively supported in modern browsers).Use Cases: Live dashboards (e.g., IoT device status, sensor data). Notifications (e.g., alerts, system events). Stock tickers, news feeds, or sports scores.2. How SSE Works The client initiates a connection to the server using an HTTP request. The server keeps the connection open and sends updates as text/event-stream. Each update is a plain text message in a specific format (e.g., data: {&quot;message&quot;: &quot;Hello&quot;}\\n\\n). The client listens for these messages and processes them in real time.3. SSE vs WebSockets| Feature | SSE | WebSockets ||————————-|———————————————-|—————————————–|| Direction | One-way (server → client) | Two-way (client ↔ server) || Protocol | HTTP | WebSocket (ws:// or wss://) || Complexity | Simple | More complex || Browser Support | Native support in modern browsers | Native support in modern browsers || Use Case | Real-time updates, notifications | Real-time bidirectional communication |4. Implementing SSE in Spring BootHere’s how to implement SSE in your Spring Boot backend:Step 1: Add DependenciesEnsure you have the spring-boot-starter-web dependency in your pom.xml:&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;&amp;lt;/dependency&amp;gt;Step 2: Create an SSE EndpointUse SseEmitter to send events to the client.Controller:import org.springframework.http.MediaType;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.servlet.mvc.method.annotation.SseEmitter;import java.io.IOException;import java.util.concurrent.CopyOnWriteArrayList;@RestController@RequestMapping(&quot;/sse&quot;)public class SseController { private final CopyOnWriteArrayList&amp;lt;SseEmitter&amp;gt; emitters = new CopyOnWriteArrayList&amp;lt;&amp;gt;(); @GetMapping(path = &quot;/updates&quot;, produces = MediaType.TEXT_EVENT_STREAM_VALUE) public SseEmitter subscribe() { SseEmitter emitter = new SseEmitter(Long.MAX_VALUE); // No timeout emitters.add(emitter); // Remove the emitter when the client disconnects emitter.onCompletion(() -&amp;gt; emitters.remove(emitter)); emitter.onTimeout(() -&amp;gt; emitters.remove(emitter)); return emitter; } public void sendEvent(String data) { for (SseEmitter emitter : emitters) { try { emitter.send(SseEmitter.event().data(data)); } catch (IOException e) { emitters.remove(emitter); } } }}Service:Trigger events from your service layer:import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class SseService { @Autowired private SseController sseController; public void sendUpdate(String message) { sseController.sendEvent(message); }}Usage:Call sendUpdate to push updates to all connected clients:@Autowiredprivate SseService sseService;public void checkSoilMoisture() { if (soilMoisture &amp;lt; threshold) { sseService.sendUpdate(&quot;Low soil moisture detected!&quot;); }}5. Implementing SSE in AngularHere’s how to consume SSE in your Angular frontend:Step 1: Create an SSE ServiceCreate a service to handle the SSE connection.Service:import { Injectable } from &#39;@angular/core&#39;;import { Observable } from &#39;rxjs&#39;;@Injectable({ providedIn: &#39;root&#39;,})export class SseService { private eventSource: EventSource; constructor() {} getServerSentEvents(url: string): Observable&amp;lt;string&amp;gt; { return new Observable((observer) =&amp;gt; { this.eventSource = new EventSource(url); this.eventSource.onmessage = (event) =&amp;gt; { observer.next(event.data); }; this.eventSource.onerror = (error) =&amp;gt; { observer.error(error); }; }); } closeConnection() { if (this.eventSource) { this.eventSource.close(); } }}Step 2: Consume SSE in a ComponentUse the service to listen for updates.Component:import { Component, OnInit, OnDestroy } from &#39;@angular/core&#39;;import { SseService } from &#39;./sse.service&#39;;import { Subscription } from &#39;rxjs&#39;;@Component({ selector: &#39;app-root&#39;, template: ` &amp;lt;h1&amp;gt;Real-Time Updates&amp;lt;/h1&amp;gt; &amp;lt;div *ngFor=&quot;let message of messages&quot;&amp;gt;{{ message }}&amp;lt;/div&amp;gt; `,})export class AppComponent implements OnInit, OnDestroy { messages: string[] = []; private sseSubscription: Subscription; constructor(private sseService: SseService) {} ngOnInit() { this.sseSubscription = this.sseService .getServerSentEvents(&#39;http://localhost:8080/sse/updates&#39;) .subscribe({ next: (message) =&amp;gt; this.messages.push(message), error: (err) =&amp;gt; console.error(&#39;SSE error:&#39;, err), }); } ngOnDestroy() { this.sseSubscription.unsubscribe(); this.sseService.closeConnection(); }}6. Testing the Implementation Start your Spring Boot application. Open the Angular app in a browser. Trigger updates from your backend (e.g., simulate low soil moisture). Verify that the updates appear in real time on the Angular frontend.7. Advanced Featuresa. Event TypesYou can send different types of events by specifying an event name:emitter.send(SseEmitter.event().name(&quot;soil-moisture&quot;).data(&quot;Low moisture detected!&quot;));In Angular, listen for specific event types:this.eventSource.addEventListener(&#39;soil-moisture&#39;, (event) =&amp;gt; { console.log(&#39;Soil moisture event:&#39;, event.data);});b. ReconnectionSSE automatically reconnects if the connection is lost. You can customize the reconnection delay:emitter.send(SseEmitter.event().reconnectTime(5000)); // Reconnect after 5 secondsc. Error HandlingHandle errors gracefully in Angular:this.eventSource.onerror = (error) =&amp;gt; { console.error(&#39;SSE error:&#39;, error); // Optionally reconnect};8. When Not to Use SSE Bidirectional Communication: Use WebSockets if the client needs to send data to the server. High-Frequency Updates: SSE can be less efficient than WebSockets for very high-frequency updates. Legacy Browser Support: SSE is not supported in Internet Explorer.Let me know if you need further assistance with SSE or any other part of your system! 😊" }, { "title": "How the service can authenticates the device", "url": "/posts/how-the-service-can-authenticates-the-device/", "categories": "Architecture", "tags": "authentication, security", "date": "2025-02-09 21:35:00 -0500", "snippet": "Device authentication is a critical part of the Device Ingestion Service to ensure that only authorized devices can send data to your system. There are several methods to authenticate devices, each with its own pros and cons. Below, I’ll explain the most common approaches and provide implementation details.1. Authentication Methods for Devicesa. API Keys How It Works: Each device is assigned a unique API key. The device includes the API key in its requests (e.g., in the HTTP header, MQTT topic, or WebSocket message). The service validates the API key against a database or in-memory store. Pros: Simple to implement. Lightweight and fast. Cons: API keys can be compromised if not transmitted securely (e.g., over HTTPS or TLS). Requires secure storage and rotation of keys. b. Mutual TLS (mTLS) How It Works: Each device has a client certificate issued by a trusted Certificate Authority (CA). The device presents its certificate when establishing a connection (e.g., MQTT over TLS or HTTPS). The service validates the certificate and authenticates the device. Pros: Highly secure. No need to transmit sensitive credentials (e.g., API keys) over the network. Cons: Requires certificate management (issuance, renewal, revocation). Adds complexity to the setup. c. JSON Web Tokens (JWT) How It Works: Devices are issued a JWT (signed by a private key) during registration. The device includes the JWT in its requests (e.g., in the HTTP header or MQTT payload). The service validates the JWT using the corresponding public key. Pros: Stateless and scalable. Can include additional claims (e.g., device permissions, expiration). Cons: Requires secure key management. JWTs can be large, increasing network overhead. d. OAuth 2.0 (Device Flow) How It Works: Devices use the OAuth 2.0 Device Flow to obtain an access token. The device includes the access token in its requests. The service validates the token with the OAuth 2.0 authorization server. Pros: Standardized and widely used. Supports fine-grained access control. Cons: More complex to implement. Requires an OAuth 2.0 authorization server. 2. Implementation Examplesa. API Key AuthenticationHere’s how you can implement API key authentication in the Device Ingestion Service:HTTP Example (Node.js with Express):const express = require(&#39;express&#39;);const app = express();app.use(express.json());// In-memory store for API keys (replace with a database in production)const validApiKeys = new Set([&#39;device1-api-key&#39;, &#39;device2-api-key&#39;]);app.post(&#39;/data&#39;, (req, res) =&amp;gt; { const apiKey = req.headers[&#39;x-api-key&#39;]; if (!apiKey || !validApiKeys.has(apiKey)) { return res.status(401).send(&#39;Unauthorized&#39;); } const data = req.body; if (validateData(data)) { forwardToMessageBroker(data); res.status(200).send(&#39;Data received&#39;); } else { res.status(400).send(&#39;Invalid data&#39;); }});app.listen(3000, () =&amp;gt; { console.log(&#39;HTTP server listening on port 3000&#39;);});MQTT Example (Node.js with mqtt.js):const mqtt = require(&#39;mqtt&#39;);const client = mqtt.connect(&#39;mqtt://broker.example.com&#39;);// In-memory store for API keys (replace with a database in production)const validApiKeys = new Set([&#39;device1-api-key&#39;, &#39;device2-api-key&#39;]);client.on(&#39;connect&#39;, () =&amp;gt; { client.subscribe(&#39;devices/+/data&#39;, (err) =&amp;gt; { if (!err) console.log(&#39;Subscribed to device data topics&#39;); });});client.on(&#39;message&#39;, (topic, message) =&amp;gt; { const data = JSON.parse(message.toString()); const apiKey = data.apiKey; // API key included in the payload if (!apiKey || !validApiKeys.has(apiKey)) { console.error(&#39;Unauthorized device:&#39;, data.device_id); return; } if (validateData(data)) { forwardToMessageBroker(data); } else { console.error(&#39;Invalid data received:&#39;, data); }});b. Mutual TLS (mTLS) AuthenticationHere’s how you can implement mTLS for device authentication:MQTT with mTLS (Mosquitto Broker): Generate certificates for the CA, server, and devices: # Generate CA certificateopenssl req -new -x509 -days 365 -extensions v3_ca -keyout ca.key -out ca.crt# Generate server certificateopenssl genrsa -out server.key 2048openssl req -new -out server.csr -key server.keyopenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365# Generate device certificateopenssl genrsa -out device1.key 2048openssl req -new -out device1.csr -key device1.keyopenssl x509 -req -in device1.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out device1.crt -days 365 Configure the Mosquitto broker to use mTLS: # mosquitto.conflistener 8883cafile /path/to/ca.crtcertfile /path/to/server.crtkeyfile /path/to/server.keyrequire_certificate true Devices connect to the broker using their client certificates: const fs = require(&#39;fs&#39;);const mqtt = require(&#39;mqtt&#39;);const client = mqtt.connect(&#39;mqtts://broker.example.com:8883&#39;, { key: fs.readFileSync(&#39;/path/to/device1.key&#39;), cert: fs.readFileSync(&#39;/path/to/device1.crt&#39;), ca: fs.readFileSync(&#39;/path/to/ca.crt&#39;), rejectUnauthorized: true,});client.on(&#39;connect&#39;, () =&amp;gt; { client.publish(&#39;devices/device1/data&#39;, JSON.stringify({ sensor_value: 25 }));}); c. JWT AuthenticationHere’s how you can implement JWT authentication:HTTP Example (Node.js with Express and jsonwebtoken):const express = require(&#39;express&#39;);const jwt = require(&#39;jsonwebtoken&#39;);const app = express();app.use(express.json());const SECRET_KEY = &#39;your-secret-key&#39;;app.post(&#39;/data&#39;, (req, res) =&amp;gt; { const token = req.headers[&#39;authorization&#39;]; if (!token) { return res.status(401).send(&#39;Unauthorized&#39;); } try { const decoded = jwt.verify(token, SECRET_KEY); const deviceId = decoded.deviceId; // Extract device ID from the token const data = req.body; if (validateData(data)) { forwardToMessageBroker(data); res.status(200).send(&#39;Data received&#39;); } else { res.status(400).send(&#39;Invalid data&#39;); } } catch (err) { res.status(401).send(&#39;Invalid token&#39;); }});app.listen(3000, () =&amp;gt; { console.log(&#39;HTTP server listening on port 3000&#39;);});3. Choosing the Right Method For simplicity: Use API keys. For high security: Use mTLS. For scalability and flexibility: Use JWT or OAuth 2.0.If you’d like, I can help you implement any of these methods in your system or provide more details on a specific approach. Let me know! 😊" }, { "title": "How Mutual TLS (mTLS) works", "url": "/posts/how-mutual-TLS-(mTLS)-works/", "categories": "Architecture", "tags": "mtls, protocol, certificate", "date": "2025-02-09 19:35:00 -0500", "snippet": "Mutual TLS (mTLS) is a security protocol that ensures both the client and the server authenticate each other using TLS (Transport Layer Security) certificates. It adds an extra layer of security compared to standard TLS, where only the server is authenticated. Here’s a detailed explanation of how mTLS works:1. Key Concepts TLS (Transport Layer Security): A cryptographic protocol that ensures secure communication over a network. Certificates: Digital documents that verify the identity of a device or server. Certificate Authority (CA): A trusted entity that issues and signs certificates. Public Key Infrastructure (PKI): A framework for managing certificates and public-key encryption.2. How mTLS WorksmTLS involves the following steps:Step 1: Certificate Setup Generate Certificates: A Certificate Authority (CA) generates a root certificate and private key. The CA issues certificates for the server and each client (device). Each certificate includes: A public key. Identity information (e.g., server name, device ID). A digital signature from the CA. Distribute Certificates: The server and clients are configured with their respective certificates and private keys. The CA’s root certificate is distributed to all parties to establish trust. Step 2: TLS HandshakeWhen a client (device) connects to the server, the following steps occur: Client Hello: The client sends a “Client Hello” message to the server, indicating supported TLS versions and cipher suites. Server Hello: The server responds with a “Server Hello” message, selecting the TLS version and cipher suite. The server sends its certificate to the client. Client Verification: The client verifies the server’s certificate using the CA’s root certificate. If the server’s certificate is valid, the client generates a pre-master secret, encrypts it with the server’s public key, and sends it to the server. Server Verification: The server requests the client’s certificate. The client sends its certificate to the server. The server verifies the client’s certificate using the CA’s root certificate. Key Exchange: Both the client and server use the pre-master secret to generate a shared session key. This session key is used to encrypt and decrypt data during the session. Secure Communication: Once the handshake is complete, both parties use the session key to encrypt and decrypt data. 3. Benefits of mTLS Strong Authentication: Ensures both the client and server are who they claim to be. Data Integrity: Prevents tampering with data during transmission. Confidentiality: Encrypts data to prevent eavesdropping. Prevents Impersonation: Protects against man-in-the-middle (MITM) attacks.4. Use Cases for mTLS IoT Devices: Authenticating devices in an IoT ecosystem. Microservices: Securing communication between microservices in a distributed system. APIs: Protecting sensitive APIs (e.g., financial or healthcare systems). VPNs: Establishing secure connections in virtual private networks.5. Example: Implementing mTLSHere’s how you can set up mTLS for a server and client:Step 1: Generate Certificates Generate CA Certificate: openssl req -new -x509 -days 365 -extensions v3_ca -keyout ca.key -out ca.crt Generate Server Certificate: openssl genrsa -out server.key 2048openssl req -new -out server.csr -key server.keyopenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 Generate Client Certificate: openssl genrsa -out client.key 2048openssl req -new -out client.csr -key client.keyopenssl x509 -req -in client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out client.crt -days 365 Step 2: Configure the ServerFor example, in Node.js with the https module:const https = require(&#39;https&#39;);const fs = require(&#39;fs&#39;);const options = { key: fs.readFileSync(&#39;server.key&#39;), cert: fs.readFileSync(&#39;server.crt&#39;), ca: fs.readFileSync(&#39;ca.crt&#39;), // CA certificate requestCert: true, // Require client certificate rejectUnauthorized: true, // Reject unauthorized clients};https.createServer(options, (req, res) =&amp;gt; { res.writeHead(200); res.end(&#39;Hello, world!&#39;);}).listen(443, () =&amp;gt; { console.log(&#39;Server running on port 443&#39;);});Step 3: Configure the ClientFor example, in Node.js with the https module:const https = require(&#39;https&#39;);const fs = require(&#39;fs&#39;);const options = { hostname: &#39;example.com&#39;, port: 443, path: &#39;/&#39;, method: &#39;GET&#39;, key: fs.readFileSync(&#39;client.key&#39;), cert: fs.readFileSync(&#39;client.crt&#39;), ca: fs.readFileSync(&#39;ca.crt&#39;), // CA certificate rejectUnauthorized: true, // Reject unauthorized servers};const req = https.request(options, (res) =&amp;gt; { res.on(&#39;data&#39;, (data) =&amp;gt; { console.log(data.toString()); });});req.on(&#39;error&#39;, (err) =&amp;gt; { console.error(&#39;Error:&#39;, err);});req.end();6. Tools for mTLS OpenSSL: For generating and managing certificates. NGINX: For configuring mTLS in web servers. Kubernetes: For securing communication between microservices. Mosquitto: For MQTT-based IoT systems.7. Challenges of mTLS Certificate Management: Requires a robust system for issuing, renewing, and revoking certificates. Complexity: Adds complexity to the setup and maintenance of the system. Performance Overhead: Slightly increases latency due to the TLS handshake.If you’d like, I can help you set up mTLS for your specific use case (e.g., IoT devices, microservices, or APIs). Let me know! 😊Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "Home system control", "url": "/posts/home-system-control/", "categories": "Architecture", "tags": "control, iot, automation", "date": "2025-02-09 19:35:00 -0500", "snippet": " Deep thinking … Okay, let’s tackle this user’s question. They want to control and monitor various home systems like garden irrigation, pool pump, lights, heater, etc. They mentioned they’re comfortable with Angular, Spring Boot, and Docker. So, the user is looking for recommendations on IoT devices or microcontrollers to set up a secure system.First, I need to consider their technical background. They know Angular for the frontend, Spring Boot for the backend, and Docker for deployment. That gives a good foundation. Now, they need hardware to interface with the physical devices. The key here is to suggest microcontrollers that are reliable, widely supported, and secure.Raspberry Pi comes to mind because it’s versatile, runs a full OS, and can handle multiple tasks. It’s good for more complex scenarios where they might need to run Docker containers or a MQTT broker. However, for simpler tasks like controlling lights or irrigation, ESP32 or Arduino might be better. They are cheaper, low-power, and have good community support.Security is a big concern. The user wants everything secured, so I should emphasize secure communication protocols like MQTT over TLS, authentication methods, and network segmentation. Maybe suggest VLANs for IoT devices to isolate them from the main network.For the software architecture, since they know Spring Boot, using it as the backend makes sense. Angular for the frontend dashboard. Docker to containerize the backend, databases, and any other services like Node-RED or Mosquitto MQTT broker.Device management is important. They’ll need a way to add/remove devices, update firmware, and monitor status. The backend should handle REST APIs for control and maybe WebSockets for real-time updates.In terms of protocols, MQTT is lightweight and suitable for IoT. HTTPS for the frontend-backend communication. Maybe mention using TLS certificates for both backend and devices.I should also mention specific sensors and actuators: soil moisture sensors for the garden, relays for controlling pumps and heaters, motion sensors for lights.Security practices: regular firmware updates, strong authentication (like mutual TLS or JWT), encrypting data at rest and in transit, and maybe a VPN for remote access instead of exposing ports.Putting it all together, the architecture would have microcontrollers (ESP32/RPi) connected to sensors/actuators, communicating via MQTT to a broker. The Spring Boot backend processes data, stores it in a database, and exposes APIs. Angular frontend for the user interface. Docker to manage containers, and security measures throughout.Need to check if I’m missing any components. Maybe redundancy for critical systems, like the pool pump, but that might be beyond the initial setup. Also, consider power management for devices, maybe battery-powered sensors with ESP32 in deep sleep mode.Overall, the answer should guide them through selecting hardware, setting up the architecture, securing communications, and using their existing skills with Angular, Spring Boot, and Docker.Great choice! Let’s design a secure, scalable, and user-friendly IoT system for your home automation project. Below is a detailed breakdown of the recommended hardware, software architecture, security practices, and tools based on your familiarity with Angular, Spring Boot, and Docker.Great choice! Let’s design a secure, scalable, and user-friendly IoT system for your home automation project. Below is a detailed breakdown of the recommended hardware, software architecture, security practices, and tools based on your familiarity with Angular, Spring Boot, and Docker.1. Recommended Microcontrollers/IoT DevicesFor controlling devices like irrigation systems, pool pumps, lights, and heaters, you’ll need reliable microcontrollers with Wi-Fi/Bluetooth connectivity and GPIO pins for sensors/actuators. Here are my top picks: Device Use Case Why It’s Great ESP32 Lights, irrigation, sensors (soil moisture, temperature) Cheap, low-power, built-in Wi-Fi/Bluetooth, supports Arduino/C++/MicroPython. Raspberry Pi 4/5 Central hub, running MQTT broker, Docker containers, or complex logic Full Linux OS, Docker support, GPIO pins, and Wi-Fi/Ethernet for heavy lifting. Arduino MKR WiFi 1010 Pool pump, heater control (if simplicity is key) Easy to program, Wi-Fi connectivity, compatible with Arduino ecosystem. Sensors/Actuators: Relay Modules: To control high-voltage devices (pool pumps, heaters, lights). Soil Moisture Sensors: For garden irrigation automation. Temperature/Humidity Sensors: For environmental monitoring. Motion Sensors: For outdoor lights. Water Flow Sensors: To monitor pool pump activity.2. System ArchitectureHere’s how to structure your system:a. Hardware Layer ESP32/Arduino Devices: Connect to sensors/actuators (e.g., soil moisture sensor → ESP32 → relay → irrigation valve). Communicate with the backend via MQTT over TLS (secure messaging). Raspberry Pi: Acts as a local MQTT broker (e.g., Mosquitto) or edge computing node. Optional: Host lightweight Docker containers (e.g., Node-RED for logic). b. Communication Layer MQTT: Lightweight protocol for device-to-server communication (ideal for low-power devices). HTTP/HTTPS: For frontend/backend interactions (Angular → Spring Boot). WebSockets: For real-time updates on the Angular dashboard.c. Backend Layer (Spring Boot) API Endpoints: POST /api/irrigation/start: Trigger garden irrigation. GET /api/devices/status: Fetch real-time device statuses. Services: Device Management: Register/authenticate devices (use JWT or MQTT credentials). Data Processing: Store sensor data in a database (e.g., PostgreSQL/InfluxDB). Security: Handle TLS encryption, API authentication, and role-based access control (RBAC). d. Frontend Layer (Angular) Dashboard Features: Real-time controls (toggle lights, pool pump, etc.). Historical data visualization (charts for soil moisture, temperature). Alerts/notifications (e.g., leak detection, device offline). e. Infrastructure Layer (Docker) Containers: Spring Boot app. PostgreSQL/InfluxDB for storage. Mosquitto MQTT broker. Prometheus + Grafana for monitoring. Deployment: Use Docker Compose for local development. Deploy to a Raspberry Pi or cloud (AWS/Azure) for remote access. 3. Security Best PracticesTo secure your system:a. Device Security Authentication: Use MQTT credentials (username/password) or X.509 certificates (mTLS) for devices. Store credentials securely (e.g., encrypted in device firmware). Firmware Updates: Implement OTA (Over-the-Air) updates for ESP32/Arduino devices. Network Segmentation: Isolate IoT devices on a separate VLAN or Wi-Fi network. b. Backend/Frontend Security HTTPS Everywhere: Encrypt all traffic between Angular and Spring Boot. JWT Authentication: Use tokens for user logins and API access. Rate Limiting: Prevent abuse of APIs (e.g., Spring Boot + Redis). Input Validation: Sanitize all user/device inputs to prevent SQL/command injection.c. Data Security Encrypt Sensitive Data: Use AES-256 for data at rest (e.g., database fields). Role-Based Access Control (RBAC): Example: Only admins can control the pool pump/heater. 4. Tools &amp;amp; Libraries| Category | Tools ||———————–|—————————————————————————|| IoT Frameworks | Arduino IDE, PlatformIO (ESP32), Raspberry Pi OS || MQTT Broker | Mosquitto, HiveMQ (cloud-based) || Backend | Spring Boot (Java), MQTT Java Client (Eclipse Paho), Spring Security || Database | PostgreSQL (relational), InfluxDB (time-series), Redis (caching) || Frontend | Angular, Chart.js/Plotly, Angular Material || Deployment | Docker, Docker Compose, Portainer (for Raspberry Pi) || Monitoring | Prometheus, Grafana, ELK Stack |5. Example Workflow Garden Irrigation: ESP32 (with soil moisture sensor) detects dry soil. Publishes garden/soil-moisture data to MQTT broker. Spring Boot processes the data and triggers garden/irrigation/start via MQTT. ESP32 activates the irrigation relay. Pool Pump Control: User clicks “Start Pool Pump” in Angular UI. Angular sends POST /api/pool/pump/start to Spring Boot. Spring Boot validates the JWT token and publishes pool/pump/start to MQTT. Raspberry Pi (or ESP32) subscribes to the topic and triggers the pump relay. Real-Time Dashboard: Angular uses WebSockets or MQTT over WebSocket to display live data (e.g., lights/status). 6. Cost-Effective Setup ESP32: ~$10 per unit (buy in bulk for discounts). Raspberry Pi 4: ~$70 (as a central hub). Relay Modules: ~$5 each. Sensors: ~$2–$15 each (soil moisture, temperature, etc.).7. Getting Started Prototype One Device: Start with an ESP32 + relay to control a light or irrigation valve. Use Arduino IDE to program basic MQTT communication. Build the Backend: Set up Spring Boot with MQTT integration (Eclipse Paho). Create the Angular Dashboard: Use Angular Material for UI components and connect to Spring Boot APIs. Deploy with Docker: Containerize Mosquitto, Spring Boot, and PostgreSQL. 8. Final Tips Use Node-RED (on Raspberry Pi) for quick logic prototyping (e.g., “If soil dry → trigger irrigation”). For remote access, use Tailscale or Cloudflare Tunnel instead of exposing ports. Implement automated alerts (e.g., Telegram/Email notifications for leaks or device failures)." }, { "title": "Using Mutual TLS (mTLS) between backend server and frontend application", "url": "/posts/mtls-between-backend-and-frontend-application/", "categories": "Architecture", "tags": "security, protocol, mtls", "date": "2025-02-09 11:35:00 -0500", "snippet": "Yes, it is technically possible to use Mutual TLS (mTLS) between a backend server and a frontend application like an Angular Single Page Application (SPA). However, there are some challenges and considerations to keep in mind when implementing mTLS in this context. Let’s break it down:1. How mTLS Works Between Backend and FrontendIn a typical setup: The backend server is configured to require client certificates (mTLS). The frontend application (e.g., Angular SPA) acts as the client and presents its certificate during the TLS handshake.2. Challenges of Using mTLS with SPAsWhile mTLS is commonly used for server-to-server communication or IoT devices, using it with a frontend SPA introduces some challenges:a. Certificate Management Browser Limitations: Browsers do not natively support presenting client certificates for SPAs without user interaction. User Experience: If the browser prompts the user to select a certificate, it can disrupt the user experience. Certificate Storage: Storing and managing client certificates securely in the browser is difficult.b. Deployment Complexity Certificate Distribution: You need a secure way to distribute client certificates to users’ browsers. Revocation and Renewal: Managing certificate revocation and renewal for frontend clients is complex.c. Security Risks Certificate Theft: If client certificates are stored in the browser, they could be stolen (e.g., via XSS attacks). Limited Use Cases: mTLS is overkill for most frontend-backend communication unless you have extremely high security requirements.3. When to Use mTLS with SPAsmTLS between a backend server and an SPA is rare but can be justified in specific scenarios: High-Security Applications: For example, banking, healthcare, or government systems where client authentication must be extremely robust. Internal Applications: For internal tools where you have control over the client environment (e.g., corporate networks).4. Alternatives to mTLS for SPAsFor most use cases, mTLS is not the best choice for SPAs. Instead, consider these alternatives: OAuth 2.0 with PKCE: A modern, secure way to authenticate users in SPAs. JWT (JSON Web Tokens): Use JWTs for stateless authentication and authorization. API Keys: For simpler use cases, API keys can be used (with HTTPS for encryption).5. Implementing mTLS with an Angular SPAIf you still want to proceed with mTLS, here’s how you can implement it:Step 1: Backend Server ConfigurationConfigure your backend server (e.g., Node.js, Nginx, or Spring Boot) to require client certificates.Example: Node.js with mTLSconst https = require(&#39;https&#39;);const fs = require(&#39;fs&#39;);const options = { key: fs.readFileSync(&#39;server.key&#39;), cert: fs.readFileSync(&#39;server.crt&#39;), ca: fs.readFileSync(&#39;ca.crt&#39;), // CA certificate requestCert: true, // Require client certificate rejectUnauthorized: true, // Reject unauthorized clients};https.createServer(options, (req, res) =&amp;gt; { res.writeHead(200); res.end(&#39;Hello, world!&#39;);}).listen(443, () =&amp;gt; { console.log(&#39;Server running on port 443&#39;);});Step 2: Frontend ConfigurationTo use mTLS in the frontend, you need to configure the browser to present the client certificate.Option 1: Browser Certificate Import Export the client certificate and private key as a .p12 or .pfx file. Instruct users to import the certificate into their browser. In Chrome/Firefox: Go to Settings &amp;gt; Privacy &amp;amp; Security &amp;gt; Certificates &amp;gt; Import. When the frontend makes requests to the backend, the browser will prompt the user to select the certificate.Option 2: Programmatic Certificate Access (Experimental)Some modern browsers support the Web Crypto API and Web Authentication (WebAuthn), which can be used to programmatically access certificates. However, this is complex and not widely supported.Example: Angular HTTP Client with mTLSIf the certificate is imported into the browser, the Angular HTTP client will automatically use it for requests to the backend:import { HttpClient } from &#39;@angular/common/http&#39;;constructor(private http: HttpClient) {}fetchData() { return this.http.get(&#39;https://backend.example.com/api/data&#39;);}6. Example Workflow Certificate Setup: Generate client certificates for each user or device. Distribute the certificates securely (e.g., via email or a secure portal). Frontend: Users import the client certificate into their browser. The Angular SPA makes requests to the backend. Backend: The backend server validates the client certificate during the TLS handshake. If the certificate is valid, the request is processed; otherwise, it is rejected. 7. Tools and Libraries OpenSSL: For generating and managing certificates. Nginx: For configuring mTLS in web servers. Angular: For building the frontend SPA. Node.js/Express: For the backend server.8. Recommendations Avoid mTLS for SPAs: Unless you have a very specific use case, mTLS is not recommended for SPAs due to its complexity and usability challenges. Use Modern Alternatives: Consider using OAuth 2.0 with PKCE or JWT for secure authentication in SPAs. Combine mTLS with Other Methods: If you must use mTLS, combine it with other authentication methods (e.g., OAuth 2.0) for better usability.If you have a specific use case in mind, I can help you design a more tailored solution! Let me know. 😊" }, { "title": "Build a secure and scalable system for exposing real-time data from your controller to a website", "url": "/posts/build-a-secure-and-scalable-system-for-exposing-real-time-data-from-your-controller-to-a-website/", "categories": "Architecture", "tags": "docker, microservice, real-time", "date": "2025-02-08 19:35:00 -0500", "snippet": "To build a secure and scalable system for exposing real-time data from your controller to a website, you should consider the following technologies and best practices:1. Backend Technologies Programming Language: Use a modern, secure, and scalable language like Python (Django/Flask/FastAPI), Node.js, Java (Spring Boot), or Go. API Framework: Expose your data via a RESTful API or GraphQL API. For real-time updates, consider using WebSockets or Server-Sent Events (SSE). REST API: For general data retrieval and actions. WebSockets: For real-time bidirectional communication (e.g., live updates from devices). GraphQL: If you need flexible querying and efficient data fetching. 2. Frontend Technologies JavaScript Framework: Use a modern framework like React, Vue.js, or Angular to build a dynamic and responsive user interface. Real-Time Updates: Use libraries like Socket.IO (for WebSockets) or GraphQL Subscriptions to display real-time data on the website. Visualization: Use libraries like Chart.js, D3.js, or Plotly for displaying data trends and graphs.3. Communication Protocols HTTP/HTTPS: For standard API communication. WebSockets: For real-time, low-latency communication between the server and the website. MQTT: If your devices are IoT-based, MQTT is a lightweight protocol designed for real-time messaging in low-bandwidth environments.4. Security Measures Authentication and Authorization: Use OAuth 2.0 or JWT (JSON Web Tokens) for secure user authentication and authorization. Implement role-based access control (RBAC) to restrict access to sensitive data and actions. Encryption: Use HTTPS (TLS/SSL) to encrypt data in transit. Encrypt sensitive data at rest using algorithms like AES-256. Input Validation: Sanitize and validate all inputs to prevent SQL injection, XSS, and other attacks. Rate Limiting: Implement rate limiting to prevent abuse of your API (e.g., using tools like Redis). Device Authentication: Use secure methods like API keys, certificates, or mutual TLS (mTLS) to authenticate devices. 5. Database Relational Database: Use PostgreSQL or MySQL for structured data. NoSQL Database: Use MongoDB or Cassandra if you need flexibility for unstructured or semi-structured data. Time-Series Database: If your data is time-based (e.g., sensor readings), consider InfluxDB or TimescaleDB.6. Real-Time Data Processing Message Broker: Use a message broker like RabbitMQ, Apache Kafka, or Redis Pub/Sub to handle real-time data streams from devices. Stream Processing: Use frameworks like Apache Flink or Apache Storm for real-time data processing and analytics.7. Hosting and Deployment Cloud Providers: Use cloud platforms like AWS, Azure, or Google Cloud for scalability and reliability. Containerization: Use Docker to containerize your application and Kubernetes for orchestration. Serverless: Consider serverless architectures (e.g., AWS Lambda, Azure Functions) for specific tasks.8. Monitoring and Logging Monitoring: Use tools like Prometheus, Grafana, or Datadog to monitor system performance and device status. Logging: Use centralized logging with tools like ELK Stack (Elasticsearch, Logstash, Kibana) or Splunk.9. Evolution and Scalability Microservices Architecture: Break your system into smaller, independent services to ensure scalability and maintainability. CI/CD Pipeline: Implement continuous integration and deployment using tools like Jenkins, GitLab CI/CD, or GitHub Actions. Versioning: Use API versioning (e.g., /v1/endpoint) to ensure backward compatibility as your system evolves.10. Example Tech Stack Backend: Python (FastAPI) + WebSockets Frontend: React + Socket.IO Database: PostgreSQL + InfluxDB Message Broker: RabbitMQ Authentication: JWT + OAuth 2.0 Hosting: AWS (EC2, RDS, S3) Monitoring: Prometheus + Grafana11. Additional Considerations Data Privacy: Ensure compliance with regulations like GDPR or CCPA if applicable. Disaster Recovery: Implement backup and recovery strategies for your data and services. Documentation: Maintain clear documentation for your API and system architecture.By combining these technologies and best practices, you can build a secure, scalable, and future-proof system for real-time device monitoring and control." }, { "title": "Simulating IoT devices (sensors, actuators, etc.)", "url": "/posts/simulating-iot-devices/", "categories": "Architecture", "tags": "simulation, iot, devices, test", "date": "2025-02-08 16:35:00 -0500", "snippet": "Simulating IoT devices (sensors, actuators, etc.) is a great way to test and develop your system without needing physical hardware. Here are some tools and libraries you can use to simulate device behavior:1. Programming Languages and LibrariesYou can write scripts to simulate devices using popular programming languages and IoT libraries.a. Python Why: Simple and widely used for IoT simulations. Libraries: Paho MQTT: Simulate MQTT clients. Faker: Generate fake sensor data (e.g., temperature, humidity). Random: Generate random values for sensor readings. Example: import paho.mqtt.client as mqttimport randomimport timeclient = mqtt.Client()client.connect(&quot;mqtt.broker.com&quot;, 1883, 60)while True: temperature = random.uniform(20, 30) # Simulate temperature (20-30°C) client.publish(&quot;sensors/temperature&quot;, temperature) print(f&quot;Published: {temperature}°C&quot;) time.sleep(2) # Send data every 2 seconds b. Node.js Why: Lightweight and great for real-time applications. Libraries: mqtt: Simulate MQTT clients. faker.js: Generate fake sensor data. Example: const mqtt = require(&#39;mqtt&#39;);const client = mqtt.connect(&#39;mqtt://mqtt.broker.com&#39;);client.on(&#39;connect&#39;, () =&amp;gt; { setInterval(() =&amp;gt; { const temperature = (Math.random() * 10 + 20).toFixed(2); // 20-30°C client.publish(&#39;sensors/temperature&#39;, temperature); console.log(`Published: ${temperature}°C`); }, 2000); // Send data every 2 seconds}); c. Java Why: Good for enterprise-grade simulations. Libraries: Eclipse Paho: Simulate MQTT clients. JavaFaker: Generate fake sensor data. Example: import org.eclipse.paho.client.mqttv3.MqttClient;import org.eclipse.paho.client.mqttv3.MqttMessage;import java.util.Random;public class SensorSimulator { public static void main(String[] args) throws Exception { MqttClient client = new MqttClient(&quot;tcp://mqtt.broker.com:1883&quot;, MqttClient.generateClientId()); client.connect(); Random random = new Random(); while (true) { double temperature = 20 + random.nextDouble() * 10; // 20-30°C String payload = String.format(&quot;%.2f&quot;, temperature); client.publish(&quot;sensors/temperature&quot;, new MqttMessage(payload.getBytes())); System.out.println(&quot;Published: &quot; + payload + &quot;°C&quot;); Thread.sleep(2000); // Send data every 2 seconds } }} 2. IoT Simulation PlatformsThese platforms provide pre-built tools for simulating devices and testing IoT systems.a. Node-RED Why: Visual programming tool for IoT. Features: Drag-and-drop interface. Built-in MQTT nodes. Can simulate sensors and actuators. Example: Use the inject node to simulate sensor data. Use the MQTT out node to publish data to a broker. b. ThingsBoard Why: Open-source IoT platform with device simulation. Features: Simulate devices using REST API or MQTT. Visualize data on dashboards. Example: Use the Device Simulator feature to create virtual devices. c. AWS IoT Device Simulator Why: Cloud-based simulator for AWS IoT. Features: Simulate thousands of devices. Test IoT rules and workflows. Example: Use the AWS IoT console to create and manage virtual devices. 3. Standalone SimulatorsThese tools are specifically designed for simulating IoT devices.a. MQTT Simulator Why: Lightweight tool for testing MQTT communication. Features: Simulate multiple MQTT clients. Publish and subscribe to topics. Example: Use MQTT Explorer or MQTT.fx to simulate devices. b. Cooja (for Contiki OS) Why: Simulate low-power IoT devices. Features: Simulate sensors, actuators, and networks. Ideal for testing constrained devices. Example: Use Cooja to simulate a network of temperature sensors. 4. Custom SimulatorsIf you need more control, you can build your own simulator using frameworks like:a. Docker Why: Simulate multiple devices in isolated containers. Example: Create a Docker image for your device simulator. Run multiple containers to simulate a large number of devices. b. Kubernetes Why: Simulate devices at scale. Example: Deploy device simulators as Kubernetes pods. Use Helm charts to manage deployments. 5. Example: Simulating Multiple DevicesHere’s how you can simulate multiple devices using Python:import paho.mqtt.client as mqttimport randomimport timedef simulate_device(device_id): client = mqtt.Client() client.connect(&quot;mqtt.broker.com&quot;, 1883, 60) while True: temperature = random.uniform(20, 30) # Simulate temperature (20-30°C) client.publish(f&quot;sensors/{device_id}/temperature&quot;, temperature) print(f&quot;Device {device_id}: Published {temperature}°C&quot;) time.sleep(2) # Send data every 2 seconds# Simulate 5 devicesfor i in range(1, 6): simulate_device(f&quot;device_{i}&quot;)6. Tools for Performance TestingOnce you’ve simulated devices, you can test the performance of your system using:a. JMeter Why: Load testing for MQTT and HTTP. Example: Use the MQTT Plugin to simulate thousands of devices. b. Gatling Why: High-performance load testing. Example: Write simulations in Scala to test your backend. c. K6 Why: Modern load testing tool. Example: Write scripts in JavaScript to simulate device traffic. 7. Final Recommendation For quick prototyping: Use Node-RED or Python. For scalable simulations: Use Docker or Kubernetes. For cloud-based testing: Use AWS IoT Device Simulator." }, { "title": "Device Ingestion Service", "url": "/posts/device-ingestion-service/", "categories": "Architecture", "tags": "docker, data, ingestion, automation", "date": "2025-02-08 15:35:00 -0500", "snippet": "Absolutely! Let’s dive deeper into the Device Ingestion Service, which is a critical component of your system. This service is responsible for collecting data from your devices in real time and forwarding it to the rest of the system for processing and storage.1. Responsibilities of the Device Ingestion Service Receive Data: Accept data from devices via protocols like MQTT, WebSocket, or HTTP. Validate Data: Ensure the incoming data is in the correct format and free of errors. Forward Data: Send the validated data to a message broker (e.g., Kafka, RabbitMQ) for further processing. Handle Device Authentication: Authenticate devices to ensure only authorized devices can send data. Scale for High Throughput: Handle a large number of devices and high data volumes.2. Key Features Protocol Support: Support multiple communication protocols (e.g., MQTT, WebSocket, HTTP). Data Validation: Validate incoming data for format, type, and integrity. Authentication: Authenticate devices using API keys, certificates, or mutual TLS (mTLS). Scalability: Handle high throughput and scale horizontally as the number of devices grows. Fault Tolerance: Ensure data is not lost even if the service or downstream systems fail.3. Design of the Device Ingestion ServiceHere’s how you can design the service:a. Protocol Handlers MQTT Handler: Use an MQTT broker like Mosquitto or EMQX. Devices publish data to specific topics (e.g., devices/device-id/data). The service subscribes to these topics to receive data. WebSocket Handler: Use a WebSocket server (e.g., Socket.IO or ws in Node.js). Devices establish a WebSocket connection and send data in real time. HTTP Handler: Use a REST API endpoint to accept HTTP POST requests from devices. Devices send data as JSON payloads. b. Data Validation Validate the structure and content of incoming data. Example validation checks: Required fields (e.g., device_id, timestamp, sensor_value). Data types (e.g., sensor_value should be a number). Range checks (e.g., sensor_value should be between 0 and 100). c. Authentication API Keys: Devices include an API key in their requests (e.g., in the HTTP header or MQTT topic). Certificates: Use mutual TLS (mTLS) to authenticate devices with client certificates. JWT: Devices send a signed JWT token with each request.d. Data Forwarding Use a message broker like Apache Kafka, RabbitMQ, or Redis Pub/Sub to forward data to the Data Processing Service. Example workflow: The service receives data from a device. It validates and authenticates the data. It publishes the data to a message broker topic (e.g., device-data). e. Scalability Use a load balancer (e.g., NGINX, HAProxy) to distribute traffic across multiple instances of the service. Use a distributed message broker (e.g., Kafka) to handle high throughput.f. Fault Tolerance Use persistent connections (e.g., WebSocket or MQTT) to ensure data is not lost during network interruptions. Implement retries and acknowledgments when forwarding data to the message broker.4. Example ImplementationHere’s an example implementation of the Device Ingestion Service using Node.js and MQTT:a. MQTT Handlerconst mqtt = require(&#39;mqtt&#39;);const client = mqtt.connect(&#39;mqtt://broker.example.com&#39;);// Subscribe to device topicsclient.on(&#39;connect&#39;, () =&amp;gt; { client.subscribe(&#39;devices/+/data&#39;, (err) =&amp;gt; { if (!err) console.log(&#39;Subscribed to device data topics&#39;); });});// Handle incoming messagesclient.on(&#39;message&#39;, (topic, message) =&amp;gt; { const data = JSON.parse(message.toString()); if (validateData(data)) { // Forward data to the message broker forwardToMessageBroker(data); } else { console.error(&#39;Invalid data received:&#39;, data); }});// Validate datafunction validateData(data) { return data.device_id &amp;amp;&amp;amp; data.timestamp &amp;amp;&amp;amp; data.sensor_value !== undefined;}// Forward data to Kafkafunction forwardToMessageBroker(data) { // Example: Send data to Kafka const kafka = require(&#39;kafka-node&#39;); const Producer = kafka.Producer; const client = new kafka.KafkaClient({ kafkaHost: &#39;kafka.example.com:9092&#39; }); const producer = new Producer(client); producer.on(&#39;ready&#39;, () =&amp;gt; { const payload = [{ topic: &#39;device-data&#39;, messages: JSON.stringify(data) }]; producer.send(payload, (err, data) =&amp;gt; { if (err) console.error(&#39;Error forwarding data to Kafka:&#39;, err); }); });}b. WebSocket Handlerconst WebSocket = require(&#39;ws&#39;);const wss = new WebSocket.Server({ port: 8080 });wss.on(&#39;connection&#39;, (ws) =&amp;gt; { ws.on(&#39;message&#39;, (message) =&amp;gt; { const data = JSON.parse(message); if (validateData(data)) { forwardToMessageBroker(data); } else { console.error(&#39;Invalid data received:&#39;, data); } });});c. HTTP Handlerconst express = require(&#39;express&#39;);const app = express();app.use(express.json());app.post(&#39;/data&#39;, (req, res) =&amp;gt; { const data = req.body; if (validateData(data)) { forwardToMessageBroker(data); res.status(200).send(&#39;Data received&#39;); } else { res.status(400).send(&#39;Invalid data&#39;); }});app.listen(3000, () =&amp;gt; { console.log(&#39;HTTP server listening on port 3000&#39;);});5. Tools and Libraries| Functionality | Tools/Libraries ||————————-|————————————————————————————-|| MQTT | Mosquitto, EMQX, mqtt.js (Node.js) || WebSocket | ws (Node.js), Socket.IO || HTTP | Express.js, FastAPI (Python), Spring Boot (Java) || Message Broker | Apache Kafka, RabbitMQ, Redis Pub/Sub || Authentication | JWT, OAuth 2.0, mutual TLS (mTLS) || Data Validation | JSON Schema, Joi (Node.js), Pydantic (Python) |6. Deployment Considerations Containerization: Package the service in a Docker container for easy deployment. Orchestration: Use Kubernetes to manage and scale the service. Monitoring: Monitor the service using Prometheus and Grafana. Load Balancing: Use NGINX or HAProxy to distribute traffic across multiple instances.7. Example Workflow A device sends data to the Device Ingestion Service via MQTT, WebSocket, or HTTP. The service validates the data and authenticates the device. The service forwards the data to a message broker (e.g., Kafka). The Data Processing Service consumes the data from the message broker for further processing.If you’d like, I can help you dive deeper into any specific part of the implementation (e.g., MQTT setup, Kafka integration, or authentication). Let me know! 😊" }, { "title": "Designing a microservices architecture and system architecture", "url": "/posts/designing-a-microservices-architecture-and-system-architecture/", "categories": "Architecture", "tags": "controller, design, patterns, real-time", "date": "2025-02-08 12:35:00 -0500", "snippet": "Absolutely! Designing a microservices architecture and system architecture for your real-time device monitoring and control system involves breaking down the system into smaller, independent services that work together seamlessly. Below, I’ll guide you through the process step by step, including the architecture design, service decomposition, and communication patterns.1. High-Level System ArchitectureHere’s an overview of the system architecture: Devices: Your IoT devices or controllers that collect data and perform actions. Ingestion Layer: Handles real-time data ingestion from devices. Processing Layer: Processes and transforms the data. Storage Layer: Stores data in appropriate databases. API Gateway: Exposes APIs for the frontend and external systems. Frontend: A web application for real-time monitoring and control. Authentication &amp;amp; Authorization: Manages user and device access. Monitoring &amp;amp; Logging: Tracks system performance and logs events.2. Microservices ArchitectureWe’ll break the system into the following microservices:a. Device Ingestion Service Responsibility: Collects data from devices in real time. Protocols: HTTP, WebSocket, MQTT. Tools: MQTT Broker (e.g., Mosquitto), WebSocket server. Output: Sends data to the Message Broker for further processing.b. Data Processing Service Responsibility: Processes and transforms raw device data. Tasks: Data validation, aggregation, and enrichment. Tools: Apache Kafka, RabbitMQ, or Redis for stream processing. Output: Sends processed data to the Storage Service and Real-Time Notification Service.c. Storage Service Responsibility: Stores processed data in appropriate databases. Databases: Time-Series Data: InfluxDB or TimescaleDB. Structured Data: PostgreSQL or MySQL. Unstructured Data: MongoDB. Output: Provides APIs for querying historical data.d. Real-Time Notification Service Responsibility: Sends real-time updates to the frontend. Protocols: WebSocket, Server-Sent Events (SSE). Tools: Socket.IO, GraphQL Subscriptions. Output: Pushes updates to the Frontend.e. API Gateway Responsibility: Acts as a single entry point for all client requests. Tasks: Routes requests to appropriate microservices, handles authentication, and rate limiting. Tools: NGINX, Kong, or Spring Cloud Gateway. Output: Exposes RESTful APIs or GraphQL endpoints.f. Authentication &amp;amp; Authorization Service Responsibility: Manages user and device authentication. Tasks: Issues JWT tokens, validates permissions. Tools: OAuth 2.0, OpenID Connect, or custom JWT-based authentication. Output: Secures access to all services.g. Frontend Service Responsibility: Displays real-time data and allows user interaction. Tools: React, Vue.js, or Angular. Features: Real-time dashboards. Device control interface. Data visualization (e.g., charts, graphs). h. Monitoring &amp;amp; Logging Service Responsibility: Tracks system performance and logs events. Tools: Prometheus, Grafana, ELK Stack (Elasticsearch, Logstash, Kibana). Output: Provides insights into system health and debugging information.3. Communication Patterns Synchronous Communication: Use REST APIs or GraphQL for request-response interactions (e.g., fetching historical data). Asynchronous Communication: Use message brokers (e.g., Kafka, RabbitMQ) for event-driven communication between microservices. Real-Time Communication: Use WebSockets or SSE for real-time updates to the frontend.4. Deployment Architecture Containerization: Use Docker to containerize each microservice. Orchestration: Use Kubernetes to manage and scale microservices. Cloud Provider: Deploy on AWS, Azure, or Google Cloud for scalability and reliability. Serverless: Use serverless functions (e.g., AWS Lambda) for specific tasks like data processing.5. Example Workflow Device Data Ingestion: Devices send data to the Device Ingestion Service via MQTT or WebSocket. The service forwards the data to a message broker (e.g., Kafka). Data Processing: The Data Processing Service consumes data from the message broker, processes it, and stores it in the appropriate database. Real-Time Updates: The Real-Time Notification Service listens for processed data and pushes updates to the frontend via WebSocket. Frontend Interaction: The frontend displays real-time data and allows users to control devices via the API Gateway. Authentication: All requests are authenticated and authorized by the Authentication &amp;amp; Authorization Service. Monitoring: The Monitoring &amp;amp; Logging Service tracks system performance and logs events for debugging. 6. Tools and Technologies Component Tools/Technologies Device Communication MQTT, WebSocket, HTTP Message Broker Apache Kafka, RabbitMQ, Redis Pub/Sub Database PostgreSQL, InfluxDB, MongoDB API Gateway NGINX, Kong, Spring Cloud Gateway Authentication OAuth 2.0, JWT, OpenID Connect Frontend React, Vue.js, Angular, Socket.IO Monitoring &amp;amp; Logging Prometheus, Grafana, ELK Stack Containerization Docker Orchestration Kubernetes Cloud Provider AWS, Azure, Google Cloud 7. Diagram of the ArchitectureHere’s a simplified diagram of the architecture:[Devices] --&amp;gt; [Device Ingestion Service] --&amp;gt; [Message Broker] --&amp;gt; [Data Processing Service] | v[Storage Service] &amp;lt;-- [Processed Data] --&amp;gt; [Real-Time Notification Service] --&amp;gt; [Frontend] | v[API Gateway] &amp;lt;--&amp;gt; [Authentication Service] &amp;lt;--&amp;gt; [Monitoring &amp;amp; Logging Service]8. Next Steps Define APIs: Design RESTful or GraphQL APIs for each microservice. Set Up Infrastructure: Use Docker and Kubernetes to set up your development environment. Implement Core Services: Start with the Device Ingestion Service and Data Processing Service. Add Security: Implement authentication and authorization using JWT or OAuth 2.0. Build the Frontend: Create a real-time dashboard using React or Vue.js. Monitor and Scale: Set up monitoring and logging, and plan for scalability.If you’d like, I can help you dive deeper into any specific part of the architecture (e.g., API design, Kubernetes setup, or frontend implementation). Let me know how you’d like to proceed! 😊" }, { "title": "List of free open source and privacy respecting", "url": "/posts/list-of-free-opensource/", "categories": "Privacy", "tags": "apps, free, opensource", "date": "2025-02-07 15:35:00 -0500", "snippet": "Here’s a comprehensive list of free, open-source, and privacy-respecting alternatives to popular proprietary services. These tools prioritize user privacy, transparency, and control over data.Communication| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| WhatsApp | Signal | Encrypted messaging and calls. || Zoom | Jitsi Meet | Open-source video conferencing. || Slack | Rocket.Chat | Team collaboration and chat. || Microsoft Teams | Element (Matrix) | Decentralized chat and video calls. || Skype | Jami | Peer-to-peer encrypted calls and messaging. |Email| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Gmail | ProtonMail | Encrypted email service. || Outlook | Tutanota | Secure email with end-to-end encryption. || Yahoo Mail | Mailpile | Privacy-focused email client. |Cloud Storage| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Google Drive | Nextcloud | Self-hosted file storage and sharing. || Dropbox | Syncthing | Decentralized file synchronization. || OneDrive | Seafile | Self-hosted cloud storage. |Search Engines| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Google Search | Searx | Privacy-respecting meta-search engine. || Bing | DuckDuckGo | Privacy-focused search engine. || Yahoo Search | Qwant | European search engine with privacy focus. |Social Media| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Facebook | Mastodon | Decentralized social network. || Twitter | Pleroma | Federated microblogging platform. || Instagram | PixelFed | Decentralized photo sharing. || LinkedIn | LinkedIn (no direct alternative) | (No direct open-source alternative, but consider self-hosted professional networks like HumHub) |Web Browsers| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Google Chrome | Firefox | Privacy-focused browser. || Microsoft Edge | Brave | Browser with built-in ad blocking and privacy features. || Safari | Tor Browser | Browser for anonymous browsing. |Office Suites| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Microsoft Office | LibreOffice | Free and open-source office suite. || Google Docs | OnlyOffice | Self-hosted office suite. || Notion | Joplin | Open-source note-taking app. |Password Managers| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| LastPass | Bitwarden | Open-source password manager. || 1Password | KeePass | Local password manager. || Dashlane | Passbolt | Self-hosted password manager. |Operating Systems| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Windows | Linux (e.g., Ubuntu) | Free and open-source OS. || macOS | Linux (e.g., Fedora) | Free and open-source OS. || iOS/Android | LineageOS | Open-source mobile OS. |Maps and Navigation| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Google Maps | OpenStreetMap | Collaborative mapping project. || Waze | OsmAnd | Offline maps and navigation. || Apple Maps | Maps.me | Offline maps based on OpenStreetMap. |Video Streaming| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| YouTube | PeerTube | Decentralized video hosting. || Netflix | Jellyfin | Self-hosted media server. || Twitch | Owncast | Self-hosted live streaming. |Music Streaming| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Spotify | Funkwhale | Decentralized music streaming. || Apple Music | Ampache | Self-hosted music server. || SoundCloud | Audius | Decentralized music sharing. |File Sharing| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| WeTransfer | OnionShare | Secure file sharing over Tor. || Google Drive | Nextcloud | Self-hosted file sharing. || Dropbox | Syncthing | Decentralized file synchronization. |VPNs| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| NordVPN | OpenVPN | Open-source VPN protocol. || ExpressVPN | WireGuard | Modern, lightweight VPN protocol. || Surfshark | Algo VPN | Self-hosted VPN solution. |Project Management| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Trello | Wekan | Open-source kanban board. || Asana | OpenProject | Self-hosted project management. || Jira | Taiga | Agile project management tool. |E-Commerce| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Shopify | PrestaShop | Open-source e-commerce platform. || WooCommerce | Magento | Open-source e-commerce solution. || Amazon | OpenBazaar | Decentralized marketplace. |Analytics| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Google Analytics | Matomo | Privacy-focused web analytics. || Mixpanel | Plausible Analytics | Lightweight and open-source analytics. || Adobe Analytics | Fathom Analytics | Simple and privacy-focused analytics. |Other Tools| Proprietary Service | Open Source Alternative | Description ||————————–|—————————–|—————–|| Photoshop | GIMP | Open-source image editor. || Illustrator | Inkscape | Open-source vector graphics editor. || Premiere Pro | Kdenlive | Open-source video editor. |This list provides privacy-respecting alternatives to popular proprietary services. Many of these tools are self-hostable, giving you full control over your data. Let me know if you need help setting up any of these! 🚀" }, { "title": "Automatically Refreshes Data", "url": "/posts/automatically-refreshes-data/", "categories": "Architecture", "tags": "real-time, data", "date": "2025-02-07 10:35:00 -0500", "snippet": "Designing a website that automatically refreshes data (e.g., live basketball scores) requires a combination of real-time communication protocols, efficient data handling, and modern architectural patterns. Below is a step-by-step guide to designing such a system, along with examples and best practices.Key Requirements Real-Time Updates: Data (e.g., scores) must update instantly without manual page refreshes. Scalability: Handle thousands of concurrent users during peak times (e.g., live games). Low Latency: Minimize delays between data updates and user visibility. Resilience: Handle failures gracefully (e.g., API outages, server crashes). User Experience: Smooth, non-disruptive updates (no flickering or page reloads).Architecture Design1. Frontend (Client-Side)Goal: Display live data and receive real-time updates. Framework: Use React, Vue.js, or Angular for dynamic UI. Real-Time Updates: WebSockets: Bidirectional communication (e.g., Socket.IO for JavaScript). Server-Sent Events (SSE): Simpler, unidirectional updates from server to client. State Management: Tools like Redux (React) or Vuex (Vue.js) to manage live data. UI/UX: Use animations (e.g., fade-ins) to highlight updates. Implement lazy loading for older data (e.g., game history). 2. Backend (Server-Side)Goal: Process, store, and broadcast live data. Framework: Node.js (Express/NestJS), Spring Boot, or Django. Real-Time Communication: WebSocket Server: Use Socket.IO (Node.js) or Spring WebSockets (Java). Pub/Sub System: Use Redis Pub/Sub or Kafka to handle real-time event broadcasting. APIs: REST/GraphQL for non-real-time requests (e.g., historical data). Database: Relational DB (PostgreSQL/MySQL): Store structured data (teams, schedules). NoSQL DB (MongoDB/Cassandra): Store unstructured data (live events, logs). Caching: Use Redis to cache frequently accessed data (e.g., live scores).3. Data IngestionGoal: Fetch live data from external sources. Third-Party APIs: Use sports APIs like Sportradar or ESPN. Web Scraping (if no API): Python: BeautifulSoup or Scrapy. Node.js: Puppeteer or Cheerio. Data Processing: Validate and transform raw data into a usable format. Trigger real-time updates via WebSockets or Pub/Sub. 4. InfrastructureGoal: Ensure scalability and reliability. Cloud Hosting: AWS, Google Cloud, or Azure. Containerization: Docker + Kubernetes for scaling backend services. Load Balancer: Distribute traffic (e.g., NGINX, AWS ALB). CDN: Cache static assets (e.g., Cloudflare). Monitoring: Tools like Prometheus + Grafana for performance tracking.High-Level Architecture Diagram+---------------------+ +---------------------+| Third-Party API | | Web Scraping || (Sportradar/ESPN) | | (Puppeteer/BS4) |+---------------------+ +---------------------+ | | v v+-----------------------------------------------+| Data Ingestion Layer || - Fetch/parse data || - Validate &amp;amp; format || - Publish updates to Redis/Kafka |+-----------------------------------------------+ | | v v+-----------------------------------------------+| Backend Server || - WebSocket Server (Socket.IO/Spring) || - REST/GraphQL API || - Redis (Pub/Sub + Caching) || - Database (PostgreSQL/MongoDB) |+-----------------------------------------------+ | | v v+-----------------------------------------------+| Frontend Client || - React/Vue.js/Angular || - WebSocket/SSE client || - Dynamic UI updates |+-----------------------------------------------+Example Implementation1. Real-Time Updates with WebSocketsFrontend (React + Socket.IO):// LiveScoreComponent.jsximport React, { useEffect, useState } from &quot;react&quot;;import io from &quot;socket.io-client&quot;;const socket = io(&quot;https://api.yoursite.com&quot;);function LiveScores() { const [scores, setScores] = useState([]); useEffect(() =&amp;gt; { socket.on(&quot;scoreUpdate&quot;, (data) =&amp;gt; { setScores(data); // Auto-updates UI }); return () =&amp;gt; socket.disconnect(); }, []); return ( &amp;lt;div&amp;gt; {scores.map((game) =&amp;gt; ( &amp;lt;div key={game.id} className=&quot;score-card&quot;&amp;gt; {game.homeTeam} vs {game.awayTeam}: {game.score} &amp;lt;/div&amp;gt; ))} &amp;lt;/div&amp;gt; );}Backend (Node.js + Socket.IO):// server.jsconst express = require(&quot;express&quot;);const http = require(&quot;http&quot;);const { Server } = require(&quot;socket.io&quot;);const redis = require(&quot;redis&quot;);const app = express();const server = http.createServer(app);const io = new Server(server);const redisClient = redis.createClient();// Subscribe to Redis channel for score updatesredisClient.subscribe(&quot;scoreUpdates&quot;);redisClient.on(&quot;message&quot;, (channel, message) =&amp;gt; { io.emit(&quot;scoreUpdate&quot;, JSON.parse(message)); // Broadcast to clients});server.listen(3000, () =&amp;gt; { console.log(&quot;WebSocket server running on port 3000&quot;);});2. Data Ingestion (Python + Web Scraping):# scraper.pyimport requestsfrom bs4 import BeautifulSoupimport redisimport jsonredis_client = redis.Redis(host=&#39;localhost&#39;, port=6379)def fetch_scores(): url = &quot;https://example-sports-site.com/live-scores&quot; response = requests.get(url) soup = BeautifulSoup(response.text, &quot;html.parser&quot;) # Extract scores (example) games = [] for game in soup.find_all(&quot;div&quot;, class_=&quot;game&quot;): home_team = game.find(&quot;div&quot;, class_=&quot;home-team&quot;).text away_team = game.find(&quot;div&quot;, class_=&quot;away-team&quot;).text score = game.find(&quot;div&quot;, class_=&quot;score&quot;).text games.append({ &quot;homeTeam&quot;: home_team, &quot;awayTeam&quot;: away_team, &quot;score&quot;: score }) # Publish to Redis redis_client.publish(&quot;scoreUpdates&quot;, json.dumps(games))# Run every 5 secondswhile True: fetch_scores() time.sleep(5)Best Practices Optimize Data Transfer: Send only incremental updates (deltas) instead of full data payloads. Throttle Updates: Limit updates to 1-2 seconds to avoid overwhelming clients. Fallback Mechanism: If WebSockets fail, fall back to SSE or periodic polling. Security: Use wss:// (secure WebSockets) and validate incoming data to prevent injection attacks. Scalability: Use Kubernetes to auto-scale backend services based on traffic. Tools &amp;amp; Services Real-Time Frameworks: Socket.IO, Pusher, Ably. APIs: Sportradar, TheSportsDB, ESPN API. Cloud: AWS EC2 (servers), S3 (static assets), Lambda (serverless scraping). Monitoring: Datadog, New Relic.This architecture ensures real-time updates, scalability, and a seamless user experience. Let me know if you need help with specific components (e.g., Kubernetes setup, database design)! 🏀🚀" }, { "title": "Website with Real-Time Updates Data", "url": "/posts/website-realtime-updates-data/", "categories": "Architecture", "tags": "real-time, data", "date": "2025-02-07 10:32:00 -0500", "snippet": "Designing a website that automatically refreshes data (like live basketball scores) requires a combination of real-time data updates, efficient client-server communication, and a scalable architecture. Below, I’ll outline the best practices and technologies to achieve this, along with a high-level architecture design.Key Requirements Real-Time Updates: Scores and game data should update automatically without requiring a page refresh. Scalability: The system should handle a large number of concurrent users. Performance: Minimal latency in delivering updates. User Experience: Smooth and seamless updates without disrupting the user.Architecture Design1. Frontend (Client-Side)The frontend is responsible for displaying data and receiving real-time updates. Framework: Use a modern JavaScript framework like React, Vue.js, or Angular. Real-Time Updates: Use WebSockets or Server-Sent Events (SSE) for real-time communication. Libraries like Socket.IO (for WebSockets) or EventSource (for SSE) can simplify implementation. UI/UX: Display live scores using animations or notifications to highlight updates. Use infinite scrolling or pagination for game history or statistics. 2. Backend (Server-Side)The backend handles data processing, storage, and real-time updates. Framework: Use Node.js (with Express), Spring Boot, Django, or Flask. Real-Time Communication: Implement WebSockets or SSE for pushing updates to clients. Use Redis Pub/Sub or Message Queues (e.g., RabbitMQ, Kafka) for handling real-time events. Database: Use a relational database (e.g., PostgreSQL, MySQL) for structured data like game schedules, teams, and players. Use a NoSQL database (e.g., MongoDB, Cassandra) for storing unstructured or semi-structured data like live scores and events. Caching: Use Redis or Memcached to cache frequently accessed data (e.g., live scores) and reduce database load. 3. Data Source Integration Third-Party APIs: Integrate with sports data providers like ESPN API, Sportradar, or The Sports DB to fetch live scores and game data. Web Scraping (if APIs are unavailable): Use libraries like BeautifulSoup (Python) or Puppeteer (Node.js) to scrape data from sports websites. 4. Deployment and Scalability Hosting: Use cloud platforms like AWS, Google Cloud, or Azure for scalability. Load Balancing: Use a load balancer (e.g., NGINX, AWS ELB) to distribute traffic across multiple servers. Containerization: Use Docker to containerize the application and Kubernetes for orchestration. CDN: Use a Content Delivery Network (e.g., Cloudflare, Akamai) to serve static assets (CSS, JS, images) faster. High-Level Architecture DiagramTechnologies and ToolsFrontend Frameworks: React, Vue.js, Angular Real-Time Libraries: Socket.IO, EventSource Styling: Tailwind CSS, Bootstrap State Management: Redux (React), Vuex (Vue.js)Backend Frameworks: Node.js (Express), Spring Boot, Django, Flask Real-Time Communication: WebSockets, SSE, Redis Pub/Sub Databases: PostgreSQL, MySQL, MongoDB Caching: Redis, MemcachedData Sources APIs: ESPN API, Sportradar, The Sports DB Web Scraping: BeautifulSoup (Python), Puppeteer (Node.js)Deployment Cloud Platforms: AWS, Google Cloud, Azure Containerization: Docker, Kubernetes Load Balancer: NGINX, AWS ELB CDN: Cloudflare, AkamaiExample Workflow Fetch Data: The backend fetches live scores from a third-party API or via web scraping. Store Data: Store the data in a database (e.g., PostgreSQL for structured data, MongoDB for live events). Push Updates: Use WebSockets or SSE to push real-time updates to connected clients. Display Data: The frontend displays live scores and updates in real-time using WebSocket/SSE. Cache Data: Cache frequently accessed data (e.g., live scores) in Redis to reduce database load. Example Code SnippetsFrontend (React + Socket.IO)import React, { useEffect, useState } from &quot;react&quot;;import io from &quot;socket.io-client&quot;;const socket = io(&quot;http://localhost:3000&quot;);function LiveScores() { const [scores, setScores] = useState([]); useEffect(() =&amp;gt; { socket.on(&quot;scoreUpdate&quot;, (data) =&amp;gt; { setScores(data); }); return () =&amp;gt; socket.disconnect(); }, []); return ( &amp;lt;div&amp;gt; &amp;lt;h1&amp;gt;Live Basketball Scores&amp;lt;/h1&amp;gt; &amp;lt;ul&amp;gt; {scores.map((score, index) =&amp;gt; ( &amp;lt;li key={index}&amp;gt;{score.game}: {score.score}&amp;lt;/li&amp;gt; ))} &amp;lt;/ul&amp;gt; &amp;lt;/div&amp;gt; );}export default LiveScores;Backend (Node.js + Socket.IO)const express = require(&quot;express&quot;);const http = require(&quot;http&quot;);const { Server } = require(&quot;socket.io&quot;);const app = express();const server = http.createServer(app);const io = new Server(server);// Simulate live score updatessetInterval(() =&amp;gt; { const scores = [ { game: &quot;Lakers vs Warriors&quot;, score: &quot;102-98&quot; }, { game: &quot;Bulls vs Celtics&quot;, score: &quot;89-85&quot; }, ]; io.emit(&quot;scoreUpdate&quot;, scores);}, 5000);server.listen(3000, () =&amp;gt; { console.log(&quot;Server is running on port 3000&quot;);});Scalability Tips Horizontal Scaling: Deploy multiple instances of your backend server and use a load balancer. Database Optimization: Use indexing, sharding, and replication to improve database performance. Caching: Cache frequently accessed data (e.g., live scores) to reduce database load. Monitoring: Use tools like Prometheus and Grafana to monitor system performance. This architecture ensures real-time updates, scalability, and a great user experience. Let me know if you need further details or help with implementation!" }, { "title": "Docker Network", "url": "/posts/docker-network/", "categories": "DEV, Docker", "tags": "network, cmd, docker", "date": "2025-02-07 09:35:00 -0500", "snippet": "Certainly! Docker networking allows containers to communicate with each other, the host system, and external networks. Docker provides several networking options, each suited for different use cases. Below, I’ll explain Docker networking in detail, including network types, commands, and examples.Docker Network TypesDocker supports several network drivers, each serving a specific purpose: Bridge Network: Default network driver. Containers on the same bridge network can communicate with each other. Isolated from the host and external networks by default. Suitable for single-host deployments. Host Network: Containers share the host’s network stack. No network isolation between the container and the host. Suitable for high-performance use cases. Overlay Network: Enables multi-host communication in Docker Swarm. Containers on different hosts can communicate securely. Suitable for distributed applications. Macvlan Network: Assigns a MAC address to each container, making it appear as a physical device on the network. Suitable for legacy applications that require a MAC address. None Network: Disables all networking for the container. Useful for completely isolated containers. Docker Network Commands1. List Networksdocker network lsOutput:NETWORK ID NAME DRIVER SCOPEabc123def456 bridge bridge localdef456abc123 host host localghi789jkl012 none null local2. Create a Networkdocker network create &amp;lt;network_name&amp;gt;Example:docker network create my_bridge_network3. Inspect a Networkdocker network inspect &amp;lt;network_name&amp;gt;Example:docker network inspect my_bridge_network4. Connect a Container to a Networkdocker network connect &amp;lt;network_name&amp;gt; &amp;lt;container_name&amp;gt;Example:docker network connect my_bridge_network my_container5. Disconnect a Container from a Networkdocker network disconnect &amp;lt;network_name&amp;gt; &amp;lt;container_name&amp;gt;Example:docker network disconnect my_bridge_network my_container6. Remove a Networkdocker network rm &amp;lt;network_name&amp;gt;Example:docker network rm my_bridge_networkExamples of Docker NetworkingExample 1: Bridge Network Create a Bridge Network: docker network create my_bridge Run Containers on the Bridge Network: docker run -d --name container1 --network my_bridge nginxdocker run -d --name container2 --network my_bridge nginx Verify Communication Between Containers: docker exec -it container1 ping container2 Example 2: Host Network Run a Container on the Host Network: docker run -d --name host_container --network host nginx Access the Container: Since the container uses the host’s network, you can access it directly via localhost: curl localhost Example 3: Overlay Network (Docker Swarm) Initialize Docker Swarm: docker swarm init Create an Overlay Network: docker network create --driver overlay my_overlay Deploy a Service on the Overlay Network: docker service create --name web --network my_overlay --replicas 3 nginx Verify Communication Between Containers on Different Hosts: Containers on different hosts can communicate using the overlay network. Example 4: Macvlan Network Create a Macvlan Network: docker network create -d macvlan --subnet=192.168.1.0/24 --gateway=192.168.1.1 -o parent=eth0 my_macvlan Run a Container on the Macvlan Network: docker run -d --name macvlan_container --network my_macvlan nginx Access the Container: The container will have its own MAC address and IP on the physical network. Advanced Networking Scenarios1. Linking Containers Legacy Linking (deprecated): docker run -d --name container1 nginxdocker run -d --name container2 --link container1 nginx Modern Alternative (use custom bridge networks): docker network create my_networkdocker run -d --name container1 --network my_network nginxdocker run -d --name container2 --network my_network nginx 2. Exposing Ports Expose a container port to the host: docker run -d --name web -p 8080:80 nginx Access the container via http://localhost:8080. 3. DNS Resolution Docker provides automatic DNS resolution for containers on the same network: docker run -d --name container1 --network my_bridge nginxdocker run -d --name container2 --network my_bridge busybox ping container1 4. Network Aliases Assign a custom DNS alias to a container: docker run -d --name container1 --network my_bridge --network-alias app nginxdocker run -d --name container2 --network my_bridge busybox ping app Troubleshooting Docker Networks Check Container IP Address: docker inspect -f &#39;{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}&#39; &amp;lt;container_name&amp;gt; Test Connectivity Between Containers: docker exec -it &amp;lt;container1&amp;gt; ping &amp;lt;container2&amp;gt; View Network Logs: docker logs &amp;lt;container_name&amp;gt; Summary of Docker Network Drivers| Network Type | Use Case ||——————|—————————————————————————–|| Bridge | Default network for single-host communication. || Host | High-performance use cases with no network isolation. || Overlay | Multi-host communication in Docker Swarm. || Macvlan | Assign MAC addresses to containers for legacy applications. || None | Disable networking for completely isolated containers. |This guide covers the essentials of Docker networking. Let me know if you need further clarification or help with specific scenarios!" }, { "title": "The most useful Docker commands", "url": "/posts/docker-usefull-commands/", "categories": "DEV, Docker", "tags": "cli, docker", "date": "2025-02-07 08:35:00 -0500", "snippet": "Here’s a comprehensive list of the most useful Docker commands for managing containers, images, networks, and volumes. These commands are essential for working with Docker in development, testing, and production environments.Docker Basics Check Docker Version: docker --version Display System-Wide Information: docker info Working with Images Pull an Image from Docker Hub: docker pull &amp;lt;image_name&amp;gt;:&amp;lt;tag&amp;gt; Example: docker pull nginx:latest List Downloaded Images: docker images Remove an Image: docker rmi &amp;lt;image_name&amp;gt;:&amp;lt;tag&amp;gt; Build an Image from a Dockerfile: docker build -t &amp;lt;image_name&amp;gt;:&amp;lt;tag&amp;gt; . Example: docker build -t myapp:1.0 . Tag an Image: docker tag &amp;lt;image_id&amp;gt; &amp;lt;new_image_name&amp;gt;:&amp;lt;tag&amp;gt; Push an Image to Docker Hub: docker push &amp;lt;image_name&amp;gt;:&amp;lt;tag&amp;gt; Working with Containers Run a Container: docker run &amp;lt;image_name&amp;gt;:&amp;lt;tag&amp;gt; Example: docker run -d -p 8080:80 nginx:latest Run a Container in Interactive Mode: docker run -it &amp;lt;image_name&amp;gt;:&amp;lt;tag&amp;gt; /bin/bash List Running Containers: docker ps List All Containers (Running and Stopped): docker ps -a Stop a Running Container: docker stop &amp;lt;container_id&amp;gt; Start a Stopped Container: docker start &amp;lt;container_id&amp;gt; Restart a Container: docker restart &amp;lt;container_id&amp;gt; Remove a Stopped Container: docker rm &amp;lt;container_id&amp;gt; Force Remove a Running Container: docker rm -f &amp;lt;container_id&amp;gt; View Container Logs: docker logs &amp;lt;container_id&amp;gt; Follow Container Logs: docker logs -f &amp;lt;container_id&amp;gt; Execute a Command in a Running Container: docker exec -it &amp;lt;container_id&amp;gt; &amp;lt;command&amp;gt; Example: docker exec -it mycontainer /bin/bash Inspect a Container: docker inspect &amp;lt;container_id&amp;gt; Copy Files Between Host and Container: docker cp &amp;lt;file_path&amp;gt; &amp;lt;container_id&amp;gt;:&amp;lt;container_path&amp;gt;docker cp &amp;lt;container_id&amp;gt;:&amp;lt;container_path&amp;gt; &amp;lt;file_path&amp;gt; Pause/Unpause a Container: docker pause &amp;lt;container_id&amp;gt;docker unpause &amp;lt;container_id&amp;gt; Networking List Networks: docker network ls Create a Network: docker network create &amp;lt;network_name&amp;gt; Inspect a Network: docker network inspect &amp;lt;network_name&amp;gt; Connect a Container to a Network: docker network connect &amp;lt;network_name&amp;gt; &amp;lt;container_id&amp;gt; Disconnect a Container from a Network: docker network disconnect &amp;lt;network_name&amp;gt; &amp;lt;container_id&amp;gt; Remove a Network: docker network rm &amp;lt;network_name&amp;gt; Volumes List Volumes: docker volume ls Create a Volume: docker volume create &amp;lt;volume_name&amp;gt; Inspect a Volume: docker volume inspect &amp;lt;volume_name&amp;gt; Remove a Volume: docker volume rm &amp;lt;volume_name&amp;gt; Mount a Volume in a Container: docker run -v &amp;lt;volume_name&amp;gt;:&amp;lt;container_path&amp;gt; &amp;lt;image_name&amp;gt; Docker Compose Start Services: docker-compose up Start Services in Detached Mode: docker-compose up -d Stop Services: docker-compose down Rebuild and Restart Services: docker-compose up --build View Logs for a Service: docker-compose logs &amp;lt;service_name&amp;gt; Scale Services: docker-compose up --scale &amp;lt;service_name&amp;gt;=&amp;lt;num_instances&amp;gt; System Cleanup Remove All Stopped Containers: docker container prune Remove All Unused Images: docker image prune -a Remove All Unused Networks: docker network prune Remove All Unused Volumes: docker volume prune Remove All Unused Objects: docker system prune -a Advanced Commands Save an Image to a Tar File: docker save -o &amp;lt;file_name&amp;gt;.tar &amp;lt;image_name&amp;gt; Load an Image from a Tar File: docker load -i &amp;lt;file_name&amp;gt;.tar View Resource Usage: docker stats View Docker Events: docker events Check Disk Usage: docker system df Docker Swarm (For Orchestration) Initialize a Swarm: docker swarm init Join a Swarm as a Worker: docker swarm join --token &amp;lt;worker_token&amp;gt; &amp;lt;manager_ip&amp;gt;:&amp;lt;port&amp;gt; List Nodes in the Swarm: docker node ls Deploy a Stack: docker stack deploy -c &amp;lt;compose_file&amp;gt; &amp;lt;stack_name&amp;gt; List Services in a Stack: docker stack services &amp;lt;stack_name&amp;gt; This list covers the most useful Docker commands for day-to-day operations. Let me know if you need further clarification or examples! 🐳🚀" }, { "title": "List of 100 essential Vim commands", "url": "/posts/list-of-100-essential-Vim-commands/", "categories": "CLI", "tags": "vim, edi", "date": "2025-02-06 17:35:00 -0500", "snippet": "Vim is a powerful text editor, but it has a steep learning curve. Below is a list of 100 essential Vim commands categorized for easy reference. These commands will help you navigate, edit, and manage files efficiently in Vim.Basic Navigation h - Move cursor left. j - Move cursor down. k - Move cursor up. l - Move cursor right. w - Move to the start of the next word. b - Move to the start of the previous word. e - Move to the end of the current word. 0 - Move to the beginning of the line. $ - Move to the end of the line. gg - Move to the first line of the file. G - Move to the last line of the file. :n - Move to line number n (e.g., :10 goes to line 10). Ctrl + f - Scroll forward one page. Ctrl + b - Scroll backward one page. Ctrl + d - Scroll down half a page. Ctrl + u - Scroll up half a page. H - Move to the top of the screen. M - Move to the middle of the screen. L - Move to the bottom of the screen. % - Jump to the matching bracket ({[]}).Insert Mode i - Insert before the cursor. I - Insert at the beginning of the line. a - Insert after the cursor. A - Insert at the end of the line. o - Insert a new line below the current line. O - Insert a new line above the current line. s - Delete the character under the cursor and enter insert mode. S - Delete the entire line and enter insert mode. Esc - Exit insert mode and return to normal mode.Editing Text x - Delete the character under the cursor. X - Delete the character before the cursor. dd - Delete the current line. dw - Delete from the cursor to the start of the next word. d$ - Delete from the cursor to the end of the line. d0 - Delete from the cursor to the beginning of the line. yy - Yank (copy) the current line. yw - Yank from the cursor to the start of the next word. y$ - Yank from the cursor to the end of the line. p - Paste after the cursor. P - Paste before the cursor. u - Undo the last action. Ctrl + r - Redo the last undone action. . - Repeat the last command. J - Join the current line with the next line. r - Replace the character under the cursor. R - Enter replace mode (overwrite text). ~ - Toggle case of the character under the cursor. &amp;gt;&amp;gt; - Indent the current line. &amp;lt;&amp;lt; - Unindent the current line. == - Auto-indent the current line.Search and Replace /pattern - Search forward for pattern. ?pattern - Search backward for pattern. n - Repeat the last search forward. N - Repeat the last search backward. :%s/old/new/g - Replace all occurrences of old with new in the entire file. :%s/old/new/gc - Replace with confirmation. :s/old/new/g - Replace in the current line. :s/old/new/gc - Replace in the current line with confirmation. * - Search for the word under the cursor. # - Search backward for the word under the cursor.File Operations :w - Save the file. :wq - Save and quit. :q - Quit (if no changes were made). :q! - Quit without saving changes. :w filename - Save the file with a new name. :e filename - Open another file in the current buffer. :r filename - Insert the contents of another file into the current file. :saveas filename - Save the file with a new name and switch to it. :bn - Switch to the next buffer. :bp - Switch to the previous buffer. :bd - Close the current buffer. :ls - List all open buffers. :e! - Revert to the last saved version of the file.Visual Mode v - Enter visual mode (character-wise). V - Enter visual line mode. Ctrl + v - Enter visual block mode. y - Yank (copy) the selected text. d - Delete the selected text. &amp;gt; - Indent the selected text. &amp;lt; - Unindent the selected text. ~ - Toggle case of the selected text.Advanced Editing ciw - Change the inner word (delete and enter insert mode). c$ - Change from the cursor to the end of the line. c0 - Change from the cursor to the beginning of the line. ct&amp;lt;char&amp;gt; - Change until the specified character. cf&amp;lt;char&amp;gt; - Change up to and including the specified character. diw - Delete the inner word. dt&amp;lt;char&amp;gt; - Delete until the specified character. df&amp;lt;char&amp;gt; - Delete up to and including the specified character. gUw - Uppercase the current word. guw - Lowercase the current word. g~~ - Toggle case of the entire line. gqq - Format the current line to a specific width. :set number - Show line numbers. :set nonumber - Hide line numbers. :set wrap - Enable line wrapping. :set nowrap - Disable line wrapping. :set paste - Enable paste mode (prevents auto-indentation). :set nopaste - Disable paste mode. :set spell - Enable spell checking.Tips for Using Vim Efficiently Practice regularly to build muscle memory. Use .vimrc to customize Vim settings and create shortcuts. Learn to use macros (q) for repetitive tasks. Explore plugins like NERDTree, vim-airline, and fzf for enhanced functionality.Let me know if you need further clarification or examples! 🚀" }, { "title": "Functional Analysis (FA) Example", "url": "/posts/functional-analysis-example/", "categories": "Divers", "tags": "fa", "date": "2025-02-06 14:35:00 -0500", "snippet": "Certainly! Functional Analysis in the context of Information Technology (TI) involves breaking down a system or application into its functional components to understand how it works, identify requirements, and ensure it meets user needs. Below is an example of a Functional Analysis for a Customer Relationship Management (CRM) System in a TI context.Example: Functional Analysis of a CRM System1. Project Overview System Name: CRM System Purpose: Manage customer interactions, track sales, and improve customer service. Stakeholders: Sales team, customer support team, IT department.2. Functional RequirementsFunctional requirements describe what the system should do. These are typically divided into modules or features. Module Functional Requirements User Management - Allow administrators to create, update, and delete user accounts.   - Assign roles (e.g., sales rep, manager) and permissions to users. Customer Management - Add, edit, and delete customer records.   - Store customer contact information, purchase history, and preferences. Sales Management - Track sales opportunities and deals.   - Generate sales reports and forecasts. Support Management - Log and track customer support tickets.   - Assign tickets to support agents and monitor resolution times. Reporting - Generate reports on sales performance, customer satisfaction, and support metrics.   - Export reports in PDF, Excel, or CSV formats. 3. Use CasesUse cases describe how users interact with the system. Use Case Description Create Customer Record A sales representative adds a new customer to the system. Update Sales Opportunity A manager updates the status of a sales opportunity (e.g., from “Prospect” to “Closed-Won”). Generate Sales Report A manager generates a monthly sales report to analyze performance. Assign Support Ticket A support manager assigns a ticket to an agent based on workload and expertise. 4. Data Flow DiagramsData flow diagrams (DFDs) show how data moves through the system. Level 0 (Context Diagram): +-------------------+ +-------------------+ +-------------------+| Sales Team | | Support Team | | Admin Team |+-------------------+ +-------------------+ +-------------------+ | | | v v v+---------------------------------------------------------------+| CRM System |+---------------------------------------------------------------+ | | | v v v+-------------------+ +-------------------+ +-------------------+| Database | | Reporting Tool | | Email Service |+-------------------+ +-------------------+ +-------------------+ Level 1 (Detailed DFD): Shows how data flows within the CRM system (e.g., customer data → sales module → database). 5. Functional DecompositionBreak down the system into smaller, manageable components. Component Subcomponents User Management - User authentication   - Role-based access control Customer Management - Customer profile management   - Interaction history tracking Sales Management - Sales pipeline tracking   - Deal stage management Support Management - Ticket creation and assignment   - Ticket resolution tracking Reporting - Sales performance reports   - Customer satisfaction reports 6. Non-Functional RequirementsThese describe how the system should perform. Category Requirements Performance - The system should handle up to 1,000 concurrent users. Security - Data should be encrypted in transit and at rest.   - Implement role-based access control (RBAC). Scalability - The system should scale horizontally to support future growth. Usability - The user interface should be intuitive and require minimal training. 7. Functional Analysis Deliverables Functional Requirements Document (FRD): Detailed description of system features. Use Case Diagrams: Visual representation of user interactions. Data Flow Diagrams (DFDs): Show how data moves through the system. Wireframes/Mockups: Visual design of the user interface. Traceability Matrix: Maps requirements to use cases and test cases.Example: Functional Requirements Document (FRD) Excerpt1. User Management Requirement ID: UM-001 Description: Administrators should be able to create user accounts. Input: User details (name, email, role). Output: New user account created in the database. Priority: High2. Sales Management Requirement ID: SM-001 Description: Sales representatives should be able to track sales opportunities. Input: Opportunity details (customer, value, stage). Output: Opportunity added to the sales pipeline. Priority: HighExample: Use Case Diagram+-------------------+ +-------------------+ +-------------------+| Sales Rep | | Support Agent | | Admin |+-------------------+ +-------------------+ +-------------------+ | | | v v v +-------------------+ +-------------------+ +-------------------+ | Create Customer | | Assign Ticket | | Manage Users | +-------------------+ +-------------------+ +-------------------+Example: Traceability Matrix| Requirement ID | Use Case | Test Case ||———————|———————–|————————|| UM-001 | Create User Account | TC-001: Verify user creation. || SM-001 | Track Sales Opportunity| TC-002: Verify opportunity tracking. |Tools for Functional Analysis Diagramming Tools: Lucidchart, Visio: For creating use case diagrams and DFDs. Requirements Management: Jira, Trello: For tracking functional requirements. Wireframing: Figma, Balsamiq: For designing user interfaces. This example demonstrates how to perform a Functional Analysis in a TI context. Let me know if you need further details or help with specific tools! 🚀" }, { "title": "Example VBB", "url": "/posts/vbb-example/", "categories": "Marketing", "tags": "vbb", "date": "2025-02-06 01:35:00 -0500", "snippet": "Certainly! Let’s dive deeper into a detailed example of how value-based bidding works in practice, using a fictional e-commerce business.Business Scenario:Imagine you own an online store called “TechGadgets” that sells a variety of electronics, including smartphones, laptops, and accessories. Your average order values are as follows: Smartphones: $500 Laptops: $1,200 Accessories: $50You want to run a Google Ads campaign to drive sales, and you decide to use value-based bidding to maximize your return on ad spend (ROAS).Step-by-Step Example:1. Set Up Conversion Tracking: Conversion Actions: You set up conversion tracking in Google Ads to track purchases. Each conversion action is tagged with the value of the purchase. Value Assignment: You assign different values to different products: Smartphone purchase: $500 Laptop purchase: $1,200 Accessory purchase: $50 2. Choose a Value-Based Bidding Strategy: Maximize Conversion Value: You select the “Maximize Conversion Value” bidding strategy. This tells Google Ads to automatically adjust your bids to get the most conversion value within your daily budget.3. Campaign Structure: Ad Groups: You create separate ad groups for each product category: Ad Group 1: Smartphones Ad Group 2: Laptops Ad Group 3: Accessories 4. Historical Data and Learning Phase: Initial Data: Google Ads uses your historical data to understand which keywords, audiences, and placements are most likely to lead to high-value conversions. Learning Phase: During the initial phase, the algorithm tests different bid amounts and gathers data on which impressions lead to the highest-value conversions.5. Real-Time Bidding: Smartphone Search: A user searches for “best smartphone deals.” Google Ads predicts that this user has a 10% chance of making a $500 purchase. Laptop Search: Another user searches for “affordable laptops for students.” Google Ads predicts that this user has a 5% chance of making a $1,200 purchase. Accessory Search: A third user searches for “phone cases.” Google Ads predicts that this user has a 20% chance of making a $50 purchase.6. Bid Adjustments: Smartphone Bid: Based on the predicted conversion value ($500 * 10% = $50), Google Ads might bid $10 for this impression. Laptop Bid: Based on the predicted conversion value ($1,200 * 5% = $60), Google Ads might bid $15 for this impression. Accessory Bid: Based on the predicted conversion value ($50 * 20% = $10), Google Ads might bid $2 for this impression.7. Optimization Over Time: Performance Data: As the campaign runs, Google Ads collects more data and refines its predictions. It might find that certain demographics or times of day lead to higher-value conversions. Budget Allocation: The algorithm automatically allocates more of your budget to the ad groups and keywords that are driving the highest-value conversions. For example, if laptop purchases are generating the most value, more budget might be shifted to the “Laptops” ad group.8. Reporting and Analysis: ROAS Calculation: After a month, you review your campaign performance. You spent $5,000 on ads and generated $25,000 in revenue, resulting in a ROAS of 5:1. Breakdown by Product: Smartphones: $2,000 spent, $10,000 revenue (ROAS: 5:1) Laptops: $2,500 spent, $12,000 revenue (ROAS: 4.8:1) Accessories: $500 spent, $3,000 revenue (ROAS: 6:1) 9. Adjustments: Refine Targeting: Based on the data, you might decide to refine your targeting further. For example, you could create a custom audience for users who have previously visited your site but didn’t purchase, and assign higher values to these potential conversions. Ad Creatives: You might also test different ad creatives to see if they can drive even higher-value conversions. For instance, highlighting the benefits of your laptops in a video ad might increase their perceived value.Summary:In this example, value-based bidding allows TechGadgets to focus its advertising budget on the products and customers that generate the most revenue. By assigning specific values to different conversion actions and using an automated bidding strategy, the campaign maximizes the overall return on ad spend. Over time, the system learns and optimizes, ensuring that your budget is used as efficiently as possible to drive high-value conversions.This approach not only increases revenue but also provides valuable insights into which products and customer segments are most profitable, allowing for more informed business decisions.Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "How VBB works", "url": "/posts/how-vbb-works/", "categories": "Marketing", "tags": "vbb", "date": "2025-02-06 01:35:00 -0500", "snippet": "Certainly! Value-based bidding is a strategy used in digital advertising, particularly in platforms like Google Ads and Facebook Ads, where the primary goal is to maximize the value generated from each ad impression rather than just focusing on cost-per-click (CPC) or cost-per-impression (CPM). Here’s a breakdown of how it works:1. Understanding Value: Value in this context refers to the potential revenue or profit that can be generated from a conversion (e.g., a sale, lead, or other desired action). This value can be estimated based on historical data, customer lifetime value (CLV), or other business metrics.2. Conversion Tracking: To implement value-based bidding, you need to have robust conversion tracking in place. This means tracking not just whether a conversion happened, but also the value associated with each conversion. For example, if you’re running an e-commerce campaign, you would track the revenue generated from each sale.3. Bidding Strategy: Automated Bidding: Most value-based bidding strategies are automated, meaning the advertising platform uses machine learning to adjust bids in real-time. The platform considers the likelihood of a conversion and the expected value of that conversion when deciding how much to bid for each impression. Maximize Conversion Value: One common value-based bidding strategy is “Maximize Conversion Value.” In this strategy, the platform automatically sets bids to get the most conversion value within your budget. It will bid higher on impressions that are more likely to lead to high-value conversions and lower on those that are less likely.4. Data Input: Historical Data: The platform uses historical data to predict future performance. The more data you have, the better the platform can optimize your bids. Value Rules: You can also set rules or provide additional data to help the platform understand the value of different types of conversions. For example, you might assign a higher value to conversions from new customers versus returning customers.5. Optimization: Continuous Learning: The bidding algorithm continuously learns and adjusts based on new data. This means that over time, the system becomes better at predicting which impressions will lead to the highest value conversions. Budget Allocation: The platform will allocate more of your budget to campaigns, ad groups, or keywords that are driving higher-value conversions, ensuring that your budget is spent as efficiently as possible.6. Reporting and Analysis: Performance Metrics: You’ll have access to metrics like return on ad spend (ROAS), which measures the revenue generated for every dollar spent on advertising. This helps you understand the effectiveness of your value-based bidding strategy. Adjustments: Based on performance data, you can make adjustments to your campaigns, such as reallocating budget, refining target audiences, or tweaking ad creatives to further optimize for value.Example Scenario:Imagine you run an online store selling electronics. You know that customers who purchase laptops tend to spend more than those who buy accessories. With value-based bidding, you can set a higher value for conversions related to laptop purchases. The advertising platform will then prioritize showing your ads to users who are more likely to buy laptops, even if it means bidding higher for those impressions. Over time, this strategy should lead to a higher overall return on investment (ROI) for your ad spend.Benefits: Efficiency: Maximizes the return on your ad spend by focusing on high-value conversions. Automation: Reduces the need for manual bid adjustments, saving time and effort. Scalability: Works well for large campaigns with many variables, where manual bidding would be impractical.Challenges: Data Dependency: Requires accurate and comprehensive conversion and value data. Learning Period: The algorithm may take some time to optimize, especially for new campaigns or products. Complexity: May be more complex to set up and manage compared to simpler bidding strategies.In summary, value-based bidding is a powerful strategy that aligns your advertising efforts with your business goals by focusing on the value of conversions rather than just the cost of acquiring them. It leverages automation and data to optimize bids in real-time, helping you get the most out of your advertising budget." }, { "title": "Manage and control access to social media accounts", "url": "/posts/Manage-and-control-access-to-social-media-accounts/", "categories": "Marketing", "tags": "control, access, scim, sso, social-media", "date": "2025-02-06 01:35:00 -0500", "snippet": "Managing access to social media accounts in a large enterprise with multiple accounts across platforms, while also granting access to third-party partners, requires a robust and secure solution. Below are the best practices and tools to achieve this:1. Centralized Social Media Management PlatformUse a Social Media Management (SMM) platform that supports enterprise-grade features, including SSO integration, role-based access control (RBAC), and third-party collaboration.Top Tools: Hootsuite Enterprise: Supports SSO (e.g., SAML, OAuth). Role-based access control for internal teams and external partners. Advanced analytics and reporting. Sprout Social: SSO integration (e.g., Okta, Azure AD). Granular permissions for teams and external collaborators. Unified inbox for managing messages and comments. Buffer (for Enterprise): SSO support. Team collaboration features. Simple and intuitive interface. HubSpot (Social Media Tools): SSO integration with CRM. Advanced automation and reporting. Role-based access for teams and partners. Agorapulse: SSO support. Role-based permissions for internal and external users. Social inbox and scheduling features. 2. Single Sign-On (SSO) IntegrationIntegrate your SMM platform with your enterprise SSO provider (e.g., Okta, Azure AD, OneLogin) to ensure secure and centralized access.Steps to Enable SSO: Configure SSO in the SMM Platform: Go to the platform’s settings and enable SSO. Provide your IdP’s metadata (e.g., SAML certificate, SSO URL). Map User Attributes: Ensure user roles and permissions are mapped correctly. Test SSO: Verify that users can log in using their enterprise credentials. 3. Role-Based Access Control (RBAC)Implement RBAC to define what users (internal and external) can do within the SMM platform.Common Roles: Admin: Full access to all accounts and settings. Content Creator: Can create and schedule posts but cannot publish. Analyst: Can view analytics and reports but cannot post. Moderator: Can respond to comments and messages but cannot create posts. Partner: Limited access to specific accounts or features.How to Implement RBAC: Use the SMM platform’s built-in role management features. Assign roles based on responsibilities (e.g., internal team vs. third-party partners).4. Third-Party Access ManagementGrant limited access to third-party partners while maintaining control and security.Best Practices: Use Partner Roles: Create custom roles for partners with limited permissions (e.g., view-only access or access to specific accounts). Audit Access Regularly: Review and update partner permissions periodically. Use Separate Accounts: Create separate accounts for partners to avoid sharing internal credentials. Monitor Activity: Use the SMM platform’s audit logs to track partner activity. 5. Secure Password ManagementUse a password manager to securely store and share credentials for social media accounts.Top Tools: LastPass 1Password DashlaneHow to Use: Store social media account credentials in the password manager. Share credentials securely with internal teams and partners.6. Approval WorkflowsImplement approval workflows to ensure all posts and actions are reviewed before publishing.How to Set Up: Use the SMM platform’s approval features. Define a workflow (e.g., content creator → manager → admin).7. Monitor and Analyze PerformanceUse the SMM platform’s analytics and reporting tools to track performance and identify trends.Key Metrics to Track: Engagement (likes, comments, shares). Reach and impressions. Conversions (leads, sales). Sentiment analysis.8. Backup and RecoveryRegularly back up your social media data and configurations.How to Back Up: Use the SMM platform’s export features. Store backups securely (e.g., cloud storage, encrypted drives).9. Training and DocumentationTrain your team and partners on how to use the SMM platform securely and effectively.Training Topics: How to use the SMM platform. Social media policies and compliance. Crisis management and escalation procedures.10. Example Workflow Content Creation: A content creator drafts posts and schedules them in the SMM platform. Approval: The post is reviewed and approved by a manager or compliance officer. Publishing: The approved post is published across all relevant platforms. Monitoring: A moderator monitors comments and messages, responding as needed. Reporting: An analyst generates performance reports and shares insights with the team. Recommended SolutionFor a large enterprise with multiple accounts and third-party partners, I recommend using Hootsuite Enterprise or Sprout Social with SSO integration and RBAC. These platforms offer the necessary features to securely manage access, collaborate with partners, and monitor performance.ConclusionBy using a centralized SMM platform, integrating SSO, implementing RBAC, and following best practices for third-party access management, you can securely and efficiently manage social media accounts in a large enterprise. Let me know if you need further assistance! 🚀" }, { "title": "Access to home services from local network (DS)", "url": "/posts/Access-to-home-services-from-local-network/", "categories": "Architecture", "tags": "privacy, docker, wireguard, local-network, local-services", "date": "2025-02-05 19:35:00 -0500", "snippet": "Yes, it is absolutely possible to access your home services (e.g., Nextcloud, Gogs) from your local network even when they are configured to be accessible only via WireGuard. However, there are a few considerations and configurations you need to address to make this work seamlessly.Why This HappensWhen you configure services to bind to the WireGuard IP (e.g., 10.8.0.2), they are only accessible through the WireGuard interface. If you try to access them from your local network (e.g., 192.168.1.x), the traffic won’t route correctly because: The services are bound to the WireGuard IP (10.8.0.2), not the local network IP (192.168.1.x). Your local devices don’t know how to route traffic to the WireGuard subnet (10.8.0.0/24).Solutions to Access Home Services Locally1. Bind Services to Both WireGuard and Local Network IPsModify your Docker Compose configuration to bind services to both the WireGuard IP and the local network IP.Example for Nextcloud:nextcloud: image: nextcloud:latest container_name: nextcloud ports: - &quot;10.8.0.2:8080:80&quot; # Bind to WireGuard IP - &quot;192.168.1.100:8080:80&quot; # Bind to local network IP volumes: - nextcloud_data:/var/www/html restart: unless-stopped Replace 192.168.1.100 with the local IP of your home server. Now, you can access Nextcloud: From the local network: http://192.168.1.100:8080 From the WireGuard VPN: http://10.8.0.2:8080 2. Use Split DNSSet up a local DNS server (e.g., Pi-hole) to resolve your service domains (e.g., nextcloud.home) to the local IP (192.168.1.100) when accessed from the local network, and to the WireGuard IP (10.8.0.2) when accessed remotely.Steps: Install Pi-hole:Add Pi-hole to your Docker Compose: pihole: image: pihole/pihole:latest container_name: pihole environment: - TZ=UTC - WEBPASSWORD=your_password_here volumes: - pihole_data:/etc/pihole - pihole_dnsmasq:/etc/dnsmasq.d ports: - &quot;53:53/tcp&quot; - &quot;53:53/udp&quot; - &quot;80:80/tcp&quot; restart: unless-stopped Add Local DNS Entries: In the Pi-hole web interface, go to Local DNS &amp;gt; DNS Records. Add entries for your services: nextcloud.home → 192.168.1.100 gogs.home → 192.168.1.100 Configure Remote DNS: On the WireGuard client configuration, set the DNS server to the Pi-hole IP (10.8.0.2). Add DNS entries in Pi-hole for the WireGuard subnet: nextcloud.home → 10.8.0.2 gogs.home → 10.8.0.2 Access Services: From the local network: http://nextcloud.home:8080 From the WireGuard VPN: http://nextcloud.home:8080 3. Use NAT (Network Address Translation)Configure your home server to route traffic between the local network (192.168.1.0/24) and the WireGuard subnet (10.8.0.0/24).Steps: Enable IP Forwarding:On the home server, edit /etc/sysctl.conf: net.ipv4.ip_forward=1 Apply changes: sudo sysctl -p Add NAT Rules:On the home server, add iptables rules to route traffic: sudo iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -d 10.8.0.0/24 -j MASQUERADEsudo iptables -A FORWARD -s 192.168.1.0/24 -d 10.8.0.0/24 -j ACCEPTsudo iptables -A FORWARD -s 10.8.0.0/24 -d 192.168.1.0/24 -j ACCEPT Persist iptables Rules:Save the rules so they persist after reboot: sudo apt install iptables-persistentsudo netfilter-persistent save Access Services: From the local network, use the WireGuard IP (10.8.0.2) to access services. Example: http://10.8.0.2:8080 4. Use a Reverse ProxySet up a reverse proxy (e.g., Nginx, Traefik) to handle routing for both local and WireGuard traffic.Example with Nginx: Install Nginx:Add Nginx to your Docker Compose: nginx: image: nginx:latest container_name: nginx ports: - &quot;80:80&quot; - &quot;443:443&quot; volumes: - ./nginx.conf:/etc/nginx/nginx.conf - ./certs:/etc/ssl/certs restart: unless-stopped Configure Nginx:Edit nginx.conf to route traffic based on the source IP: server { listen 80; server_name nextcloud.home; location / { proxy_pass http://10.8.0.2:8080; # WireGuard IP }}server { listen 80; server_name nextcloud.home; # Local network IP set $local_ip 192.168.1.100; if ($remote_addr ~ ^192\\.168\\.1\\.) { proxy_pass http://$local_ip:8080; } location / { proxy_pass http://10.8.0.2:8080; # WireGuard IP }} Access Services: From the local network: http://nextcloud.home From the WireGuard VPN: http://nextcloud.home Which Solution Should You Use? Bind to Both IPs: Simplest solution if you don’t mind managing multiple bindings. Split DNS: Best for seamless access with domain names. NAT: Useful if you want to route all traffic through WireGuard. Reverse Proxy: Ideal for advanced setups with multiple services and domains." }, { "title": "Docker compose for &quot;hub and spoke&quot;", "url": "/posts/docker-compose-for-hub-and-spoke/", "categories": "Architecture", "tags": "hub-and-spoke, docker, docker-compose", "date": "2025-02-05 02:35:00 -0500", "snippet": "Absolutely! Below are Docker Compose examples for both the cloud server (DigitalOcean) and the home server (local network). These configurations will help you set up WireGuard on the cloud server and run your services (e.g., Nextcloud, Gogs) on the home server.1. Docker Compose for the Cloud Server (DigitalOcean)This setup will run WireGuard on the cloud server to act as the VPN hub.docker-compose.yml for Cloud Serverversion: &#39;3.8&#39;services: wireguard: image: linuxserver/wireguard:latest container_name: wireguard environment: - PUID=1000 - PGID=1000 - TZ=UTC - SERVERURL=&amp;lt;DROPLET_IP&amp;gt; # Replace with your DigitalOcean Droplet IP - SERVERPORT=51820 - PEERS=home # Name of the peer (home server) - PEERDNS=1.1.1.1 volumes: - wireguard_config:/config - /lib/modules:/lib/modules ports: - &quot;51820:51820/udp&quot; cap_add: - NET_ADMIN - SYS_MODULE sysctls: - net.ipv4.ip_forward=1 - net.ipv6.conf.all.forwarding=1 restart: unless-stoppedvolumes: wireguard_config:Steps to Run on Cloud Server Install Docker and Docker Compose: curl -fsSL https://get.docker.com | shsudo usermod -aG docker $USERsudo apt install docker-compose Create the docker-compose.yml file:Save the above configuration in a file named docker-compose.yml. Start WireGuard: docker-compose up -d Get the Home Server Configuration: After starting the container, the WireGuard configuration for the home server will be generated in the wireguard_config volume. Locate the home.conf file (for the home server peer) in the volume: docker-compose exec wireguard cat /config/peer_home/peer_home.conf Copy this configuration to your home server. 2. Docker Compose for the Home Server (Local Network)This setup will run your services (e.g., Nextcloud, Gogs) and connect to the cloud server via WireGuard.docker-compose.yml for Home Serverversion: &#39;3.8&#39;services: wireguard: image: linuxserver/wireguard:latest container_name: wireguard environment: - PUID=1000 - PGID=1000 - TZ=UTC - PEERDNS=1.1.1.1 volumes: - wireguard_config:/config - /lib/modules:/lib/modules cap_add: - NET_ADMIN - SYS_MODULE sysctls: - net.ipv4.ip_forward=1 - net.ipv6.conf.all.forwarding=1 restart: unless-stopped nextcloud: image: nextcloud:latest container_name: nextcloud depends_on: - wireguard volumes: - nextcloud_data:/var/www/html ports: - &quot;10.8.0.2:8080:80&quot; # Bind to WireGuard IP restart: unless-stopped gogs: image: gogs/gogs:latest container_name: gogs depends_on: - wireguard volumes: - gogs_data:/data ports: - &quot;10.8.0.2:3000:3000&quot; # Bind to WireGuard IP restart: unless-stoppedvolumes: wireguard_config: nextcloud_data: gogs_data:Steps to Run on Home Server Install Docker and Docker Compose: curl -fsSL https://get.docker.com | shsudo usermod -aG docker $USERsudo apt install docker-compose Create the docker-compose.yml file:Save the above configuration in a file named docker-compose.yml. Add the WireGuard Configuration: Copy the home.conf file (from the cloud server) to the home server. Place it in the wireguard_config volume (e.g., ./wireguard_config/peer_home/peer_home.conf). Start the Services: docker-compose up -d 3. Accessing ServicesFrom the Cloud Server Use the WireGuard IP (10.8.0.2) to access services on the home server: Nextcloud: http://10.8.0.2:8080 Gogs: http://10.8.0.2:3000 From Your Local Machine Connect to the Cloud Server via WireGuard: Install WireGuard on your local machine. Use the configuration file generated for your local machine (similar to the home server setup). Access Services: Once connected to the VPN, use the WireGuard IP (10.8.0.2) to access services. 4. Additional ServicesYou can add more services to the home server’s docker-compose.yml file. For example:Add Pi-hole for Ad Blockingpihole: image: pihole/pihole:latest container_name: pihole environment: - TZ=UTC - WEBPASSWORD=your_password_here volumes: - pihole_data:/etc/pihole - pihole_dnsmasq:/etc/dnsmasq.d ports: - &quot;10.8.0.2:53:53/tcp&quot; - &quot;10.8.0.2:53:53/udp&quot; - &quot;10.8.0.2:8085:80&quot; # Web UI restart: unless-stoppedvolumes: pihole_data: pihole_dnsmasq:5. Security Considerations Firewall: Ensure only necessary ports are open on the cloud server (e.g., 51820/udp for WireGuard). Backups: Regularly back up your Docker volumes and configurations. Updates: Keep your Docker images and host system up to date.This setup provides a secure and private way to access your home services via a cloud server. Let me know if you need further assistance! 😊" }, { "title": "Hub-and-Spoke Architecture", "url": "/posts/hub-and-spoke-architecture/", "categories": "Architecture", "tags": "hub-and-spoke", "date": "2025-02-05 01:35:00 -0500", "snippet": "Certainly! A hub-and-spoke architecture is a network topology where a central hub acts as a single point of connectivity for multiple spokes (remote devices or networks). In your case, the cloud server (hub) connects to your home network (spoke), and possibly other devices (e.g., laptops, phones) as additional spokes. Here’s a breakdown of its benefits, use cases, and whether it’s preferable for your setup:Benefits of Hub-and-Spoke Architecture1. Centralized Management Simplified Configuration: All VPN connections terminate at the hub (cloud server), making it easier to manage peers, routes, and security policies. Single Point of Control: Apply firewall rules, access controls, or updates in one place (the hub) instead of configuring each spoke individually.2. Scalability Easily Add Spokes: Adding new devices (e.g., a second home, a phone, or a laptop) requires minimal changes to the hub. Just add a new peer configuration. Centralized Routing: The hub can route traffic between spokes (e.g., home server ↔ laptop) without requiring direct connections between spokes.3. Security Controlled Exposure: Only the hub’s IP (e.g., DigitalOcean droplet) is exposed to the internet. Your home network and services remain hidden behind the VPN. Encrypted Traffic: All communication between spokes and the hub uses WireGuard’s encryption, protecting data from eavesdropping.4. Cost-Effectiveness Static IP: A cloud server provides a stable endpoint (IP) for remote access, avoiding the need for dynamic DNS (DDNS) if your home IP changes. Reduced Complexity: Avoids the need for complex mesh networking setups (where every device connects to every other device).5. Simplified Monitoring Single Logging Point: Monitor all traffic and connections at the hub, making troubleshooting easier. Traffic Inspection: Apply security tools (e.g., intrusion detection) at the hub to analyze traffic entering/leaving the network.6. Reliability Persistent Hub: The cloud server (hub) is always online, ensuring spokes (e.g., your home server) can reconnect automatically if your home internet drops.7. Reduced Attack Surface No Publicly Exposed Services: Your home services (Nextcloud, Gogs) are only accessible via the encrypted VPN tunnel, not the public internet.When is Hub-and-Spoke Preferable?Use Cases Remote Access to Multiple Services: Access all your home services (Nextcloud, Gogs, etc.) through a single entry point (the hub). Multi-Device/Network Scenarios: Connect laptops, phones, or multiple home networks to the same hub. Static IP Requirement: Use the cloud server’s static IP to avoid dealing with dynamic home IPs. Centralized Security Policies: Enforce firewall rules or VPN access controls at the hub.When to Avoid Hub-and-Spoke Direct Peer-to-Peer Needs: If devices need to communicate directly (e.g., a laptop accessing a home server without routing through the hub), a mesh topology (e.g., Tailscale) is better. High Latency Sensitivity: If the hub is geographically distant, latency between spokes may increase. Single Point of Failure: If the hub goes down, all connectivity is lost. Mitigate this by using a highly available cloud provider.Your Use Case: Why Hub-and-Spoke WorksYou’re hosting services at home (Nextcloud, Gogs) and want to access them remotely via a cloud-based WireGuard VPN. Here’s why hub-and-spuche is ideal: Privacy: Services are never exposed to the public internet. Simplicity: Manage all devices (laptop, phone) through the cloud server. Scalability: Easily add more services or devices later. Cost: A $5/month DigitalOcean droplet is affordable for most users.Potential Downsides to Consider Single Point of Failure: If the cloud server goes down, you lose access. Mitigate by: Choosing a reliable cloud provider (e.g., AWS, DigitalOcean). Setting up automated backups for the hub. Latency: Traffic routes through the hub, which might add latency. For most applications (e.g., Nextcloud), this is negligible. Bandwidth Bottlenecks: The hub’s bandwidth limits throughput for all spokes. For example, a 1 Gbps cloud server can handle ~100 users streaming 10 Mbps each. Complexity: Requires initial setup (Docker, WireGuard, routing).Alternatives Mesh VPN (Tailscale): Automates peer-to-peer connections without a central hub. Better for direct device-to-device communication. Point-to-Point VPN: Directly connect your laptop to your home server (no hub). Works for single-device access but doesn’t scale well.Conclusion: Is Hub-and-Spoke Preferable for You?Yes, if: You want a centralized, secure way to access multiple home services. You’re comfortable managing a cloud server. You plan to add more devices (e.g., family members’ phones) later.No, if: You only need occasional access from one device (use point-to-point WireGuard). You want zero maintenance (use Tailscale instead).For your described use case (privacy-focused services at home, remote access via WireGuard), the hub-and-spuche architecture is an excellent choice. It balances security, scalability, and ease of management. Let me know if you’d like help optimizing it further! 😊" }, { "title": "Hub and Spoke or Site to Site VPN on docker", "url": "/posts/hub-and-spoke-with-docker/", "categories": "Architecture", "tags": "docker-compose, ubuntu, wireguard, gogs, nextcloud, cloud, middleware", "date": "2025-02-04 05:06:00 -0500", "snippet": "Absolutely! Below are Docker Compose examples for both the cloud server (DigitalOcean) and the home server (local network). These configurations will help you set up WireGuard on the cloud server and run your services (e.g., Nextcloud, Gogs) on the home server.1. Docker Compose for the Cloud Server (DigitalOcean)This setup will run WireGuard on the cloud server to act as the VPN hub.docker-compose.yml for Cloud Serverversion: &#39;3.8&#39;services: wireguard: image: linuxserver/wireguard:latest container_name: wireguard environment: - PUID=1000 - PGID=1000 - TZ=UTC - SERVERURL=&amp;lt;DROPLET_IP&amp;gt; # Replace with your DigitalOcean Droplet IP - SERVERPORT=51820 - PEERS=home # Name of the peer (home server) - PEERDNS=1.1.1.1 volumes: - wireguard_config:/config - /lib/modules:/lib/modules ports: - &quot;51820:51820/udp&quot; cap_add: - NET_ADMIN - SYS_MODULE sysctls: - net.ipv4.ip_forward=1 - net.ipv6.conf.all.forwarding=1 restart: unless-stoppedvolumes: wireguard_config:Steps to Run on Cloud Server Install Docker and Docker Compose: curl -fsSL https://get.docker.com | shsudo usermod -aG docker $USERsudo apt install docker-compose Create the docker-compose.yml file:Save the above configuration in a file named docker-compose.yml. Start WireGuard: docker-compose up -d Get the Home Server Configuration: After starting the container, the WireGuard configuration for the home server will be generated in the wireguard_config volume. Locate the home.conf file (for the home server peer) in the volume: docker-compose exec wireguard cat /config/peer_home/peer_home.conf Copy this configuration to your home server. 2. Docker Compose for the Home Server (Local Network)This setup will run your services (e.g., Nextcloud, Gogs) and connect to the cloud server via WireGuard.docker-compose.yml for Home Serverversion: &#39;3.8&#39;services: wireguard: image: linuxserver/wireguard:latest container_name: wireguard environment: - PUID=1000 - PGID=1000 - TZ=UTC - PEERDNS=1.1.1.1 volumes: - wireguard_config:/config - /lib/modules:/lib/modules cap_add: - NET_ADMIN - SYS_MODULE sysctls: - net.ipv4.ip_forward=1 - net.ipv6.conf.all.forwarding=1 restart: unless-stopped nextcloud: image: nextcloud:latest container_name: nextcloud depends_on: - wireguard volumes: - nextcloud_data:/var/www/html ports: - &quot;10.8.0.2:8080:80&quot; # Bind to WireGuard IP restart: unless-stopped gogs: image: gogs/gogs:latest container_name: gogs depends_on: - wireguard volumes: - gogs_data:/data ports: - &quot;10.8.0.2:3000:3000&quot; # Bind to WireGuard IP restart: unless-stoppedvolumes: wireguard_config: nextcloud_data: gogs_data:Steps to Run on Home Server Install Docker and Docker Compose: curl -fsSL https://get.docker.com | shsudo usermod -aG docker $USERsudo apt install docker-compose Create the docker-compose.yml file:Save the above configuration in a file named docker-compose.yml. Add the WireGuard Configuration: Copy the home.conf file (from the cloud server) to the home server. Place it in the wireguard_config volume (e.g., ./wireguard_config/peer_home/peer_home.conf). Start the Services: docker-compose up -d 3. Accessing ServicesFrom the Cloud Server Use the WireGuard IP (10.8.0.2) to access services on the home server: Nextcloud: http://10.8.0.2:8080 Gogs: http://10.8.0.2:3000 From Your Local Machine Connect to the Cloud Server via WireGuard: Install WireGuard on your local machine. Use the configuration file generated for your local machine (similar to the home server setup). Access Services: Once connected to the VPN, use the WireGuard IP (10.8.0.2) to access services. 4. Additional ServicesYou can add more services to the home server’s docker-compose.yml file. For example:Add Pi-hole for Ad Blockingpihole: image: pihole/pihole:latest container_name: pihole environment: - TZ=UTC - WEBPASSWORD=your_password_here volumes: - pihole_data:/etc/pihole - pihole_dnsmasq:/etc/dnsmasq.d ports: - &quot;10.8.0.2:53:53/tcp&quot; - &quot;10.8.0.2:53:53/udp&quot; - &quot;10.8.0.2:8085:80&quot; # Web UI restart: unless-stoppedvolumes: pihole_data: pihole_dnsmasq:5. Security Considerations Firewall: Ensure only necessary ports are open on the cloud server (e.g., 51820/udp for WireGuard). Backups: Regularly back up your Docker volumes and configurations. Updates: Keep your Docker images and host system up to date.Reverse Footnote" }, { "title": "Hub and Spoke or Site to Site VPN", "url": "/posts/hub-and-spoke/", "categories": "Architecture", "tags": "docker, cloud, ubuntu, wireguard, gogs, nextcloud", "date": "2025-02-04 03:03:00 -0500", "snippet": "Cet article montre comment mettre en place l’architecture connue comme Hub And Spoke 1.Yes, absolutely! You can set up WireGuard on a cloud provider (like DigitalOcean) and use it to securely connect to your home local network, where your services (e.g., Nextcloud, Gogs, etc.) are running. This setup is often called a “hub-and-spoke” or “site-to-site” VPN, where the cloud server acts as the central hub, and your home network is the spoke.This approach has several advantages: Static IP: The cloud server provides a static IP address, making it easier to connect to your home network. Accessibility: You can access your home services from anywhere via the cloud server. Security: All traffic between the cloud server and your home network is encrypted via WireGuard.Step-by-Step Guide: WireGuard in the Cloud + Home Local NetworkStep 1: Set Up a Cloud Server (DigitalOcean) Create a Droplet: Go to DigitalOcean and create a new Droplet. Choose Ubuntu or Debian as the OS. Enable IPv6 (optional but recommended for future-proofing). SSH into the Droplet: ssh root@&amp;lt;DROPLET_IP&amp;gt; Install WireGuard: sudo apt updatesudo apt install wireguard resolvconf Generate WireGuard Keys: wg genkey | sudo tee /etc/wireguard/privatekey | wg pubkey | sudo tee /etc/wireguard/publickey Step 2: Configure WireGuard on the Cloud Server Create the WireGuard Config:Edit /etc/wireguard/wg0.conf: [Interface]Address = 10.8.0.1/24PrivateKey = &amp;lt;CLOUD_SERVER_PRIVATE_KEY&amp;gt;ListenPort = 51820PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADEPostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE[Peer]# Home ServerPublicKey = &amp;lt;HOME_SERVER_PUBLIC_KEY&amp;gt;AllowedIPs = 10.8.0.2/32, 192.168.1.0/24 # Include your home LAN subnet Replace: &amp;lt;CLOUD_SERVER_PRIVATE_KEY&amp;gt;: The private key generated on the cloud server. &amp;lt;HOME_SERVER_PUBLIC_KEY&amp;gt;: The public key from your home server (generated in Step 3). Enable IP Forwarding:Edit /etc/sysctl.conf: net.ipv4.ip_forward=1 Apply changes: sudo sysctl -p Start WireGuard: sudo wg-quick up wg0sudo systemctl enable wg-quick@wg0 Step 3: Set Up WireGuard on Your Home Server Install WireGuard: sudo apt updatesudo apt install wireguard resolvconf Generate WireGuard Keys: wg genkey | sudo tee /etc/wireguard/privatekey | wg pubkey | sudo tee /etc/wireguard/publickey Create the WireGuard Config:Edit /etc/wireguard/wg0.conf: [Interface]Address = 10.8.0.2/24PrivateKey = &amp;lt;HOME_SERVER_PRIVATE_KEY&amp;gt;DNS = 1.1.1.1 # Optional[Peer]PublicKey = &amp;lt;CLOUD_SERVER_PUBLIC_KEY&amp;gt;Endpoint = &amp;lt;DROPLET_IP&amp;gt;:51820AllowedIPs = 10.8.0.0/24, 0.0.0.0/0 # Route all traffic through the VPN (optional)PersistentKeepalive = 25 Replace: &amp;lt;HOME_SERVER_PRIVATE_KEY&amp;gt;: The private key generated on the home server. &amp;lt;CLOUD_SERVER_PUBLIC_KEY&amp;gt;: The public key from the cloud server. Start WireGuard: sudo wg-quick up wg0sudo systemctl enable wg-quick@wg0 Step 4: Configure Routing on the Home Server Add a Route to the Home LAN:On the cloud server, ensure traffic to your home LAN subnet (192.168.1.0/24) is routed through the WireGuard interface: sudo ip route add 192.168.1.0/24 dev wg0 Test Connectivity:From the cloud server, ping your home server: ping 10.8.0.2 From the home server, ping the cloud server: ping 10.8.0.1 Step 5: Access Home Services via the Cloud Server Expose Services: Ensure your home services (e.g., Nextcloud, Gogs) are bound to the WireGuard IP (10.8.0.2) or your home LAN IP (192.168.1.x). Example for Nextcloud: docker run -d \\ --name nextcloud \\ -p 10.8.0.2:8080:80 \\ nextcloud Access Services: From the cloud server, access your home services using the WireGuard IP (e.g., http://10.8.0.2:8080). From your local machine, connect to the cloud server via WireGuard and access the services. Step 6: Secure the Setup Firewall Rules: On the cloud server, allow only WireGuard traffic: sudo ufw allow 51820/udpsudo ufw enable On the home server, restrict access to services via the WireGuard interface. Disable Public Access: Ensure your home services are not exposed to the public internet. Step 7: Optional - Add More DevicesYou can add more devices (e.g., laptop, phone) as peers to the WireGuard network: Generate keys for each device. Add them as peers in the cloud server’s wg0.conf: [Peer]PublicKey = &amp;lt;DEVICE_PUBLIC_KEY&amp;gt;AllowedIPs = 10.8.0.3/32 Configure the devices to connect to the cloud server.Benefits of This Setup Centralized Access: Use the cloud server as a gateway to access your home services. Privacy: All traffic is encrypted via WireGuard. Scalability: Easily add more devices or services.Reverse Footnote Voir ce liensur youtube &amp;#8617; " }, { "title": "Remotely access to home sercices via wireguard", "url": "/posts/acess-home-services-docker-wireguard/", "categories": "Privacy", "tags": "docker, gogs, nextcloud, duckdns, wireguard", "date": "2025-02-03 19:35:00 -0500", "snippet": "This post will guide and help you how to set up home services accessed remotely via wireguard.Absolutely! Let’s design a secure, privacy-focused setup where you host services (Nextcloud, Gogs, etc.) in Docker containers on your home network and access them remotely only via WireGuard. Here’s a complete step-by-step guide:Architecture Overview Home Server: A machine (e.g., Raspberry Pi, old PC) running Docker and WireGuard. Services: Nextcloud, Gogs, etc., running in Docker containers on the home server. WireGuard VPN: Securely connects your laptop/mobile devices to the home network. Access: Services are only exposed to the WireGuard VPN IP range (not the public internet).Step 1: Prepare the Home Server1.1 Install Docker and Docker Compose# Install Dockercurl -fsSL https://get.docker.com | shsudo usermod -aG docker $USER# Install Docker Composesudo apt install docker-compose1.2 Create a Docker NetworkCreate a dedicated Docker network for your services (to simplify routing):docker network create --subnet=172.20.0.0/24 internal-netStep 2: Set Up WireGuard on the Home Server2.1 Install WireGuardsudo apt install wireguard resolvconf2.2 Generate Keys# Generate server keyswg genkey | sudo tee /etc/wireguard/privatekey | wg pubkey | sudo tee /etc/wireguard/publickey2.3 Create WireGuard ConfigCreate /etc/wireguard/wg0.conf:[Interface]Address = 10.8.0.1/24PrivateKey = &amp;lt;HOME_SERVER_PRIVATE_KEY&amp;gt;ListenPort = 51820PostUp = iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADEPostDown = iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE[Peer]# LaptopPublicKey = &amp;lt;LAPTOP_PUBLIC_KEY&amp;gt;AllowedIPs = 10.8.0.2/32[Peer]# PhonePublicKey = &amp;lt;PHONE_PUBLIC_KEY&amp;gt;AllowedIPs = 10.8.0.3/32Replace: &amp;lt;HOME_SERVER_PRIVATE_KEY&amp;gt;: Contents of /etc/wireguard/privatekey. &amp;lt;LAPTOP_PUBLIC_KEY&amp;gt;/&amp;lt;PHONE_PUBLIC_KEY&amp;gt;: Public keys from your devices (generated later).2.4 Enable IP ForwardingEdit /etc/sysctl.conf:net.ipv4.ip_forward=1Apply changes:sudo sysctl -p2.5 Start WireGuardsudo wg-quick up wg0sudo systemctl enable wg-quick@wg0Step 3: Configure Docker Services3.1 Create docker-compose.ymlversion: &#39;3&#39;services: nextcloud: image: nextcloud networks: - internal-net restart: unless-stopped volumes: - nextcloud_data:/var/www/html ports: - &quot;10.8.0.1:8080:80&quot; # Expose only to WireGuard IP gogs: image: gogs/gogs networks: - internal-net restart: unless-stopped volumes: - gogs_data:/data ports: - &quot;10.8.0.1:3000:3000&quot; # Expose only to WireGuard IPnetworks: internal-net: external: truevolumes: nextcloud_data: gogs_data:3.2 Start Servicesdocker-compose up -dStep 4: Configure Client Devices (Laptop/Phone)4.1 Install WireGuard Windows/macOS: Download from wireguard.com/install. Android/iOS: Install the WireGuard app.4.2 Generate Client KeysOn each device, generate a key pair:# On Linux/macOSwg genkey | tee privatekey | wg pubkey &amp;gt; publickey4.3 Create Client ConfigsLaptop Example (laptop.conf):[Interface]Address = 10.8.0.2/24PrivateKey = &amp;lt;LAPTOP_PRIVATE_KEY&amp;gt;DNS = 1.1.1.1 # Optional[Peer]PublicKey = &amp;lt;HOME_SERVER_PUBLIC_KEY&amp;gt;Endpoint = &amp;lt;HOME_SERVER_PUBLIC_IP&amp;gt;:51820AllowedIPs = 10.8.0.0/24PersistentKeepalive = 25Phone Example (phone.conf):[Interface]Address = 10.8.0.3/24PrivateKey = &amp;lt;PHONE_PRIVATE_KEY&amp;gt;DNS = 1.1.1.1 # Optional[Peer]PublicKey = &amp;lt;HOME_SERVER_PUBLIC_KEY&amp;gt;Endpoint = &amp;lt;HOME_SERVER_PUBLIC_IP&amp;gt;:51820AllowedIPs = 10.8.0.0/24PersistentKeepalive = 254.4 Add Clients to the ServerOn the home server, add each client as a peer in /etc/wireguard/wg0.conf:[Peer]PublicKey = &amp;lt;LAPTOP_PUBLIC_KEY&amp;gt;AllowedIPs = 10.8.0.2/32[Peer]PublicKey = &amp;lt;PHONE_PUBLIC_KEY&amp;gt;AllowedIPs = 10.8.0.3/32Reload WireGuard:sudo wg syncconf wg0 &amp;lt;(sudo wg-quick strip wg0)Step 5: Access Services SecurelyOnce connected to WireGuard: Nextcloud: http://10.8.0.1:8080 Gogs: http://10.8.0.1:3000Step 6: Firewall and Security6.1 Restrict Docker PortsEnsure Docker services only bind to the WireGuard IP (as done in docker-compose.yml):ports: - &quot;10.8.0.1:8080:80&quot; # Not 0.0.0.0:8080:806.2 Configure UFW (Firewall)# Allow WireGuard trafficsudo ufw allow 51820/udp# Block all other incoming trafficsudo ufw default deny incomingsudo ufw enableStep 7: Dynamic DNS (Optional)If your home IP changes frequently, use a DDNS service (e.g., DuckDNS): Sign up at duckdns.org. Update the Endpoint in client configs to use your DDNS domain (e.g., myhome.duckdns.org:51820).Step 8: Add More ServicesTo add another service (e.g., Jellyfin):# In docker-compose.ymljellyfin: image: jellyfin/jellyfin networks: - internal-net ports: - &quot;10.8.0.1:8096:8096&quot;Troubleshooting Check WireGuard Status: sudo wg Test Connectivity: ping 10.8.0.1 # From a client device Inspect Docker Logs: docker logs nextcloud Final Notes Security: Only WireGuard port (51820/udp) is exposed to the internet. All services are hidden behind the VPN. Privacy: No reliance on third-party cloud providers. Scalability: Add more devices as WireGuard peers or deploy additional Docker services.This setup ensures your self-hosted services remain private and accessible only to your trusted devices. Let me know if you need help with specific services!" }, { "title": "Install Docker on ubuntu and Deepin", "url": "/posts/docker-install-ubuntu/", "categories": "DEV, Linux", "tags": "docker, ubuntu, deepin", "date": "2025-02-01 03:03:00 -0500", "snippet": "Cet article montre comment peut-on installer docker dans Ubuntu1.The best way to install Docker on Ubuntu is to use Docker’s official installation method, which ensures you get the latest stable version and proper configuration. Below is a step-by-step guide to install Docker on Ubuntu:UbuntuStep 1: Update Your SystemBefore installing Docker, ensure your system is up to date:sudo apt updatesudo apt upgrade -yStep 2: Install Required DependenciesInstall packages necessary for Docker installation:sudo apt install -y apt-transport-https ca-certificates curl software-properties-commonStep 3: Add Docker’s Official GPG KeyAdd Docker’s GPG key to ensure the authenticity of the packages:curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpgStep 4: Add Docker RepositoryAdd Docker’s official repository to your system’s package sources:echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &amp;gt; /dev/nullStep 5: Install Docker Engine Update the package index: sudo apt update Install Docker Engine, Docker CLI, and Containerd: sudo apt install -y docker-ce docker-ce-cli containerd.io Step 6: Verify Docker Installation Check the Docker version to confirm the installation: docker --version Start the Docker service and enable it to start on boot: sudo systemctl start dockersudo systemctl enable docker Verify Docker is running: sudo systemctl status docker Step 7: Run a Test ContainerRun a simple Docker container to verify everything is working:sudo docker run hello-worldIf Docker is installed correctly, you’ll see a message confirming the installation.Step 8: Manage Docker as a Non-Root User (Optional)By default, Docker requires sudo to run commands. To use Docker without sudo, add your user to the docker group:sudo usermod -aG docker $USERAfter running this command, log out and log back in for the changes to take effect.Step 9: Install Docker Compose (Optional)Docker Compose is a tool for defining and running multi-container Docker applications. To install it: Download the latest version of Docker Compose: sudo curl -L &quot;https://github.com/docker/compose/releases/download/$(curl -s https://api.github.com/repos/docker/compose/releases/latest | grep -Po &#39;&quot;tag_name&quot;: &quot;\\K.*\\d&#39;)&quot; -o /usr/local/bin/docker-compose Make the binary executable: sudo chmod +x /usr/local/bin/docker-compose Verify the installation: docker-compose --version Step 10: Configure Docker to Start on Boot (Optional)If you want Docker to start automatically when your system boots, enable the service:sudo systemctl enable dockerTroubleshooting Permission Denied Errors: If you encounter permission issues, ensure your user is in the docker group and restart your session. Docker Service Not Running: If Docker fails to start, check the logs with: sudo journalctl -u docker.service By following these steps, you’ll have Docker installed and ready to use on your Ubuntu system. Let me know if you need further assistance!Reverse FootnoteDeepinInstalling Docker and Docker Compose on Deepin (a Debian-based distribution) requires a few adjustments compared to Ubuntu. Here’s a step-by-step guide to ensure a correct installation:1. Install Docker on DeepinStep 1: Uninstall Old Docker Versions (if any)sudo apt remove docker docker-engine docker.io containerd runcsudo rm -rf /var/lib/dockerStep 2: Install Dependenciessudo apt updatesudo apt install -y ca-certificates curl gnupg lsb-releaseStep 3: Add Docker’s Official GPG Keycurl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpgStep 4: Add Docker RepositoryDeepin is based on Debian, so use the Debian repository (replace bullseye with your Debian codename if needed):echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian bullseye stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &amp;gt; /dev/nullStep 5: Install Docker Enginesudo apt updatesudo apt install -y docker-ce docker-ce-cli containerd.ioStep 6: Start and Enable Dockersudo systemctl start dockersudo systemctl enable dockerStep 7: Verify Installationsudo docker run hello-worldIf you see “Hello from Docker!”, it’s working!Step 8: Add User to Docker Group (Optional)To run Docker without sudo:sudo usermod -aG docker $USERnewgrp docker # Apply group changes without logout2. Install Docker Compose on DeepinOption 1: Install Docker Compose v2 (Recommended)Docker Compose v2 is included with Docker Engine for newer versions. Check if it’s already installed:docker compose versionIf not, install it manually:sudo apt install -y docker-compose-pluginOption 2: Install Docker Compose v1 (Legacy)sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-composeVerify:docker-compose --version3. Fix Common Issues on DeepinIssue 1: GPG Key ErrorsIf you get GPG errors, manually download the key:curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -Issue 2: Repository MismatchDeepin may not recognize Debian’s bullseye. Check your Debian base:lsb_release -aReplace bullseye in the repository URL with your Debian codename (e.g., buster).Issue 3: Dependency ConflictsIf apt complains about conflicts, use:sudo apt --fix-broken install4. Test Docker ComposeCreate a docker-compose.yml file:version: &#39;3&#39;services: web: image: nginx ports: - &quot;8080:80&quot;Run it:docker compose up -dAccess NGINX at http://localhost:8080.Summary Docker: Installed via Debian’s repository. Docker Compose: Use the plugin (docker compose) or legacy (docker-compose). Deepin-Specific Fixes: Adjust repositories for Debian base.After this, you’ll have a fully functional Docker and Docker Compose setup on Deepin! 🐳🚀Reverse Footnote Voir ce liensur youtube &amp;#8617; " }, { "title": "Dokerize Gogs with MySql", "url": "/posts/gogs-mysql-docker/", "categories": "Privacy", "tags": "docker, gogs, git, mysql, home-lac", "date": "2025-01-27 19:35:00 -0500", "snippet": "This post will guide and help you how to install Gogs with mysql database in docker.Running Gogs (a self-hosted Git service) with MySQL in Docker and making it accessible via SSH involves several steps. Below are the best practices and a step-by-step guide to set this up securely and efficiently.1. Prerequisites Docker and Docker Compose installed on your server. A domain name (optional but recommended for HTTPS). Basic knowledge of Docker, MySQL, and SSH.2. Directory StructureCreate a directory structure for your Gogs setup:gogs/├── docker-compose.yml├── gogs/│ ├── data/│ └── custom/conf/└── mysql/ └── data/3. Docker Compose ConfigurationCreate a docker-compose.yml file to define the Gogs and MySQL services.version: &#39;3.8&#39;services: mysql: image: mysql:5.7 container_name: gogs_mysql environment: MYSQL_ROOT_PASSWORD: your_root_password MYSQL_DATABASE: gogs MYSQL_USER: gogs MYSQL_PASSWORD: your_gogs_password volumes: - ./mysql/data:/var/lib/mysql networks: - gogs_network gogs: image: gogs/gogs container_name: gogs depends_on: - mysql ports: - &quot;3000:3000&quot; # HTTP port - &quot;2222:22&quot; # SSH port volumes: - ./gogs/data:/data - ./gogs/custom/conf:/data/gogs/conf environment: - DB_TYPE=mysql - DB_HOST=mysql:3306 - DB_NAME=gogs - DB_USER=gogs - DB_PASSWD=your_gogs_password networks: - gogs_networknetworks: gogs_network: driver: bridge4. Configure Gogs Start the containers: docker-compose up -d Access Gogs via http://your-server-ip:3000 and complete the initial setup. During setup: Set Database Type to MySQL. Use the following database details: Host: mysql:3306 User: gogs Password: your_gogs_password Database Name: gogs Configure the Application URL (e.g., http://your-domain.com:3000). Set up the SSH Server Port to 2222. 5. Enable SSH AccessTo make Gogs accessible via SSH: Configure SSH in Gogs: Go to Admin Panel &amp;gt; Configuration &amp;gt; SSH. Set SSH Port to 2222. Set Domain to your server’s domain or IP address. Update Docker Compose for SSH: Ensure the gogs service in docker-compose.yml maps port 2222 to the host. Test SSH Access: Clone a repository using SSH: git clone ssh://git@your-domain.com:2222/username/repo.git 6. Secure Your Setup Use HTTPS: Use a reverse proxy (e.g., Nginx) to serve Gogs over HTTPS. Obtain an SSL certificate using Let’s Encrypt. Firewall Rules: Allow only necessary ports (e.g., 3000 for HTTP, 2222 for SSH, 443 for HTTPS). Block all other ports. Backup Data: Regularly back up the gogs/data and mysql/data directories. Update Regularly: Keep Docker images and containers updated to the latest versions. 7. Example Nginx Reverse Proxy ConfigurationIf you want to use HTTPS, set up Nginx as a reverse proxy: Install Nginx and Certbot: sudo apt install nginx certbot python3-certbot-nginx Create an Nginx configuration file for Gogs: server { listen 80; server_name your-domain.com; location / { proxy_pass http://localhost:3000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; }} Obtain an SSL certificate: sudo certbot --nginx -d your-domain.com Reload Nginx: sudo systemctl reload nginx 8. Backup and Restore Backup: Backup the gogs/data and mysql/data directories: tar -czvf gogs_backup.tar.gz ./gogs/data ./mysql/data Restore: Extract the backup and restart the containers: tar -xzvf gogs_backup.tar.gzdocker-compose up -d 9. Monitoring and Logs View Logs: Check Gogs logs: docker logs gogs Check MySQL logs: docker logs gogs_mysql Monitor Performance: Use tools like Prometheus and Grafana to monitor Gogs and MySQL performance. 10. Troubleshooting Port Conflicts: Ensure ports 3000 and 2222 are not in use. Database Issues: Check MySQL logs for errors. Verify database credentials in the Gogs configuration. SSH Issues: Ensure the SSH port is open in your firewall. Verify the SSH configuration in Gogs. ConclusionBy following these best practices, you can run Gogs with MySQL in Docker and make it accessible via SSH securely and efficiently. Let me know if you need further assistance! 🚀" }, { "title": "Securing API requests from the client side", "url": "/posts/show-secure-client-side-request-api/", "categories": "DEV", "tags": "api, security, client-side", "date": "2025-01-20 19:35:00 -0500", "snippet": "Securing API requests from the client side is crucial to protect sensitive data, prevent unauthorized access, and ensure the integrity of your application. Below are best practices and strategies to secure API requests from the client side:1. Use HTTPS Why: Encrypts data in transit between the client and server, preventing eavesdropping and man-in-the-middle attacks. How: Ensure your API endpoints are served over HTTPS (not HTTP). Use TLS/SSL certificates from a trusted Certificate Authority (CA). Redirect all HTTP traffic to HTTPS. 2. Authentication and Authorization Why: Ensures only authorized users and applications can access your API. How: API Keys: Issue unique API keys to clients. Include the API key in the request headers. Example: GET /api/data HTTP/1.1Host: example.comAuthorization: Api-Key YOUR_API_KEY OAuth 2.0: Use OAuth 2.0 for token-based authentication. Clients obtain an access token from an authorization server and include it in the request headers. Example: GET /api/data HTTP/1.1Host: example.comAuthorization: Bearer YOUR_ACCESS_TOKEN JWT (JSON Web Tokens): Use JWTs for stateless authentication. Include the JWT in the Authorization header. Example: GET /api/data HTTP/1.1Host: example.comAuthorization: Bearer YOUR_JWT_TOKEN 3. Validate and Sanitize Input Why: Prevents injection attacks (e.g., SQL injection, XSS). How: Validate all input data on the client side before sending it to the API. Sanitize input to remove malicious content. Use libraries like validator.js (for JavaScript) or built-in validation frameworks. 4. Use Secure Headers Why: Protects against common vulnerabilities like XSS, clickjacking, and MIME sniffing. How: Set secure HTTP headers in your API responses: Content-Security-Policy (CSP): Restricts sources of content (e.g., scripts, styles). X-Content-Type-Options: Prevents MIME sniffing. X-Frame-Options: Protects against clickjacking. Strict-Transport-Security (HSTS): Enforces HTTPS. Example: HTTP/1.1 200 OKContent-Security-Policy: default-src &#39;self&#39;X-Content-Type-Options: nosniffX-Frame-Options: DENYStrict-Transport-Security: max-age=31536000; includeSubDomains 5. Rate Limiting and Throttling Why: Prevents abuse and protects against DDoS attacks. How: Implement rate limiting on the server side to restrict the number of requests a client can make within a specific time frame. Use tools like Redis or middleware like Express Rate Limit (for Node.js). 6. CORS (Cross-Origin Resource Sharing) Why: Prevents unauthorized domains from accessing your API. How: Configure CORS to allow only trusted domains to access your API. Example (Node.js with Express): const cors = require(&#39;cors&#39;);app.use(cors({ origin: &#39;https://trusted-domain.com&#39;, methods: [&#39;GET&#39;, &#39;POST&#39;], allowedHeaders: [&#39;Content-Type&#39;, &#39;Authorization&#39;]})); 7. Encrypt Sensitive Data Why: Protects sensitive data even if intercepted. How: Encrypt sensitive data (e.g., passwords, tokens) before sending it to the API. Use libraries like crypto-js (for JavaScript) or built-in encryption tools. 8. Use CSRF Tokens Why: Protects against Cross-Site Request Forgery (CSRF) attacks. How: Generate a CSRF token on the server and include it in the client-side form or request. Validate the token on the server for each request. Example: &amp;lt;form action=&quot;/api/data&quot; method=&quot;POST&quot;&amp;gt; &amp;lt;input type=&quot;hidden&quot; name=&quot;_csrf&quot; value=&quot;YOUR_CSRF_TOKEN&quot;&amp;gt; &amp;lt;!-- Other form fields --&amp;gt;&amp;lt;/form&amp;gt; 9. Secure Storage of Credentials Why: Prevents unauthorized access to sensitive data stored on the client side. How: Avoid storing sensitive data (e.g., API keys, tokens) in localStorage or sessionStorage. Use HTTP-only cookies for storing tokens securely. Example: document.cookie = &quot;token=YOUR_TOKEN; HttpOnly; Secure; SameSite=Strict&quot;; 10. Monitor and Log API Requests Why: Helps detect and respond to suspicious activity. How: Log all API requests (e.g., IP address, timestamp, endpoint). Use monitoring tools like Prometheus, Grafana, or ELK Stack. Set up alerts for unusual activity (e.g., high request volume, failed login attempts). 11. Use API Gateways Why: Centralizes security, monitoring, and rate limiting for multiple APIs. How: Deploy an API gateway (e.g., Kong, AWS API Gateway, Apigee). Configure security policies (e.g., authentication, rate limiting) at the gateway level. 12. Regularly Update Dependencies Why: Prevents vulnerabilities in third-party libraries. How: Use tools like npm audit or Dependabot to identify and update vulnerable dependencies. 13. Implement Content Security Policy (CSP) Why: Prevents unauthorized scripts from running on the client side. How: Define a CSP in your HTTP headers to restrict sources of scripts, styles, and other resources. Example: Content-Security-Policy: default-src &#39;self&#39;; script-src &#39;self&#39; https://trusted-cdn.com; 14. Use Secure Coding Practices Why: Reduces the risk of vulnerabilities in your client-side code. How: Avoid using eval() or innerHTML with untrusted data. Use parameterized queries to prevent SQL injection. Sanitize and escape user input. 15. Test for Vulnerabilities Why: Identifies and fixes security issues before they are exploited. How: Use tools like OWASP ZAP, Burp Suite, or Postman to test your API for vulnerabilities. Perform regular security audits and penetration testing. Example: Securing an API Request in JavaScriptHere’s an example of a secure API request using HTTPS, JWT, and secure headers:fetch(&#39;https://api.example.com/data&#39;, { method: &#39;GET&#39;, headers: { &#39;Authorization&#39;: &#39;Bearer YOUR_JWT_TOKEN&#39;, &#39;Content-Type&#39;: &#39;application/json&#39; }, credentials: &#39;include&#39; // Include cookies for authentication}).then(response =&amp;gt; response.json()).then(data =&amp;gt; console.log(data)).catch(error =&amp;gt; console.error(&#39;Error:&#39;, error));ConclusionSecuring API requests from the client side involves a combination of encryption, authentication, input validation, and secure coding practices. By following these best practices, you can protect your API from common threats and ensure the security of your application. Let me know if you need further clarification or examples! 🚀" }, { "title": "Social media supporting SCIM protocl", "url": "/posts/social-media-scim/", "categories": "Marketing", "tags": "scim, iam, social-media", "date": "2025-01-15 19:35:00 -0500", "snippet": "Not all social media platforms and marketing tools support the SCIM (System for Cross-domain Identity Management) protocol. SCIM is primarily used for automated user provisioning and deprovisioning, and its adoption varies across platforms. Below is a breakdown of which platforms support SCIM and which do not:Platforms That Support SCIM1. Facebook (Meta Business Suite) SCIM Support: Yes (for enterprise accounts). How It Works: SCIM allows automated provisioning and deprovisioning of users in Facebook Business Manager. Requires approval or partnership with Meta for SCIM integration. 2. LinkedIn SCIM Support: Yes (for LinkedIn Campaign Manager and Talent Solutions). How It Works: SCIM enables automated user management for LinkedIn enterprise accounts. Supported by major IdPs like Okta, Azure AD, and OneLogin. 3. Google Workspace (YouTube) SCIM Support: Yes. How It Works: SCIM is supported for Google Workspace, which includes YouTube access. Automates user provisioning and deprovisioning across Google services. 4. Hootsuite SCIM Support: Yes (for enterprise accounts). How It Works: SCIM integration allows automated user management in Hootsuite. Supported by IdPs like Okta, Azure AD, and OneLogin. 5. Sprout Social SCIM Support: Yes (for enterprise accounts). How It Works: SCIM enables automated user provisioning and deprovisioning in Sprout Social. Supported by IdPs like Okta, Azure AD, and OneLogin. Platforms That Do Not Support SCIM1. Twitter (X) SCIM Support: No. Alternative: User management is manual or via API (if available for enterprise accounts). 2. Instagram (Meta Business Suite) SCIM Support: No (managed through Facebook Business Manager). Alternative: Use Facebook Business Manager’s SCIM integration for Instagram accounts. 3. Pinterest SCIM Support: No. Alternative: User management is manual or via API (if available for enterprise accounts). 4. TikTok SCIM Support: No. Alternative: User management is manual or via API (if available for enterprise accounts). 5. Snapchat SCIM Support: No. Alternative: User management is manual or via API (if available for enterprise accounts). 6. Reddit SCIM Support: No. Alternative: User management is manual or via API (if available for enterprise accounts). Summary of SCIM Support Platform SCIM Support Notes Facebook (Meta) Yes Requires approval for SCIM integration. LinkedIn Yes Supported for enterprise accounts. Google Workspace Yes Includes YouTube access. Hootsuite Yes Supported for enterprise accounts. Sprout Social Yes Supported for enterprise accounts. Twitter (X) No Manual or API-based user management. Instagram (Meta) No Managed via Facebook Business Manager. Pinterest No Manual or API-based user management. TikTok No Manual or API-based user management. Snapchat No Manual or API-based user management. Reddit No Manual or API-based user management. Alternatives to SCIMFor platforms that do not support SCIM, you can use the following alternatives: Manual User Management: Add, update, or remove users manually in the platform’s admin console. API-Based Automation: Use the platform’s API (if available) to automate user provisioning and deprovisioning. Example: Twitter and Pinterest offer APIs for enterprise accounts. Custom Scripts: Write custom scripts to interact with the platform’s API for user management. ConclusionWhile SCIM is a powerful protocol for automating user provisioning and deprovisioning, not all social media platforms support it. Platforms like Facebook (Meta), LinkedIn, Google Workspace, Hootsuite, and Sprout Social do support SCIM, but others like Twitter, Instagram, Pinterest, TikTok, Snapchat, and Reddit do not. For platforms without SCIM support, you can use manual management or API-based automation as alternatives. Let me know if you need further assistance! 🚀" }, { "title": "Strategy Pattern example", "url": "/posts/strategy-pattern/", "categories": "Architecture", "tags": "strategy-pattern", "date": "2025-01-09 19:35:00 -0500", "snippet": "The problem you’re describing—calculating a result based on multiple input conditions where each combination requires a different algorithm—can be effectively addressed using the Strategy Pattern. This design pattern allows you to define a family of algorithms, encapsulate each one, and make them interchangeable. The Strategy Pattern lets the algorithm vary independently from the clients that use it.Why Use the Strategy Pattern? Flexibility: You can easily add, remove, or modify algorithms without changing the client code. Separation of Concerns: Each algorithm is encapsulated in its own class, making the code easier to maintain and test. Avoids Conditional Logic: Instead of using complex if-else or switch statements, you can dynamically select the appropriate algorithm at runtime.How to Implement the Strategy PatternStep 1: Define the Strategy InterfaceCreate an interface or abstract class that defines the contract for all algorithms.from abc import ABC, abstractmethodclass CalculationStrategy(ABC): @abstractmethod def calculate(self, input_data): passStep 2: Implement Concrete StrategiesCreate concrete classes that implement the CalculationStrategy interface. Each class represents a specific algorithm.class StrategyA(CalculationStrategy): def calculate(self, input_data): # Algorithm for condition A return input_data * 2class StrategyB(CalculationStrategy): def calculate(self, input_data): # Algorithm for condition B return input_data + 10class StrategyC(CalculationStrategy): def calculate(self, input_data): # Algorithm for condition C return input_data / 2Step 3: Create a Context ClassThe context class maintains a reference to a CalculationStrategy and delegates the calculation to the selected strategy.class CalculatorContext: def __init__(self, strategy: CalculationStrategy): self._strategy = strategy def set_strategy(self, strategy: CalculationStrategy): self._strategy = strategy def execute_calculation(self, input_data): return self._strategy.calculate(input_data)Step 4: Use the Context and StrategiesDynamically select and use the appropriate strategy based on input conditions.# Example input datainput_data = 5# Determine the condition (e.g., based on input or external logic)condition = &quot;A&quot; # This could be determined dynamically# Select the appropriate strategyif condition == &quot;A&quot;: strategy = StrategyA()elif condition == &quot;B&quot;: strategy = StrategyB()elif condition == &quot;C&quot;: strategy = StrategyC()else: raise ValueError(&quot;Invalid condition&quot;)# Create the context and execute the calculationcontext = CalculatorContext(strategy)result = context.execute_calculation(input_data)print(f&quot;Result: {result}&quot;)Example OutputFor input_data = 5: If condition = &quot;A&quot;, the output will be 10 (5 * 2). If condition = &quot;B&quot;, the output will be 15 (5 + 10). If condition = &quot;C&quot;, the output will be 2.5 (5 / 2).Advantages of the Strategy Pattern Extensibility: Adding a new algorithm is as simple as creating a new strategy class. Testability: Each strategy can be tested independently. Clean Code: Avoids long, complex conditional statements.Alternative: Factory PatternIf the selection of the strategy is complex or involves additional logic, you can combine the Strategy Pattern with the Factory Pattern to encapsulate the creation of strategies.class StrategyFactory: @staticmethod def get_strategy(condition): if condition == &quot;A&quot;: return StrategyA() elif condition == &quot;B&quot;: return StrategyB() elif condition == &quot;C&quot;: return StrategyC() else: raise ValueError(&quot;Invalid condition&quot;)# Usagestrategy = StrategyFactory.get_strategy(condition)context = CalculatorContext(strategy)result = context.execute_calculation(input_data)When to Use the Strategy Pattern When you have multiple algorithms for the same task and need to switch between them at runtime. When you want to avoid hardcoding conditional logic in your application. When you want to make your code more modular and maintainable.By using the Strategy Pattern, you can elegantly solve your problem of calculating results based on different input conditions. Let me know if you need further clarification or additional examples! 🚀" }, { "title": "GDQ solution", "url": "/posts/gdq-architecture/", "categories": "Architecture, DEV", "tags": "api, security, modularity, soa", "date": "2025-01-08 19:35:00 -0500", "snippet": "To design a modular Spring Boot application where external applications can selectively use specific modules (task jobs) from your application, you can follow a microservices-based architecture or a plugin-based architecture. Below is a detailed explanation of how to achieve this elegantly:Key Requirements Modularity: Each module (task job) should be independent and reusable. Selective Usage: External applications should be able to choose which modules they need. Scalability: The solution should be scalable and easy to maintain. Spring Boot: Use Spring Boot as the underlying technology.Solution 1: Microservices ArchitectureIn this approach, each module is a separate Spring Boot microservice. External applications can call only the required modules via APIs.Steps to Implement: Create Separate Microservices: Each module (task job) is a standalone Spring Boot application. Example: TaskJob1-Service TaskJob2-Service TaskJob3-Service Expose REST APIs: Each microservice exposes REST endpoints for its functionality. Example: @RestControllerpublic class TaskJob1Controller { @GetMapping(&quot;/taskjob1&quot;) public String executeTaskJob1() { return &quot;Task Job 1 executed!&quot;; }} Service Discovery (Optional): Use a service discovery tool like Eureka or Consul to dynamically locate services. API Gateway: Use an API Gateway (e.g., Spring Cloud Gateway) to route requests to the appropriate microservice. External Application Integration: External applications call the required microservices via their REST APIs. Example: curl http://localhost:8081/taskjob1 Pros: High modularity and independence. Easy to scale individual modules. Clear separation of concerns.Cons: Increased complexity due to distributed system management. Overhead of managing multiple services.Solution 2: Plugin-Based ArchitectureIn this approach, your application is a single Spring Boot application, and modules are dynamically loaded as plugins. External applications can choose which plugins to activate.Steps to Implement: Define a Module Interface: Create an interface that all modules must implement. Example: public interface TaskJob { void execute();} Implement Modules: Each module implements the TaskJob interface. Example: @Componentpublic class TaskJob1 implements TaskJob { @Override public void execute() { System.out.println(&quot;Task Job 1 executed!&quot;); }} Dynamic Module Loading: Use Spring’s @Conditional or custom logic to load modules based on configuration. Example: @Configurationpublic class TaskJobConfig { @Bean @ConditionalOnProperty(name = &quot;taskjob1.enabled&quot;, havingValue = &quot;true&quot;) public TaskJob taskJob1() { return new TaskJob1(); }} Configuration: Use application.properties or application.yml to enable/disable modules. Example: taskjob1.enabled=truetaskjob2.enabled=false External Application Integration: External applications can configure which modules to enable by setting the appropriate properties. Pros: Single application, easier to manage. Modules can be dynamically enabled/disabled. Lightweight compared to microservices.Cons: Less modular than microservices. Requires careful design to avoid tight coupling.Solution 3: Modular Monolith with Classpath IsolationIn this approach, your application is a single Spring Boot application, but modules are isolated using separate classpaths or JAR files.Steps to Implement: Create Separate Modules: Each module is a separate Maven/Gradle project. Example: taskjob1-module taskjob2-module Package Modules as JARs: Build each module as a JAR file. Dynamic Classpath Loading: Use a custom classloader to load modules dynamically at runtime. Example: URLClassLoader classLoader = new URLClassLoader(new URL[]{new File(&quot;path/to/taskjob1.jar&quot;).toURI().toURL()});TaskJob taskJob = (TaskJob) classLoader.loadClass(&quot;com.example.TaskJob1&quot;).getDeclaredConstructor().newInstance();taskJob.execute(); Configuration: Use a configuration file or database to specify which modules to load. Pros: High flexibility in loading modules. Modules are isolated in their own classpaths.Cons: Complex classloader management. Potential for classpath conflicts.Recommended SolutionIf you prefer Spring Boot and want an elegant solution, I recommend the Plugin-Based Architecture (Solution 2). It strikes a good balance between modularity and simplicity. Here’s why: It leverages Spring Boot’s dependency injection and configuration management. Modules can be easily enabled/disabled via configuration. No need to manage multiple services or complex classloaders.Example Implementation (Plugin-Based Architecture)1. Define the Module Interface:public interface TaskJob { void execute();}2. Implement Modules:@Componentpublic class TaskJob1 implements TaskJob { @Override public void execute() { System.out.println(&quot;Task Job 1 executed!&quot;); }}@Componentpublic class TaskJob2 implements TaskJob { @Override public void execute() { System.out.println(&quot;Task Job 2 executed!&quot;); }}3. Configure Modules:# application.propertiestaskjob1.enabled=truetaskjob2.enabled=false4. Conditional Bean Loading:@Configurationpublic class TaskJobConfig { @Bean @ConditionalOnProperty(name = &quot;taskjob1.enabled&quot;, havingValue = &quot;true&quot;) public TaskJob taskJob1() { return new TaskJob1(); } @Bean @ConditionalOnProperty(name = &quot;taskjob2.enabled&quot;, havingValue = &quot;true&quot;) public TaskJob taskJob2() { return new TaskJob2(); }}5. Use Modules in Your Application:@Servicepublic class TaskJobService { private final List&amp;lt;TaskJob&amp;gt; taskJobs; @Autowired public TaskJobService(List&amp;lt;TaskJob&amp;gt; taskJobs) { this.taskJobs = taskJobs; } public void executeAll() { taskJobs.forEach(TaskJob::execute); }}6. External Application Integration: External applications can enable/disable modules by modifying the application.properties file.ConclusionThe Plugin-Based Architecture is a clean and elegant solution for your requirements. It allows external applications to selectively use modules while keeping the application simple and maintainable. Let me know if you need further assistance!Yes, with the Plugin-Based Architecture, each external application can use an instance of your application with selected modules. Here’s how it works and why it meets your requirement:How It Works Single Application Instance: Your Spring Boot application is a single deployable unit (e.g., a JAR or WAR file). It contains all the modules (task jobs) but only activates the ones specified in the configuration. Configuration-Driven Module Activation: Each external application can specify which modules it needs by providing a configuration file (e.g., application.properties or application.yml). Example: # External App 1 Configurationtaskjob1.enabled=truetaskjob2.enabled=falsetaskjob3.enabled=true # External App 2 Configurationtaskjob1.enabled=falsetaskjob2.enabled=truetaskjob3.enabled=false Dynamic Module Loading: Based on the configuration, Spring Boot will only instantiate and activate the specified modules. Example: @Configurationpublic class TaskJobConfig { @Bean @ConditionalOnProperty(name = &quot;taskjob1.enabled&quot;, havingValue = &quot;true&quot;) public TaskJob taskJob1() { return new TaskJob1(); } @Bean @ConditionalOnProperty(name = &quot;taskjob2.enabled&quot;, havingValue = &quot;true&quot;) public TaskJob taskJob2() { return new TaskJob2(); }} Isolation Between Instances: Each external application runs its own instance of your application. The configuration file ensures that only the required modules are active in each instance. Example ScenarioLet’s say you have three external applications: App A, App B, and App C.App A: Needs TaskJob1 and TaskJob3. Configuration: taskjob1.enabled=truetaskjob2.enabled=falsetaskjob3.enabled=true App B: Needs TaskJob2. Configuration: taskjob1.enabled=falsetaskjob2.enabled=truetaskjob3.enabled=false App C: Needs all modules. Configuration: taskjob1.enabled=truetaskjob2.enabled=truetaskjob3.enabled=true Each application runs its own instance of your Spring Boot application, and only the specified modules are active in each instance.Benefits of This Approach Selective Module Activation: Each external application can choose which modules it needs. Unused modules are not loaded, reducing resource usage. Single Codebase: You maintain a single codebase for your application, making it easier to manage and update. Configuration Flexibility: External applications can easily customize their instance by modifying the configuration file. Isolation: Each instance of your application is independent, so there’s no risk of interference between external applications. Implementation DetailsHere’s how you can implement this in Spring Boot:1. Define the Module Interface:public interface TaskJob { void execute();}2. Implement Modules:@Componentpublic class TaskJob1 implements TaskJob { @Override public void execute() { System.out.println(&quot;Task Job 1 executed!&quot;); }}@Componentpublic class TaskJob2 implements TaskJob { @Override public void execute() { System.out.println(&quot;Task Job 2 executed!&quot;); }}@Componentpublic class TaskJob3 implements TaskJob { @Override public void execute() { System.out.println(&quot;Task Job 3 executed!&quot;); }}3. Configure Modules:# application.propertiestaskjob1.enabled=truetaskjob2.enabled=falsetaskjob3.enabled=true4. Conditional Bean Loading:@Configurationpublic class TaskJobConfig { @Bean @ConditionalOnProperty(name = &quot;taskjob1.enabled&quot;, havingValue = &quot;true&quot;) public TaskJob taskJob1() { return new TaskJob1(); } @Bean @ConditionalOnProperty(name = &quot;taskjob2.enabled&quot;, havingValue = &quot;true&quot;) public TaskJob taskJob2() { return new TaskJob2(); } @Bean @ConditionalOnProperty(name = &quot;taskjob3.enabled&quot;, havingValue = &quot;true&quot;) public TaskJob taskJob3() { return new TaskJob3(); }}5. Use Modules in Your Application:@Servicepublic class TaskJobService { private final List&amp;lt;TaskJob&amp;gt; taskJobs; @Autowired public TaskJobService(List&amp;lt;TaskJob&amp;gt; taskJobs) { this.taskJobs = taskJobs; } public void executeAll() { taskJobs.forEach(TaskJob::execute); }}6. External Application Integration: Each external application provides its own application.properties file to configure which modules to enable.Deployment Package your application as a JAR or WAR file. Each external application can deploy its own instance of your application with its specific configuration.ConclusionThe Plugin-Based Architecture ensures that each external application can use an instance of your application with selected modules. This approach is elegant, flexible, and easy to maintain. Let me know if you need further clarification or assistance! 🚀" }, { "title": "Social media and Marketing tools supporting SSO", "url": "/posts/sso-social-media-scim/", "categories": "Marketing", "tags": "sso, access, idPs", "date": "2025-01-07 19:35:00 -0500", "snippet": "Several social media platforms and marketing tools support Single Sign-On (SSO) for enterprise access, similar to Facebook (Meta Business Suite). Below is a list of popular platforms that offer SSO integration, along with details on how to set it up:1. LinkedInLinkedIn supports SSO for LinkedIn Campaign Manager and LinkedIn Talent Solutions.How to Set Up SSO for LinkedIn: Go to LinkedIn Account Settings. Navigate to Partner SSO. Provide your IdP’s metadata (e.g., SAML certificate, SSO URL). Map user attributes (e.g., email, first name, last name). Test the SSO integration.Supported IdPs: Okta Azure AD OneLogin Ping Identity2. Twitter (X)Twitter supports SSO for Twitter Ads and Twitter Analytics through its enterprise offerings.How to Set Up SSO for Twitter: Contact Twitter Business Support to enable SSO for your organization. Provide your IdP’s metadata (e.g., SAML certificate, SSO URL). Map user attributes (e.g., email, username). Test the SSO integration.Supported IdPs: Okta Azure AD OneLogin3. Instagram (via Meta Business Suite)Since Instagram is part of the Meta Business Suite, it can be accessed through the same SSO configuration as Facebook.How to Set Up SSO for Instagram: Follow the same steps as setting up SSO for Facebook Business Manager. Assign roles and permissions for Instagram accounts within Business Manager.Supported IdPs: Okta Azure AD OneLogin Ping Identity4. PinterestPinterest supports SSO for Pinterest Business Accounts.How to Set Up SSO for Pinterest: Go to Pinterest Business Settings. Navigate to Security &amp;gt; Single Sign-On. Provide your IdP’s metadata (e.g., SAML certificate, SSO URL). Map user attributes (e.g., email, username). Test the SSO integration.Supported IdPs: Okta Azure AD OneLogin5. YouTube (via Google Workspace)YouTube is part of Google Workspace, which supports SSO for enterprise accounts.How to Set Up SSO for YouTube: Configure SSO for Google Workspace in your IdP. Assign YouTube access to users via Google Workspace admin console. Users log in to YouTube using their enterprise credentials.Supported IdPs: Okta Azure AD OneLogin Ping Identity6. TikTokTikTok supports SSO for TikTok Ads Manager and TikTok for Business.How to Set Up SSO for TikTok: Contact TikTok Business Support to enable SSO for your organization. Provide your IdP’s metadata (e.g., SAML certificate, SSO URL). Map user attributes (e.g., email, username). Test the SSO integration.Supported IdPs: Okta Azure AD OneLogin7. SnapchatSnapchat supports SSO for Snapchat Ads Manager.How to Set Up SSO for Snapchat: Go to Snapchat Business Settings. Navigate to Security &amp;gt; Single Sign-On. Provide your IdP’s metadata (e.g., SAML certificate, SSO URL). Map user attributes (e.g., email, username). Test the SSO integration.Supported IdPs: Okta Azure AD OneLogin8. RedditReddit supports SSO for Reddit Ads through its enterprise offerings.How to Set Up SSO for Reddit: Contact Reddit Business Support to enable SSO for your organization. Provide your IdP’s metadata (e.g., SAML certificate, SSO URL). Map user attributes (e.g., email, username). Test the SSO integration.Supported IdPs: Okta Azure AD OneLogin9. HootsuiteHootsuite supports SSO for enterprise accounts, allowing teams to manage multiple social media platforms from a single dashboard.How to Set Up SSO for Hootsuite: Go to Hootsuite Enterprise Settings. Navigate to Security &amp;gt; Single Sign-On. Provide your IdP’s metadata (e.g., SAML certificate, SSO URL). Map user attributes (e.g., email, username). Test the SSO integration.Supported IdPs: Okta Azure AD OneLogin Ping Identity10. Sprout SocialSprout Social supports SSO for enterprise accounts, enabling teams to manage social media platforms securely.How to Set Up SSO for Sprout Social: Go to Sprout Social Settings. Navigate to Security &amp;gt; Single Sign-On. Provide your IdP’s metadata (e.g., SAML certificate, SSO URL). Map user attributes (e.g., email, username). Test the SSO integration.Supported IdPs: Okta Azure AD OneLogin Ping IdentitySummary of Platforms Supporting SSO| Platform | SSO Support | Supported IdPs ||———————–|———————–|—————————————-|| Facebook (Meta) | Yes | Okta, Azure AD, OneLogin, Ping Identity|| LinkedIn | Yes | Okta, Azure AD, OneLogin || Twitter (X) | Yes (Enterprise) | Okta, Azure AD, OneLogin || Instagram (Meta) | Yes | Okta, Azure AD, OneLogin, Ping Identity|| Pinterest | Yes | Okta, Azure AD, OneLogin || YouTube (Google) | Yes | Okta, Azure AD, OneLogin, Ping Identity|| TikTok | Yes | Okta, Azure AD, OneLogin || Snapchat | Yes | Okta, Azure AD, OneLogin || Reddit | Yes (Enterprise) | Okta, Azure AD, OneLogin || Hootsuite | Yes | Okta, Azure AD, OneLogin, Ping Identity|| Sprout Social | Yes | Okta, Azure AD, OneLogin, Ping Identity|ConclusionMany social media platforms and marketing tools support SSO for enterprise accounts, enabling secure and centralized access management. By integrating SSO with your organization’s Identity Provider (IdP), you can streamline access, enhance security, and improve compliance. Let me know if you need help setting up SSO for any specific platform! 🚀" }, { "title": "Manage Access to Facebook throw SSO and protocl SCIM", "url": "/posts/facebook-access-scim-sso/", "categories": "Marketing", "tags": "sso, access, idPs, scim, security", "date": "2025-01-06 19:35:00 -0500", "snippet": "Yes, it is possible to access Facebook (and other Meta platforms like Instagram and WhatsApp) through enterprise Single Sign-On (SSO) configuration and the SCIM (System for Cross-domain Identity Management) protocol. This allows enterprises to manage user access to Facebook Business Manager and other Meta tools securely and efficiently.How to Configure SSO and SCIM for Facebook (Meta Business Suite)1. Prerequisites A Facebook Business Manager account. An Identity Provider (IdP) that supports SAML 2.0 for SSO and SCIM (e.g., Okta, Azure AD, OneLogin, Ping Identity). Admin access to both Facebook Business Manager and your IdP.2. Steps to Configure SSO for FacebookStep 1: Enable SSO in Facebook Business Manager Log in to your Facebook Business Manager. Go to Business Settings &amp;gt; Security Center &amp;gt; Single Sign-On. Click Set Up under Single Sign-On.Step 2: Configure SSO in Your Identity Provider In your IdP (e.g., Okta, Azure AD), create a new SAML application for Facebook. Provide the following information: Single Sign-On URL (ACS URL): Provided by Facebook during setup. Audience URI (Entity ID): Provided by Facebook during setup. Name ID Format: Email or Persistent. Attributes: Map the required attributes (e.g., email, first name, last name). Step 3: Download Metadata and Upload to Facebook Download the SAML metadata file from your IdP. Upload the metadata file to Facebook Business Manager in the SSO setup section.Step 4: Test SSO Save the configuration and test SSO by logging in through your IdP. Ensure users can access Facebook Business Manager using their enterprise credentials.3. Steps to Configure SCIM for FacebookSCIM allows you to automate user provisioning and deprovisioning in Facebook Business Manager.Step 1: Enable SCIM in Facebook Business Manager Go to Business Settings &amp;gt; Security Center &amp;gt; Single Sign-On. Enable SCIM Provisioning (if available). Note: Facebook’s SCIM integration may require approval or partnership with Meta.Step 2: Configure SCIM in Your Identity Provider In your IdP, configure the SCIM integration for Facebook. Provide the following information: SCIM Endpoint URL: Provided by Facebook. Authentication Token: Provided by Facebook. User Attributes: Map attributes like email, first name, last name, and role. Step 3: Test SCIM Create a test user in your IdP and ensure it is provisioned in Facebook Business Manager. Update or deactivate the user in your IdP and verify the changes are reflected in Facebook.4. Benefits of SSO and SCIM for Facebook Centralized Access Management: Users can log in to Facebook Business Manager using their enterprise credentials. Automated User Provisioning: SCIM ensures users are added, updated, or removed automatically based on their status in the IdP. Enhanced Security: SSO reduces the risk of password-related breaches. Improved Compliance: Centralized control ensures compliance with internal policies and regulations. 5. Supported Identity ProvidersFacebook supports SSO and SCIM integration with major IdPs, including: Okta Azure Active Directory OneLogin Ping Identity Google Workspace6. Limitations and Considerations SCIM Availability: SCIM integration may not be available for all Facebook Business Manager accounts. Contact Meta support for access. Custom IdPs: If you use a custom or less common IdP, ensure it supports SAML 2.0 and SCIM. User Roles: Define user roles and permissions in Facebook Business Manager to align with your IdP configurations.7. Example Workflow User Onboarding: A new employee is added to the IdP. The IdP provisions the user in Facebook Business Manager via SCIM. The user logs in to Facebook using SSO. User Offboarding: An employee leaves the company and is deactivated in the IdP. The IdP deprovisions the user in Facebook Business Manager via SCIM. The user loses access to Facebook. ConclusionBy configuring SSO and SCIM for Facebook Business Manager, enterprises can streamline access management, enhance security, and automate user provisioning. This is especially useful for large organizations with multiple users and strict compliance requirements. Let me know if you need further assistance! 🚀" }, { "title": "Facebook access management in large enterprise with multiple accounts", "url": "/posts/facebookl-manage-access/", "categories": "Marketing", "tags": "api, security, client-side, access", "date": "2025-01-03 19:35:00 -0500", "snippet": "Yes, if an organization has multiple Facebook Pages (or other Meta assets like Instagram accounts or ad accounts), employees can access them through SSO (Single Sign-On). However, there are some important considerations and steps to ensure that employees can access the correct Pages and assets after logging in via SSO.How Employees Access Multiple Pages via SSO1. Centralized Access Through Facebook Business Manager Facebook Business Manager acts as the central hub for managing multiple Pages, ad accounts, and other assets. Employees log in to Business Manager using SSO, and their access to specific Pages is determined by their assigned roles and permissions.2. Assigning Roles and Permissions Admin Access: Assign employees as Admins or Employees in Business Manager. Page Roles: Assign specific roles (e.g., Admin, Editor, Moderator) to employees for each Page they need to access. Asset-Level Permissions: Ensure employees have the appropriate permissions for the Pages, ad accounts, or other assets they need to work on.3. Accessing Pages After SSO Login Once an employee logs in to Business Manager via SSO, they can: Switch between Pages they have access to. Perform tasks based on their assigned roles (e.g., create posts, manage ads, respond to messages). Key Considerations1. SSO Authentication Employees log in to Business Manager using their enterprise credentials (via SSO). SSO ensures secure and centralized authentication, but it does not directly control access to individual Pages. That is managed within Business Manager.2. Role-Based Access Control (RBAC) Use RBAC to define what employees can do within Business Manager and on specific Pages. For example: Admins can manage all settings and assign roles. Editors can create and publish content. Analysts can only view insights and reports. 3. Provisioning and Deprovisioning Use SCIM (if available) to automate user provisioning and deprovisioning in Business Manager. When an employee is added to the IdP, they are automatically granted access to Business Manager and the relevant Pages. When an employee leaves, their access is automatically revoked.4. Multiple Pages and Assets If the organization manages multiple Pages, ensure that employees are assigned to the correct Pages in Business Manager. Employees will only see and access the Pages they have been granted permissions for.Example Workflow1. Employee Onboarding A new employee is added to the organization’s IdP (e.g., Okta, Azure AD). The IdP provisions the employee in Facebook Business Manager via SCIM. The employee is assigned roles for specific Pages (e.g., Editor for Page A, Analyst for Page B). The employee logs in to Business Manager via SSO and accesses the assigned Pages.2. Employee Offboarding The employee is deactivated in the IdP. The IdP deprovisions the employee in Business Manager via SCIM. The employee loses access to all Pages and assets.Benefits of Using SSO for Multiple Pages Centralized Authentication: Employees use a single set of credentials (via SSO) to access Business Manager and all assigned Pages. Enhanced Security: SSO reduces the risk of password-related breaches. Efficient Access Management: Admins can easily assign and revoke access to Pages and assets. Compliance: Centralized control ensures compliance with internal policies and regulations. Limitations SSO Only for Business Manager: SSO is used to log in to Business Manager, not directly to individual Pages. Employees must still navigate to the correct Page within Business Manager. Role Management: Admins must manually or automatically assign roles for each Page, which can be time-consuming for organizations with many Pages and employees. SCIM Availability: SCIM integration may not be available for all Business Manager accounts. Check with Meta for availability. Best Practices Use Business Manager: Always manage multiple Pages and assets through Business Manager for centralized control. Leverage SCIM: Automate user provisioning and deprovisioning to save time and reduce errors. Regularly Review Permissions: Periodically audit user roles and permissions to ensure they align with current responsibilities. Train Employees: Provide training on how to use Business Manager and switch between Pages after logging in via SSO. ConclusionYes, employees can access multiple Facebook Pages through SSO by logging in to Facebook Business Manager. However, access to individual Pages is controlled by their assigned roles and permissions within Business Manager. By combining SSO with proper role management and SCIM (if available), organizations can ensure secure and efficient access to multiple Pages and assets. Let me know if you need further clarification! 🚀" }, { "title": "Manage access to social media accounts in large enterprise", "url": "/posts/social-media-access-corpo/", "categories": "Marketing, Architecture", "tags": "api, security, client-side, access", "date": "2025-01-01 19:35:00 -0500", "snippet": "Managing access to multiple social media accounts in a large enterprise requires a combination of tools, policies, and best practices to ensure security, efficiency, and consistency. Below is a comprehensive guide to help you manage this effectively:1. Use a Social Media Management ToolA Social Media Management (SMM) tool is essential for managing multiple accounts across platforms. These tools provide centralized control, scheduling, analytics, and collaboration features.Top Tools for Enterprises: Hootsuite: Manage multiple accounts (e.g., Facebook, Twitter, LinkedIn, Instagram). Schedule posts, monitor engagement, and generate reports. Team collaboration with role-based access. Sprout Social: Unified inbox for managing messages and comments. Advanced analytics and reporting. Collaboration tools for teams. Buffer: Simple scheduling and publishing. Analytics to track performance. Team collaboration features. HubSpot: Integrates with CRM for personalized social media campaigns. Advanced analytics and automation. Team collaboration and approval workflows. Agorapulse: Social inbox for managing interactions. Scheduling and reporting. Role-based access for teams. 2. Implement Role-Based Access Control (RBAC)To ensure security and accountability, assign roles and permissions to team members based on their responsibilities.Common Roles: Admin: Full access to all accounts and settings. Content Creator: Can create and schedule posts but cannot publish. Analyst: Can view analytics and reports but cannot post. Moderator: Can respond to comments and messages but cannot create posts.How to Implement RBAC: Use the SMM tool’s built-in role management features. Define clear roles and responsibilities in your social media policy.3. Create a Social Media PolicyA social media policy ensures consistency, security, and compliance across all accounts.Key Elements of a Social Media Policy: Account Ownership: Clearly define who owns each account and who has access. Password Management: Use a password manager (e.g., LastPass, 1Password) to securely store and share credentials. Content Guidelines: Define the tone, voice, and branding for all posts. Approval Workflow: Establish a process for reviewing and approving posts before publishing. Crisis Management: Outline steps to handle negative comments, PR crises, or account breaches. Compliance: Ensure adherence to legal and regulatory requirements (e.g., GDPR, FTC guidelines). 4. Centralize Account ManagementUse a single dashboard to manage all social media accounts. This simplifies monitoring, posting, and analytics.Steps to Centralize: Connect All Accounts: Link all social media accounts to your SMM tool. Unified Calendar: Use a shared content calendar to plan and schedule posts across platforms. Single Inbox: Monitor and respond to messages and comments from all accounts in one place. 5. Automate Routine TasksAutomation saves time and ensures consistency.Tasks to Automate: Post Scheduling: Schedule posts in advance using your SMM tool. Monitoring: Set up alerts for brand mentions, keywords, or hashtags. Reporting: Automate weekly or monthly performance reports.6. Monitor and Analyze PerformanceRegularly track the performance of your social media accounts to identify trends and optimize strategies.Key Metrics to Track: Engagement: Likes, comments, shares, and clicks. Reach: Number of people who saw your posts. Conversions: Leads or sales generated from social media. Sentiment: Positive, negative, or neutral feedback.Tools for Analytics: Native platform analytics (e.g., Facebook Insights, Twitter Analytics). Advanced analytics in your SMM tool (e.g., Hootsuite, Sprout Social).7. Train Your TeamEnsure your team is well-trained on the tools, policies, and best practices.Training Topics: How to use the SMM tool. Social media policy and compliance. Crisis management and escalation procedures. Content creation and branding guidelines.8. Secure Your AccountsProtect your social media accounts from unauthorized access and breaches.Security Best Practices: Use Strong Passwords: Create unique, complex passwords for each account. Enable Two-Factor Authentication (2FA): Add an extra layer of security to all accounts. Limit Access: Grant access only to team members who need it. Monitor Access Logs: Regularly review who has accessed the accounts and when. Use a Password Manager: Securely store and share credentials with your team. 9. Plan for ScalabilityAs your enterprise grows, ensure your social media management strategy can scale with it.Tips for Scalability: Use enterprise-grade SMM tools that support large teams and multiple accounts. Regularly review and update your social media policy. Invest in training and tools to streamline workflows.10. Example WorkflowHere’s how a typical workflow might look in a large enterprise: Content Creation: A content creator drafts posts and schedules them in the SMM tool. Approval: The post is reviewed and approved by a manager or compliance officer. Publishing: The approved post is published across all relevant platforms. Monitoring: A moderator monitors comments and messages, responding as needed. Reporting: An analyst generates performance reports and shares insights with the team. ConclusionManaging multiple social media accounts in a large enterprise requires a combination of tools, policies, and best practices. By using a Social Media Management tool, implementing role-based access control, and following a social media policy, you can ensure security, efficiency, and consistency across all accounts. Let me know if you need further assistance! 🚀" }, { "title": "WireGuard", "url": "/posts/wireguard/", "categories": "Privacy", "tags": "encryption, vpn", "date": "2023-03-12 13:30:00 -0400", "snippet": "This post will guide and help you how to install1 and use2 WireGuard.Install WireGuard VPN ServerPrerequisitesIn this example3: My IP: 207.148.66.172 Network Name: emp1 s0 WorGuard is listining on port: 33333 Range private IP: 192.168.10.0/24update the system$ apt update -y install utilyties (if not already installed) iptables: Iptables is an extremely flexible firewall utility built for Linux operating systems4, 5 net-tools: the collection of base networking ultilities fot linux vim: editor$ apt install iptables net-tools vim -yEnable IP Forwarding to route packets between vpn clients and internetMake these changes in the file below# uncomment line below# net.ipv4.ip_forward=1net.ipv4.ip_forward=1to validate changes$ sysctl -pnet.ipv4.ip_forward=1Check Network interface Name and IP of the system$ ifconfiginstall Wireguard vpn server on the system$ apt install wireguard -yConfigure WireGuard Server go to /etc/wireguard directory create private and public pair keys for wg server$ wg genkey | tee privatekey | wg publickey &amp;gt; publickeytwo files privatekey and publickey must be created3.3 show private, public keys and copy it3.4 create a new config file called wg0.conf3.5 open wg0.conf and add new contentReplace Rang IP, Listen Port, Private key, Netwrk Interface Name, with you info[Interface]## Address - A private IP address for wg0 interface. Address = 10.13.13.1## ListenPort - Specify the listening port of wireguard by default 51820, but you can change itListenPort = 51820## PrivateKey - A private key of the server (/etc/wireguard/privatekey)PrivateKey = your-private-key## PostUp - The postup will run when the WireGuard Server starts the virtual VPN tunnel.PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth+ -j MASQUERADE## PostDown - The postdown rules run when the WireGuard Server stops the virtual VPN tunnel.PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth+ -j MASQUERADE## Specify the command that allows traffinc to leave the server and give the VPN clients access to the internet.Change permission on privatekey and wg0.conf files$ chmod 600 privatekey$ chmod 600 wg0.conf# to check changes$ llstart and enable wg server$ systemctl start wg-quick@wg0$ systemctl enable wg-quick@wg0Make sure wg vpn server is running$ systemctl status wg-quick@wg0Verify interface named wg0 is up$ wgConfigure ufw firewall to open wg vpn serverCheck status of the ufw firewall$ ufw statusAllow port 33333/udp$ ufw allow 33333/udpReload ufw$ ufw reloaddockorize wireguardPrerequistes docker and docker compose installedRun docker composeversion: &quot;2.1&quot;services: wireguard: image: linuxserver/wireguard container_name: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - PUID=1000 - PGID=1000 - TZ=Etc/UTC - SERVERURL=auto #optional - SERVERPORT=51820 #optional - PEERS=5 #optional - PEERDNS=auto #optional - INTERNAL_SUBNET=10.13.13.0 #optional - ALLOWEDIPS=0.0.0.0/0 #optional - PERSISTENTKEEPALIVE_PEERS= #optional - LOG_CONFS=true #optional volumes: - ./config:/config - /lib/modules:/lib/modules #optional ports: - 51820:51820/udp sysctls: - net.ipv4.conf.all.src_valid_mark=1 restart: unless-stopped In this case we have 5 peers, if you want to change the number of peers, so you have to restart docker compose.Install WireGuard VPN as Client6Prerequisites for DeepinAdd the repo package to deepin$ echo &quot;deb http://deb.debian.org/debian/ buster-backports main &quot; | \\$ sudo tee /etc/apt/sources.list.d/buster-backports.list$ sudo apt update$ sudo apt install -t buster-backports wireguard-dkms wireguard-toolsIf you have this error, install openresolv as shown below. —– resolve error: /usr/bin/wg-quick: line 32: resolvconf: command not found - https://superuser.com/questions/1500691/usr-bin-wg-quick-line-31-resolvconf-command-not-found-wireguard-debian$ sudo apt install openresolvPrerequisites server installed Key Value ip server 207.148.66.172 wg port 33333 range ip vpn 192.168.10.0/24 public key on the server private key on the server Update system$ sudo apt update -yinstall wg clientinstall wg client on the system$ sudo apt install wireguard -yGo to /etc/wireguard/ as sudo$ sudo su -$ cd /etc/wireguard/create the public and the private keys to communicate with the server$ wg genkey | tee privatekey | wg pubkey | tee publickey Tow files are created publickey and privatekey inside /etc/wireguard/.Show private and public key and note themConfigure wireguard clientcreate config file called wg0.confedit wg0.confAdd client on wg serveradd client on wg serververify config filesave configure filestart connection and testing on client hoststart the connection$ wg-quick up wg0Verify$ ifconfig You should see wg0 interface.verify servier sideverify server side$ wg show wg0 You should see peerX.tempo content to delete[Interface]Address = 10.13.13.1ListenPort = 51820PrivateKey = your-private-keyPostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -A FORWARD -o %i -j ACCEPT; iptables -t nat -A POSTROUTING -o eth+ -j MASQUERADEPostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -D FORWARD -o %i -j ACCEPT; iptables -t nat -D POSTROUTING -o eth+ -j MASQUERADE[Peer]# peer1PublicKey = K8cS34Z+1Hy/fXxRdkFS/Cz+PxjrAvnPp3dEfspw6UA=PresharedKey = I62J+xVCyYksK9wL0uobAMva276yyJJf/kr2low3uwQ=AllowedIPs = 10.13.13.2/32UtilesTo view connected clients$ wgWith dockerYou have to execute commande inside docker$ docker exec -it containerName /bin/baash# inside docker container$ wgGet public ip address with cli$ curl ifconfig.me$ curl ipinfo.io/ip$ curl api.ipify.org$ curl checkip.dyndns.org$ curl ident.me$ curl ipecho.net/plainResources Wireguard installation &amp;#8617; Wireguard home page &amp;#8617; How to Install and Configure WireGuard VPN Server on Ubuntu (youtube) &amp;#8617; The Beginner’s Guide to iptables, the Linux Firewall &amp;#8617; Iptables Tutorial – Securing Ubuntu VPS with Linux Firewall &amp;#8617; Install and Configure WireGuard Client on Ubuntu &amp;#8617; " }, { "title": "GPG", "url": "/posts/gpg/", "categories": "Privacy", "tags": "encryption, decryption, signature, gpg", "date": "2023-02-18 19:35:00 -0500", "snippet": "This post will guide and help you how to use gpg. jmcPrerequistes gpg installed;if not installed$ apt update &amp;amp;&amp;amp; install -y gpgOtherweise if you have docker installed so you can play with it in debian container$ docker run -ti --rm --name debian debianin the container install gpg$ apt update &amp;amp;&amp;amp; apt install -y vim less tree gpgCLIGet gpg version$ gpg --versionOutput$ gpg (GnuPG) 2.2.27$ libgcrypt 1.8.8$ Copyright (C) 2021 Free Software Foundation, Inc.$ License GNU GPL-3.0-or-later &amp;lt;https://gnu.org/licenses/gpl.html&amp;gt;$ This is free software: you are ...$ ...Get public and secret keys listPublic keys:$ gpg -kPublic with key value:$ gpg --list-keys --keyid-format LONGSecret keys:$ gpg -KSecret with key value:$ gpg --list-secret-keys --keyid-format LONGCreate key$ gpg --full-generate-key$ gpg --full-gen-keyyou have to select one option (by default 1)you have to set keysize (between 1024 and 4096 bits)specify how long the key should be validspecify name and email …SIGNsign with sha$ sha1sum test.txt Sign with gpg encapslated in one file (signature + file)$ gpg --clearsign filename$ gpg --sign test.txt seperate the signature from the file$ gpg --detach-sign test.txt to verify the signature$ gpg --verify test.txt.gpg $ gpg --verify test.txt.sig ENCRYPT$ gpg -r &amp;lt;uidName&amp;gt; --armor --encrypt &amp;lt;fileName&amp;gt;$ gpg -r meddjerc --armor --encrypt mesTodo.txt$ gpg -e -a -r mohamedchamseddine.jerbi@proton.me file.txt $ gpg -e -a -r 0x32424D1AD356BE8E file.txt -e to encrypt -r specific key (jmc@mail.com or id form gpg -k command) -a in ascii and not byte formatDECRYPT$ gpg -d &amp;lt;fileName&amp;gt;$ gpg -d mesTodo.asc$ gpg -d -o file.txt file.txt.asc -o output fileMANAGE KEYS$ gpg --expert --edit-key meddjercAdd subkey$ gpg&amp;gt; addkeyTo change key’s validity, you have to select the key and tape expire:$ gpg -KEXPORT KEYSExporting a GPG private subkey and primary keyExporting a GPG private subkey and primary key requires extra care, as these are sensitive cryptographic materials that should be kept secure. The gpg command-line tool allows you to export private keys (including subkeys) in a format that can be imported elsewhere. Below are the steps to do this.Steps to Export GPG Private Subkey and Primary Key List Your KeysFirst, identify the key pair you want to export. Run: gpg --list-secret-keys Example output: sec rsa4096 2023-01-15 [SC] [expires: 2025-01-15] 1234567890ABCDEF1234567890ABCDEF12345678ssb rsa2048 2023-01-15 [E] FEDCBA9876543210FEDCBA9876543210FEDCBA98 sec is the primary secret key (e.g., 1234567890ABCDEF1234567890ABCDEF12345678). ssb is the secret subkey (e.g., FEDCBA9876543210FEDCBA9876543210FEDCBA98). Export the Private Primary Key and SubkeyTo export both the private primary key and its associated subkeys, use the --export-secret-keys option with the primary key ID: gpg --armor --export-secret-keys 1234567890ABCDEF1234567890ABCDEF12345678 &amp;gt; myprivatekey.asc --armor: Outputs the key in ASCII-armored format (readable text). --export-secret-keys: Exports the private key (including subkeys by default). &amp;gt; myprivatekey.asc: Saves the output to a file. If your key is passphrase-protected (which it should be), you’ll be prompted to enter the passphrase. Export Only the Private Subkey (Optional)If you want to export only the private subkey (without the primary key), use the subkey ID with an exclamation mark (!): gpg --armor --export-secret-keys FEDCBA9876543210FEDCBA9876543210FEDCBA98! &amp;gt; mysubkey_private.asc The ! ensures only the specified subkey is exported, not the primary key or other subkeys. Again, enter the passphrase if prompted. Verify the Exported FileCheck the file to ensure it was exported correctly: cat myprivatekey.asc It should start with: -----BEGIN PGP PRIVATE KEY BLOCK-----...-----END PGP PRIVATE KEY BLOCK----- Secure the Exported FilePrivate keys are sensitive! Protect the exported file: Store it in a secure location (e.g., encrypted drive). Optionally, encrypt it with a new passphrase using: gpg --armor --output myprivatekey.asc.gpg --symmetric myprivatekey.asc This creates an encrypted version (myprivatekey.asc.gpg) with a new passphrase. Important Notes Passphrase: If your key is protected (as it should be), you’ll need the passphrase to export or use it later. Subkey Inclusion: By default, --export-secret-keys with the primary key ID exports the primary key and all associated subkeys. Use the ! syntax to isolate a specific subkey. Backup: Exporting private keys is typically for backup or migration (e.g., moving to a new machine). Avoid unnecessary exports. Revocation: If the exported private key is compromised, ensure you have a revocation certificate ready to invalidate it.Example Workflow Export primary key + subkeys: gpg --armor --export-secret-keys 1234567890ABCDEF1234567890ABCDEF12345678 &amp;gt; myfullkey.asc Export only a subkey: gpg --armor --export-secret-keys FEDCBA9876543210FEDCBA9876543210FEDCBA98! &amp;gt; mysubkey.asc Exporting a GPG public subkey and primary keyTo export a GPG public subkey, you’ll need to use the gpg command-line tool. Below are the steps to do this. I’ll assume you have GPG installed and have already generated a key pair with subkeys. If not, let me know, and I can guide you through that as well!Steps to Export a GPG Public Subkey List Your KeysFirst, identify the key and subkey you want to export. Run the following command to list all keys in your keyring: gpg --list-keys This will display something like: pub rsa4096 2023-01-15 [SC] [expires: 2025-01-15] 1234567890ABCDEF1234567890ABCDEF12345678uid [ultimate] Your Name &amp;lt;your.email@example.com&amp;gt;sub rsa2048 2023-01-15 [E] pub is your primary key. sub is your subkey (e.g., for encryption [E] or signing [S]). Note the key ID (e.g., 1234567890ABCDEF1234567890ABCDEF12345678). Export the Public SubkeyTo export only the public subkey (not the primary key), use the --export option with the subkey ID. Subkeys are often referenced by appending an exclamation mark (!) to the key ID to ensure only the subkey is exported. However, you’ll need to identify the specific subkey ID first. Run this to list the keys with more detail, including subkey IDs: gpg --list-keys --with-subkey-fingerprints Output might look like: pub rsa4096 2023-01-15 [SC] [expires: 2025-01-15] 1234567890ABCDEF1234567890ABCDEF12345678uid [ultimate] Your Name &amp;lt;your.email@example.com&amp;gt;sub rsa2048 2023-01-15 [E] FEDCBA9876543210FEDCBA9876543210FEDCBA98 Here, FEDCBA9876543210FEDCBA9876543210FEDCBA98 is the subkey’s fingerprint. Now, export the subkey: gpg --armor --export FEDCBA9876543210FEDCBA9876543210FEDCBA98! &amp;gt; mysubkey.pub --armor: Outputs the key in ASCII-armored format (readable text). !: Ensures only the subkey is exported, not the primary key. &amp;gt; mysubkey.pub: Saves the output to a file (e.g., mysubkey.pub). Verify the Exported SubkeyCheck the exported file: cat mysubkey.pub It should look like a standard GPG public key block but only contain the subkey data. Optional: Export All SubkeysIf you want to export all subkeys along with the primary key, simply use the primary key ID without the !: gpg --armor --export 1234567890ABCDEF1234567890ABCDEF12345678 &amp;gt; mykey_with_subkeys.pub Notes Replace the key IDs with your actual key/subkey fingerprints. If you’re unsure which subkey to export (e.g., encryption vs. signing), the [E] or [S] in the output of --list-keys indicates its purpose. The exported file (mysubkey.pub) can be shared with others to allow them to use your subkey (e.g., for encryption).IMPORT KEYSTo import a GPG public subkey, you’ll use the gpg command-line tool to add it to your keyring. This assumes you have a file containing the public subkey (e.g., exported as described earlier). Below are the steps to import it.Steps to Import a GPG Public Subkey Obtain the Public Subkey FileEnsure you have the file containing the public subkey you want to import. For example, it might be named mysubkey.pub and contain an ASCII-armored public key block that looks like this: -----BEGIN PGP PUBLIC KEY BLOCK-----...-----END PGP PUBLIC KEY BLOCK----- Import the SubkeyUse the gpg --import command to add the subkey to your keyring: gpg --import mysubkey.pub Replace mysubkey.pub with the actual filename. Example output: gpg: key 1234567890ABCDEF12345678: public key &quot;Your Name &amp;lt;your.email@example.com&amp;gt;&quot; importedgpg: key FEDCBA9876543210FEDCBA98: subkey importedgpg: Total number processed: 1gpg: imported: 1 Verify the ImportCheck that the subkey has been added to your keyring by listing your keys: gpg --list-keys Or, for more detail including subkeys: gpg --list-keys --with-subkey-fingerprints Look for the subkey’s fingerprint (e.g., FEDCBA9876543210FEDCBA98) under the primary key it belongs to. The output might look like: pub rsa4096 2023-01-15 [SC] [expires: 2025-01-15] 1234567890ABCDEF1234567890ABCDEF12345678uid [ultimate] Your Name &amp;lt;your.email@example.com&amp;gt;sub rsa2048 2023-01-15 [E] FEDCBA9876543210FEDCBA9876543210FEDCBA98 Trust the Key (Optional)If this subkey belongs to someone else’s key and you want to use it (e.g., for encryption or verification), you may need to assign a trust level: gpg --edit-key 1234567890ABCDEF1234567890ABCDEF12345678 Inside the GPG interactive prompt: Type trust and follow the prompts to set a trust level (e.g., “marginal” or “full”). Type quit to exit. Notes The subkey will be associated with its primary key if the primary key is already in your keyring. If the primary key isn’t present, GPG will still import the subkey, but it may appear as an orphan until the primary key is imported. If the subkey file contains multiple keys or subkeys, gpg --import will process all of them. If you encounter an error (e.g., “not a key”), ensure the file is a valid GPG public key block.DELETE KEYS You have to delete the secret before the public key.$ gpg --delete-secret-keys mohamedchamseddine.jerbi@proton.me$ gpg --delete-secret-keys 0xCFB96EEE8C837DDA$ gpg --delete-keys mohamedchaseddine.jerbi@proton.me To exit from mutliline insert command, tape Ctrl+D. keys are saved in ~/.gnupg/Change passphrase$ gpg --passwd usename@mail.com you have to put your current passphrase and after the new one.REVOKE KEYgo to location where your located revoke file$ gpg --import revoke-file-nameEDIT KEY Unable to monitor directories for changes because iNotify max watches exceeded.Run these commands from the host (not in the docker container)$ gpg --edit-key mohamedchamseddine.jerbi@proton.me you have to select wich key you want to edit (key 0, key 1 …) to chage validity tape expire don’t forget to save before quit" }, { "title": "Firewall secure home network", "url": "/posts/firewall-home-network/", "categories": "Security, Network", "tags": "nextcloud, nginx-proxymanager, tuto", "date": "2023-02-16 20:35:00 -0500", "snippet": "CE N’est pas le bon contenu …This post will guide you how to install nextcloud.Je dois ajouter tous cce que je pourrai trouver …Prerequistes server; docker; nginx proxy manager; ajouter les liens des la documentation …version: &#39;3&#39;services: jekyll-serve: image: jekyll/jekyll:4.0 volumes: - &#39;.:/srv/jekyll&#39; ports: - 4000:4000 command: &#39;jekyll serve&#39;to run docker-compose$ docker-compose upErrors Unable to monitor directories for changes because iNotify max watches exceeded.Run these commands from the host (not in the docker container)$ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf$ sudo sysctl -pLearn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "Dockerize nextCloud", "url": "/posts/install-nextcloud/", "categories": "Privacy", "tags": "nextcloud, nginx-proxymanager, tuto", "date": "2023-02-16 19:55:00 -0500", "snippet": "This post will guide you how to install nextcloud.Je dois ajouter tous cce que je pourrai trouver …Prerequistes server; docker; nginx proxy manager; ajouter les liens des la documentation …version: &#39;3&#39;services: jekyll-serve: image: jekyll/jekyll:4.0 volumes: - &#39;.:/srv/jekyll&#39; ports: - 4000:4000 command: &#39;jekyll serve&#39;to run docker-compose$ docker-compose upErrors Unable to monitor directories for changes because iNotify max watches exceeded.Run these commands from the host (not in the docker container)$ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf$ sudo sysctl -p" }, { "title": "Degoogled your phone", "url": "/posts/install-grapheneos/", "categories": "Privacy, GrapheneOS", "tags": "degoogled, pixel, tuto", "date": "2023-02-16 19:40:00 -0500", "snippet": "This post will guide you how to install GrapheneOS.Prerequistes Google Pixel; GrapheneOSversion: &#39;3&#39;services: jekyll-serve: image: jekyll/jekyll:4.0 volumes: - &#39;.:/srv/jekyll&#39; ports: - 4000:4000 command: &#39;jekyll serve&#39;to run docker-compose$ docker-compose upErrors Unable to monitor directories for changes because iNotify max watches exceeded.Run these commands from the host (not in the docker container)$ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf$ sudo sysctl -pLearn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "my Utils in Deepin 20.x", "url": "/posts/utils-in-deepin/", "categories": "Linux", "tags": "deepin, utils, java, firefox, dbeaver", "date": "2022-05-21 10:25:45 -0400", "snippet": " Status in progressListe des outils que j’utilise souvent dans mon travail.I add somereferences temporary handel restapi error1, how to add UUID in DB2 and generator 3.TODO after installing Deepin What to do after installing Deepin OS 10 Things To Do After Installing Deepin 20.1 TOP 15 Things You MUST DO After Installing Deepin Linux 20.3java Status confirmed My first refFirefox Status confirmedUsed resource, in section - Install Firefox from Mozilla builds (For advanced users)link to download Info use sudo if you are not a root user Extract the contents of the downloaded file by typing:$ tar xjf firefox-*.tar.bz2Move the uncompressed Firefox folder to /opt:$ sudo mv firefox /optCreate a symlink to the Firefox executable:$ sudo ln -s /opt/firefox/firefox /usr/local/bin/firefoxDownload a copy of the desktop file$ sudo wget https://raw.githubusercontent.com/mozilla/sumo-kb/main/install-firefox-linux/firefox.desktop -P /usr/local/share/applicationsSome links to get more more secure and privacy … link1: https://www.youtube.com/watch?v=NH4DdXC0RFw link2: https://www.youtube.com/watch?v=tQhWdsFMc24 link3: https://restoreprivacy.com/firefox-privacy/ link4: https://support.mozilla.org/en-US/products/firefox/privacy-and-security link5: https://www.vpnmentor.com/blog/the-complete-guide-to-privacy-on-firefox-browser/ link6: https://www.lifewire.com/firefox-security-tips-and-tools-2487972 privacy setup FF default browser Enable DNS over HTTPS clean home page only DuckDuckGo Privacy and security 5.1 select Custom and disable all of them 5.2 send wesites … –&amp;gt; always 5.3 login and pwd –&amp;gt; i didn’t touch until i found a good password manager 5.4 history –&amp;gt; enable “Clear history when Firefox closes” in “Use custom settings for history” 5.5 Firefox Data Collection and Use –&amp;gt; disabled every thing install extensions 6.1 Privacy Badger 6.2 Https everywhere 6.3 FF multi-account containers synergy Status not confirmedfrom the downloader pkg, use packge installerdbever-CE Status confirmedFrom the downloader pkg, using packge installer. Link to downloadSTS Tool Status not confirmedfrom Deepin installer Entre STS er JAVA pour ne pas laisser les mdp dans application.properties voir le projet ses dans Run –&amp;gt; Run configuration –&amp;gt; (x)=Arguments add in VN arguments:-Dspring.mail.password=tomMDP mvn spring-boot:run -Dspring-boot.run.arguments=”toto –spring.mail.password=yzrtgrumdpnsgnab”to run jar file$ java -jar nameOfJarFileif error error - No compiler is provided in this environment. Perhaps you are running on a JRE rather than a JDK?, take a look to tihis link source to resolve error Run as –&amp;gt; Maven Test to run jar file with arg in this case i add the email psw: java -jar ses-0.0.1-SNAPSHOT.jar --spring.mail.password=yzrtgrumdpnsgnab in this case i add the email psw and the activ envirement: java -jar ses-0.0.1-SNAPSHOT.jar --spring.mail.password=yzrtgrumdpnsgnab spring.profiles.active=prod Reverse Footnote java - error handler in springboot restapi: https://www.youtube.com/channel/UCbA2jT41CAlZF_V1V-iSaCQ &amp;#8617; how to use UUID as id &amp;#8617; generator \\ (o-o) / &amp;#8617; " }, { "title": "crontab", "url": "/posts/crontab/", "categories": "Docker, Jekyll", "tags": "dev, script, tuto", "date": "2022-05-20 22:16:00 -0400", "snippet": " Statut non débuté.This post will guide you how to use crontab.Prerequistes Docker installed; docker-compose.yamlversion: &#39;3&#39;services: jekyll-serve: image: jekyll/jekyll:4.0 volumes: - &#39;.:/srv/jekyll&#39; ports: - 4000:4000 command: &#39;jekyll serve&#39;Errors Unable to monitor directories for changes because iNotify max watches exceeded.Run these commands from the host (not in the docker container)$ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf$ sudo sysctl -pLearn MoreFor more knowledge about Cron and crontab, visit the Le planificateur de taches!.For more knowledge about Cron job, watch this vedeo Cron job youtube.For more knowledge about Cron job, visit this link from Deepin blog Deepin scheduled task." }, { "title": "Dockerize MySql", "url": "/posts/dockerize-mysql/", "categories": "Docker, Mysql", "tags": "dev, docker, tuto", "date": "2022-05-20 21:20:00 -0400", "snippet": "This post will guide you how to dockerize MySql.Prerequistes Docker installed; docker-compose.yamlversion: &#39;3&#39;services: db: image: mysql:5.7 environment: - MYSQL_ROOT_PASSWORD=root-password - MYSQL_DATABASE=databasename - MYSQL_USER=user - MYSQL_PASSWORD=user-password# - TZ=Europe/Paris volumes: - ~/datastore/mysql/dbdev:/var/lib/mysql - /etc/timezone:/etc/timezone:ro - /etc/localtime:/etc/localtime:ro ports: - 2012:3600Backup and Restore DataBackup$ docker exec -it &amp;lt;IMAGE NAME&amp;gt; /bin/bash$ sudo docker exec -it db mysql -uroot -p....mysql&amp;gt; mysql&amp;gt; show databases;---mysql&amp;gt; show tables;---mysql&amp;gt; use tableName;---## to restore databasemysql&amp;gt; create database dbName;mysql&amp;gt; exit;$ mysqldump -u userName -p pwd --no-data dbName &amp;gt; backupFile.sql (without data)$ mysqldump -u userName -p pwd dbName &amp;gt; backupFile.sql$ mysqldump -u userName -p pwd --all-databases &amp;gt; backupFile.sql$ mysqldump -u root -p ******** --all-databases &amp;gt; backupFile.sqlRestore$ mysql -u userName -p pwd dbName &amp;lt; backupFile.sql Update users password, otherwise you won’t be able to connect from outside.Run these commands from the host (not in the docker container)$ SET PASSWORD FOR &#39;root&#39;@&#39;%&#39; = &#39;p@ssw0rd&#39;;$ SET PASSWORD FOR &#39;app_user&#39;@&#39;%&#39; = &#39;p@ssw0rd&#39;;Learn MoreFor more knowledge about time in docker, visit the 5 ways to change time in docker container." }, { "title": "ssh in linux", "url": "/posts/ssh-deepin/", "categories": "Security, ssh", "tags": "ssh, sshd", "date": "2022-04-17 18:02:00 -0400", "snippet": "This post will try to guide you how to troubleshooting ssh in Deepin1.Take a look to tihis video - OpenSSH Full Guide - Everything you need to get started!2.Connect to server via sshUse this command to log: ssh Username@192.168.2.26 where “Username” is the user name on machine 192.168.2.26. You may need to provide the password for this user during login.jmc@jmc-PC:~$ ssh 192.168.2.26ssh: connect to host 192.168.2.26 port 22: Connection refusedHere we have an error ssh: connect to host 192.168.2.26 port 22: Connection refused.check if SSH Service is down$ sudo service ssh statusYou can use the command sudo service ssh restart to try to get it runing again.check which port is being used for SSH$ grep Port /etc/ssh/sshd_configThe command line should return the correct port.check if the Port You’re Trying to Use Is Closed$ sudo lsof -i -n -P | grep LISTENThis command should return a list of ports with the LISTEN state. Ideally, you want to see port 22 or your server’s custom SSH port listed here. If it’s not, you’ll need to reopen the port in order to connect to your server.SSH Isn’t Installed on Your ServerFirewall Settings Are Preventing an SSH Connection$ sudo iptables-save # display IPv4 rules$ sudo ip6tables-save # display IPv6 rulesYour results will vary, but you’ll want to look for these elements to determine if your firewall is blocking SSH connections: dport 22: This refers to the destination port, which for SSH is usually port 22 (reminder: Kinsta doesn’t use this port number). REJECT: This would indicate that connections are being refused from the specified destination. DROP: Like REJECT, this means that connections to the relevant port are being blocked.If you search the results of the commands above for dport 22, you should be able to determine if your firewall is preventing an SSH connection. If so, you’ll have to change the rules to accept requests.SSH serviceInstallationSSH consists of SSH clicent (openssh-client) and SSH server (openssh-server)3.If you just want to log in to other machine using SSH, only openssh-client is neened. deepin has it installed by default; if not, execute in terminal:$ sudo apt-get install openssh-clientIf you would like to provide SSH service on a server, execute in terminal:$ sudo apt-get install openssh-serverUninstallation$ sudo apt-get remove openssh-client$ sudo apt-get remove openssh-serverConfigurationMake sure that SSH server has been started:$ ps -e |grep sshA process named “sshd” in the output indicates that SSH server has been started successfully. If no one presents, execute:$ sudo /etc/init.d/ssh start or$ sudo service ssh startThe configuration files of SSH server is /etc/ssh/sshd_config, where you can define the port, indentity files and other parameters. The default port for SSH service is 22, you may want to change it to another value, saying 222.Then restart SSH service:$ sudo /etc/init.d/ssh stop$ sudo /etc/init.d/ssh startNow you can log in to the server using SSH:$ ssh Username@192.168.1.112where “Username” is the user name on machine “192.168.1.112”. You may need to provide the password for this user during login.Transfer files between two Unix computersCopy from local to another unix pc$ scp test.txt jmc@192.168.2.25:~/dev/java$ sudo scp test.txt jmc@192.168.2.25:~/dev/java$ scp -r ~/dbbackup jmc@192.168.2.25:~/$ sudo scp -r ~/dbbackup jmc@192.168.2.25:~/Frequently asked questionView and manage logged-in userUse w and pkill command to see active user in the system.Reverse Footnote link to page &amp;#8617; **OpenSSH Full Guide - Everything you need to get started! ** &amp;#8617; **link ** &amp;#8617; " }, { "title": "Dockerize an Angular dev environment", "url": "/posts/dockerize-angular/", "categories": "Docker, Angular", "tags": "dev, docker, tuto, angular", "date": "2022-04-03 23:05:00 -0400", "snippet": "This post will guide you how to create an angular dev environment in docker.Contexte Pour ne pas installer sur son poste node, angular etc … Aussi avoir la possibilt/ de travailler avec differentes versions de node et/ou angularCommenten suivant cette source cloner le repos (sinon cr/ere un nouveau avec son seed) verifier Dockerfile et docker-compose.yml workdir la version de node le nom du service le nom du container builder en executant docker-compose up -d To rebuild this image you must use docker-compose build or docker-compose up --build $ docker-compose -f docker-compose.seed.yml run seed# pour voir les logs$ docker-compose logs -f dev# pour acceder au container$ docker-compose exec dev sh to deploy to ghp in container add ssh key ssh-keygen -t ed25519 -C &quot;jmc@fromdocker.com&quot; if not installed so add ghp npm install -g ghp or ng add angular-cli-ghpages after each npm install repeat 5. to fix errors on vscode, due to missed directory node_modulesrun this batch $ docker cp {containerName}:/angapp/package-lock.json . &amp;amp;&amp;amp; \\&amp;gt; docker cp {containerName}:/angapp/node_modules - &amp;gt; node_modules.tar &amp;amp;&amp;amp; \\&amp;gt; tar -xf node_modules.tar &amp;amp;&amp;amp; \\&amp;gt; rm -f node_modules.tar when we generate components, services … files in container, these are automaticly generated in the host.So we have to change permission owner$ sudo chown -R jmc:jmc folderName/lancer le tout pour continuer le developpement:lancer l’api: aller dans le rep de l’api $docker-compose up -dlancer l’app-ang: aller dans le rep de l’app-ang$docker-compose up -d$docker-compose logs -f devpour le deploiement dans ghps’assurer que le package ghp est installe sinon $ng add angular-cli-ghpagesinstaller .ssh si c’est pas fait$npm run deploy:gh🚀 Uploading via git, please wait...🌟 Successfully published via angular-cli-ghpages! Have a nice day!\\{^_^}/ hi!Errors node:internal/modules/cjs/loader:936 throw err … In container remove node_modules run command npm i to create all dependicies … Add ssh if doesn’t exist re-run the batch to fix errors in vscodesome commands cli# Check angular version$ ng --version# Check npm version$ npm -v" }, { "title": "Google Analytics in Angular", "url": "/posts/ga-in-angular/", "categories": "Marketing", "tags": "dev, ga, tuto, angular", "date": "2022-04-03 19:15:00 -0400", "snippet": "This post will guide you how to implement ga in angular application.Mes compts ga gitjmc (jmc Tizen jmctizen@gmail.com): angapp mesnotes mon site web (qui pointe vers angapp!) a rtirer … githubjn githubjn.github.io les ID de mesure sont dand mestrucs Attention le blocage des navigateurs blocker.How?Google Analytics dans AngApp How ToProperly Add Google Analytics …" }, { "title": "Dockerize Jekyll", "url": "/posts/dockerize-jekyll/", "categories": "Docker, Jekyll", "tags": "dev, docker, tuto", "date": "2022-04-03 18:40:00 -0400", "snippet": "This post will guide you how to dockerize Jekyll.Prerequistes Docker installed; docker-compose.yamlversion: &#39;3&#39;services: jekyll-serve: image: jekyll/jekyll:4.0 volumes: - &#39;.:/srv/jekyll&#39; ports: - 4000:4000 command: &#39;jekyll serve&#39;to run docker-compose$ docker-compose upErrors Unable to monitor directories for changes because iNotify max watches exceeded.Run these commands from the host (not in the docker container)$ echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf$ sudo sysctl -pto display logsdocker logs -f container-namedocker-compose logs -f service_namedeploy to github Be sure to point to the write repository, in my case to gitjmc repo.just an example to do /path/to/a/file.extend$ git add .$ git commit -m&quot;update Dockerize Jekyll&#39;s post&quot;$ git pullgit pushbackup dataall my data are in current directory (see volumes in docker-compose.yaml)" }, { "title": "Mes memo", "url": "/posts/mes-notes/", "categories": "Divers", "tags": "tempo", "date": "2022-03-28 07:41:00 -0400", "snippet": " Ce n’est pas un article mais là dedans mes memo sur ce qui est en cours pour ne pas l’oublier … \\o/ .ghpages first link, also for how to use no silent second link third linkangular multi languages first link, see also my repo json-server doit etre installe pour pouvoir tester … il y a un docker compose (fonctionnel mais tout a fait bien) dedans plusieurs etapes … un nettoyage du code est requis … TitlesH1 - headingH2 - headingH3 - headingH4 - headingParagraphI wandered lonely as a cloudThat floats on high o’er vales and hills,When all at once I saw a crowd,A host, of golden daffodils;Beside the lake, beneath the trees,Fluttering and dancing in the breeze.ListsOrdered list Firstly Secondly ThirdlyUnordered list Chapter Section Paragraph Task list TODO Completed Defeat COVID-19 Vaccine production Economic recovery People smile again Description list Sun the star around which the earth orbits Moon the natural satellite of the earth, visible by reflected light from the sunBlock Quote This line shows the block quote.Prompts An example showing the tip type prompt. An example showing the info type prompt. An example showing the warning type prompt. An example showing the danger type prompt.Tables Company Contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy Linkshttp://127.0.0.1:4000FootnoteClick the hook will locate the footnote1, and here is another footnote2.Images Default (with caption)Full screen width and center alignment Shadowshadow effect (visible in light mode) Left aligned Float to left “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Float to right “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Mermaid SVG gantt title Adding GANTT diagram functionality to mermaid apple :a, 2017-07-20, 1w banana :crit, b, 2017-07-23, 1d cherry :active, c, after b a, 1dMathematicsThe mathematics powered by MathJax:\\[\\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\\]When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]Inline codeThis is an example of Inline Code.FilepathHere is the /path/to/the/file.extend.Code blockCommonThis is a common code snippet, without syntax highlight and line number.Specific LanguagesConsole$ env |grep SHELLSHELL=/usr/local/bin/bashPYENV_SHELL=bashShellif [ $? -ne 0 ]; then echo &quot;The command was not successful.&quot;; #do the needful / exitfi;Specific filename@import &quot;colors/light-typography&quot;, &quot;colors/dark-typography&quot;Reverse Footnote The footnote source &amp;#8617; The 2nd footnote source &amp;#8617; " }, { "title": "Install nodejs on Deepin 20.x", "url": "/posts/install-nodejs-deepin/", "categories": "DEV", "tags": "nodejs, npm, angular", "date": "2022-03-20 20:53:45 -0400", "snippet": "Cet article montre comment installer node1, npm et angular dans Deepin version 20 et plus. Disons que Ça pas mal fonctionné!SolutionSolution (tested on ssd-500)Look for debian’s version-releases used in deepin.buster - est la version de Debian de Deepin 20.4If curl is not installed then you have to dosudo apt update &amp;amp;&amp;amp; sudo apt upgradesudo apt install curl Uninstall old node versionsudo apt -y remove nodejs Add the NodeSource package signing keyKEYRING=/usr/share/keyrings/nodesource.gpgcurl -fsSL https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor | sudo tee &quot;$KEYRING&quot; &amp;gt;/dev/null# wget can also be used:# wget --quiet -O - https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor | sudo tee &quot;$KEYRING&quot; &amp;gt;/dev/nullgpg --no-default-keyring --keyring &quot;$KEYRING&quot; --list-keys Add the desired NodeSource repository# Replace with the branch of Node.js or io.js you want to install: node_6.x, node_8.x, etc...VERSION=node_16.x# Replace with the keyring above, if differentKEYRING=/usr/share/keyrings/nodesource.gpg# The below command will set this correctly, but if lsb_release isn&#39;t available, you can set it manually:DISTRO=&quot;$(buster)&quot;echo &quot;deb [signed-by=$KEYRING] https://deb.nodesource.com/$VERSION $DISTRO main&quot; | sudo tee /etc/apt/sources.list.d/nodesource.listecho &quot;deb-src [signed-by=$KEYRING] https://deb.nodesource.com/$VERSION $DISTRO main&quot; | sudo tee -a /etc/apt/sources.list.d/nodesource.list update package lists and install Node.jssudo apt-get updatesudo apt-get install nodejs chaeck installed versionnode --versionnpm --versionFor more details take a look to this linkinstall angular clisudo npm install -g @angular/cling --versionReverse Footnote Voir ce lien: https://github.com/nodesource/distributions/blob/master/README.md#manual-installation &amp;#8617; " }, { "title": "Multiple accounts (github and/or bitbucket) on one computer", "url": "/posts/github-bitbucket-one-pc/", "categories": "cloud, repository", "tags": "dev, tuto, github, bitbucket", "date": "2022-03-06 08:46:00 -0500", "snippet": "How to manage multiple accounts github and bitbucket on the same computer …\\o/…RecetteFor genratre ssh keyssh-keygen -t ed25519 -C &quot;your@email.com&quot;for more details see this youtube - https://www.youtube.com/watch?v=vSeYsk4WYvgCreate config file in .ssh folder#Default accountHost bibucket.org HostName bitbucket.org User git IdentityFile ~/.ssh/id_rsa#Personal accountHost bitbucket-perso Hostname bitbucket.org User git IdentityFile ~/.ssh/id_rsa_perso#professsional accountHost bitbucket-mtl Hostname bitbucket.org User git IdentityFile ~/.ssh/id_rsa_mtl#github account - not created yet!#Host github.com# Hostname github.com# User git# IdentityFile ~/.ssh/id_#gitjmc accountHost gitjmc Hostname github.com User git IdentityFile ~/.ssh/id_ed25519#gitmohamedcjdev accountHost gitmoh Hostname github.com User git IdentityFile ~/.ssh/gitmoh#Nader accountHost jnader Hostname github.com User git IdentityFile ~/.ssh/id_ed25519_jnader$ bundlepour le déploiement angular-cli-ghpagesyoutube - https://www.youtube.com/watch?v=8Mle8owZ3_4voir les tuto de ssh - voir OpenSSH Full guide - https://www.youtube.com/watch?v=YS5Zh7KExvE - voir Getting Started with OpenSSH Key Management - - to create sshkey ssh-keygen -t ed25519 -C “jmc-dell” - to associate the key with target server or service you have to ssh-copy-id -i ~/.ssh/nameOfKey.pub serverNamecheck if openssh is installed (always installed on linux), which command tell where the binary is installed which ssh apt search openssh-client? use nanocheck a specific process ps aux | grep process-idthe first time whene you connect to server, the server ask you if you are sure to …whene you answer yes, the fingerprint done by the server is stored in known_hostsle repertoire des logs /var/logpour suivre life le log des authentifications par exemple tail -f auth.logssh config fileHost nodejmc #peut etre n’importe quel nom Hostname 172.105.4.22 #le serveur qu’on veut acceder Port 22 User rootHost nodenader Hostname 172.105.4.140 Port 22 User jmcle public key doit etre mis dans le serveur qu’on veut accederil doit etre dans le fichier (a creer s’ils n’existent pas) .ssh/authorized_keys la meme chose en utilisant ssh-copy-id -i ~/.ssh/id_rsa.pub root@172.105.7.26id_rsa.pub - le public key qu’on veut copier172.105.7.26 - le serveur qu’on veut y accederssh-i ~/.ssh/privateKey root@172.105.7.22ssh agent - it’s to save the key in memory to don’t set each time you connect to serverto find ssh-agentps aux |grep ssh-agentto activate it eval “$(ssh-agent)”###jmc@jmc-PC:/etc/apt$ ps aux |grep ssh-agentjmc 6586 0.0 0.0 6088 900 pts/1 S+ 11:29 0:00 grep ssh-agentjmc@jmc-PC:/etc/apt$ eval “$(ssh-agent)”Agent pid 6882jmc@jmc-PC:/etc/apt$ ps aux |grep ssh-agentjmc 6882 0.0 0.0 5868 472 ? Ss 11:32 0:00 ssh-agentjmc 6893 0.0 0.0 6088 896 pts/1 S+ 11:32 0:00 grep ssh-agent###whene you close the terminal, the ssh-agent is goneto add key to ssh-agent ssh-add ~/thePathToTheKeyThatWeWantToAddonly the first time he will ask you for the passphrase (unless u don’t close the terminal)to accepts connection (server side) ssh deamon –&amp;gt; sshdwhich sshdto check the status systemctl status sshd###jmc@jmc-PC:/etc/apt$ systemctl status sshd● ssh.service - OpenBSD Secure Shell server Loaded: loaded (/lib/systemd/system/ssh.service; enabled; vendor preset: enabled) Active: active (running) since Sat 2022-03-19 14:33:18 EDT; 21h ago Docs: man:sshd(8) man:sshd_config(5) Main PID: 5012 (sshd) Tasks: 1 (limit: 4915) Memory: 3.2M CGroup: /system.slice/ssh.service └─5012 /usr/sbin/sshd -DWarning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.###to change the port on command line ssh -p 2222 root@serverToReach//////////////after the clone you have to npm i ng serve//////////////////////to install ghpage for angular module npm i -g angular-cli-ghpagesng build --prod --base-href &quot;https://username.github.io/repoName/&quot; to upload project over githubangular-cli-ghpages another way to do the uploadngh --dir=dist/repoNamej’ai ajoute des modifs dans package.json.scriptspour builder npm run build npm run deploy" }, { "title": "Mes liens utiles", "url": "/posts/mes-liens-utiles/", "categories": "Divers", "tags": "dev, doc, tuto, security, access", "date": "2022-03-06 08:46:00 -0500", "snippet": "Ma liste des liens qui pourront m’être utiles …\\o/…Cette page contient des liens que j’ai rencontré qq part de partout …Liste des liensOuitl de documentation en ligne json editor - https://jsoneditoronline.org/ sequence diagrams - https://www.websequencediagrams.com/ sequence diagrams js - https://bramp.github.io/js-sequence-diagrams/ diagrams online - https://draw.io Samrt Arabic Keyboard - http://www.yamli.com/arabic-keyboard/DEVfront end (client-side) Bootstrap: Bootstrap Icons - ­https://icons.getbootstrap.com/#usage Doc Bootstrap 4 - https://getbootstrap.com/docs/4.0/components/ angular - https://www.tektutorialshub.com/angular-tutorial/ CSS css stats - Analyser et visualiser le stylesheets https://cssstats.com/ PageSpeed Insights https://pagespeed.web.dev/ Web Developer Checklist https://www.toptal.com/developers/webdevchecklist RegEx RegEx Testing - https://www.regextester.com/21 RegEx Java Tuto - http://www.vogella.com/tutorials/JavaRegularExpressions/article.html online tester - free Photos Free stock Photo - https://www.pexels.com/ Generator online hipster - https://hipsum.co/ Genrate data - http://www.generatedata.com/ Random generatot - https://www.4devs.com.br/ https certbot - https://certbot.eff.org/ ssl config generator - https://mozilla.github.io/server-side-tls/ssl-config-generator/ Let’s encrypt - https://letsencrypt.org/DevOpsDocker using volumes - https://devopsheaven.com/docker/docker-compose/volumes/2018/01/16/volumes-in-docker-compose.htmlAutresDivers mes noeuds - http://mesnoeuds.free.fr/knot.php?knotId=31 Reflexion Stratégie masquée de la guerre: youtube - https://www.youtube.com/watch?v=uz4NaV50wh0 vimeo - https://vimeo.com/129485806 Journaliste achetés - youtube - https://vimeo.com/129485806###" }, { "title": "Common Commands", "url": "/posts/common-commands/", "categories": "CLI", "tags": "cli, linux", "date": "2022-03-05 20:24:00 -0500", "snippet": "C’est la liste de commandes qui pourront me servir (pour ne pas les oublier) \\o/Liste des commandespwdPrint working directory$ pwd/home/jmc/dev/tempo/jekyll/gitjmc.github.iocpCopy files and folders …$ cp file_to_copy.txt new_file.txt$ cp -r dir_to_copy/ new_copy_dir/ Remember that in Linux, folders end with a forward slash /.rmRemove files and folders …$ rm file_to_ermove.txt$ cp -r dir_to_remove/$ rm -rf dir_with_content_to_remove/grep$ history | grep &#39;exec&#39;scpSecurely copy files and directories between two locations, How to Use SCP Command to Securely Transfer Files.Another interesting link, optimiser les recherches.$ history | grep &#39;exec&#39;vimHow can i save a file opened in vim as the wrong user E45 readonly option is set (add ! to override).:w !sudo tee %Explanation: :w – write !sudo – call shell sudo command tee – the output of write (:w) command is redirected using tee % – current file nameanother waypress Esc, then type :qa! and press Enteruninstallfor example to uninstall libreoffice$ sudo apt-get purge libreoffice*systemctlDebian Service managersystemctl status sshsystemctl start sshsystemctl stop sshsystemctl start snapd.servicesystemctl status snapd.service" }, { "title": "Install Docker on Deepin 20.x", "url": "/posts/docker-install/", "categories": "Linux", "tags": "deepin, docker", "date": "2022-02-27 15:03:00 -0500", "snippet": "Cet article montre comment peut-on installer docker dans Deepin1 version 20.x et plus. MAJ ‘2022-04-18’Solutioninstall DockerPrerequistes$ sudo apt-get update$ sudo apt install -y apt-transport-https ca-certificates gnupg2 curl software-properties-common$ sudo curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -$ sudo apt-key fingerprint 0EBFCD88Check source.list filepath /etc/apt/sources.listpath /etc/apt/sources.list.d/docker-ce.list$ cat /etc/apt/sources.listInstall docker$ sudo apt-get update$ sudo apt-get install -y docker-ce$ docker --versioninstall docker-machineSource pour installer docker machine 2.$ curl -L https://github.com/docker/machine/releases/download/v0.16.2/docker-machine-`uname -s`-`uname -m` &amp;gt;/tmp/docker-machine &amp;amp;&amp;amp;&amp;gt; chmod +x /tmp/docker-machine &amp;amp;&amp;amp;&amp;gt; sudo cp /tmp/docker-machine /usr/local/bin/docker-machine$ docker-machine --versioninstall docker-composeSource pour installer docker compose3.$ sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose$ docker-compose versionvoir aussi cette source4deb [arch=amd64] https://download.docker.com/linux/debian buster stabledeb [by-hash=force] https://community-packages.deepin.com/deepin/ apricot main contrib non-freedeb [arch=amd64] https://download.virtualbox.org/virtualbox/debian buster contrib+ deb [arch=amd64] https://download.docker.com/linux/debian buster stable buster - est la version Debian de DeepinReverse Footnote Voir ce liensur youtube &amp;#8617; source in github docker machine &amp;#8617; source in github docker compose &amp;#8617; source article &amp;#8617; " }, { "title": "Enable Google Page Views", "url": "/posts/enable-google-pv/", "categories": "Blogging, Tutorial", "tags": "ga, pageviews", "date": "2021-01-03 05:32:00 -0500", "snippet": "This post is to enable Page Views on the Chirpy theme based blog that you just built. This requires technical knowledge and it’s recommended to keep the google_analytics.pv.* empty unless you have a good reason. If your website has low traffic, the page views count would discourage you to write more blogs. With that said, let’s start with the setup.Set up Google AnalyticsCreate GA account and propertyFirst, you need to set up your account on Google analytics. While you create your account, you must create your first Property as well. Head to https://analytics.google.com/ and click on Start Measuring Enter your desired Account Name and choose the desired checkboxes Enter your desired Property Name. This is the name of the tracker project that appears on your Google Analytics dashboard Enter the required information About your business Hit Create and accept any license popup to set up your Google Analytics account and create your propertyCreate Data StreamWith your property created, you now need to set up Data Stream to track your blog traffic. After you signup, the prompt should automatically take you to create your first Data Stream. If not, follow these steps: Go to Admin on the left column Select the desired property from the drop-down on the second column Click on Data Streams Add a stream and click on Web Enter your blog’s URLIt should look like this:Now, click on the new data stream and grab the Measurement ID. It should look something like G-V6XXXXXXXX. Copy this to your _config.yml file:google_analytics: id: &#39;G-V6XXXXXXX&#39; # fill in your Google Analytics ID # Google Analytics pageviews report settings pv: proxy_endpoint: # fill in the Google Analytics superProxy endpoint of Google App Engine cache_path: # the local PV cache data, friendly to visitors from GFW regionWhen you push these changes to your blog, you should start seeing the traffic on your Google Analytics. Play around with the Google Analytics dashboard to get familiar with the options available as it takes like 5 mins to pick up your changes. You should now be able to monitor your traffic in real time.Setup Page ViewsThere is a detailed tutorial available to set up Google Analytics superProxy. But, if you are interested to just quickly get your Chirpy-based blog display page views, follow along. These steps were tested on a Linux machine. If you are running Windows, you can use the Git bash terminal to run Unix-like commands.Setup Google App Engine Visit https://console.cloud.google.com/appengine Click on Create Application Click on Create Project Enter the name and choose the data center close to you Select Python language and Standard environment Enable billing account. Yeah, you have to link your credit card. But, you won’t be billed unless you exceed your free quota. For a simple blog, the free quota is more than sufficient. Go to your App Engine dashboard on your browser and select API &amp;amp; Services from the left navigation menu Click on Enable APIs and Services button on the top Enable the following APIs: Google Analytics API On the left, Click on OAuth Consent Screen and accept Configure Consent Screen. Select External since your blog is probably hosted for the public. Click on Publish under Publishing Status Click on Credentials on the left and create a new OAuth Client IDs credential. Make sure to add an entry under Authorized redirect URIs that matches: https://&amp;lt;project-id&amp;gt;.&amp;lt;region&amp;gt;.r.appspot.com/admin/auth Note down the Your Client ID and Your Client Secret. You’ll need this in the next section. Download and install the cloud SDK for your platform: https://cloud.google.com/sdk/docs/quickstart Run the following commands: [root@bc96abf71ef8 /]# gcloud init~snip~Go to the following link in your browser: https://accounts.google.com/o/oauth2/auth?response_type=code&amp;amp;client_id=XYZ.apps.googleusercontent.com&amp;amp;redirect_uri=ABCDEFGEnter verification code: &amp;lt;VERIFICATION CODE THAT YOU GET AFTER YOU VISIT AND AUTHENTICATE FROM THE ABOVE LINK&amp;gt;You are logged in as: [blah_blah@gmail.com].Pick cloud project to use:[1] chirpy-test-300716[2] Create a new projectPlease enter numeric choice or text value (must exactly match listitem): 1[root@bc96abf71ef8 /]# gcloud info# Your selected project info should be displayed here Setup Google Analytics superProxy Clone the Google Analytics superProxy project on Github: https://github.com/googleanalytics/google-analytics-super-proxy to your local. Remove the first 2 lines in the src/app.yaml file: - application: your-project-id- version: 1 In src/config.py, add the OAUTH_CLIENT_ID and OAUTH_CLIENT_SECRET that you gathered from your App Engine Dashboard. Enter any random key for XSRF_KEY, your config.py should look similar to this #!/usr/bin/python2.7__author__ = &#39;pete.frisella@gmail.com (Pete Frisella)&#39;# OAuth 2.0 Client SettingsAUTH_CONFIG = { &#39;OAUTH_CLIENT_ID&#39;: &#39;YOUR_CLIENT_ID&#39;, &#39;OAUTH_CLIENT_SECRET&#39;: &#39;YOUR_CLIENT_SECRET&#39;, &#39;OAUTH_REDIRECT_URI&#39;: &#39;%s%s&#39; % ( &#39;https://chirpy-test-XXXXXX.ue.r.appspot.com&#39;, &#39;/admin/auth&#39; )}# XSRF SettingsXSRF_KEY = &#39;OnceUponATimeThereLivedALegend&#39; You can configure a custom domain instead of https://PROJECT_ID.REGION_ID.r.appspot.com.But, for the sake of keeping it simple, we will be using the Google provided default URL. From inside the src/ directory, deploy the app [root@bc96abf71ef8 src]# gcloud app deployServices to deploy:descriptor: [/tmp/google-analytics-super-proxy/src/app.yaml]source: [/tmp/google-analytics-super-proxy/src]target project: [chirpy-test-XXXX]target service: [default]target version: [VESRION_NUM]target url: [https://chirpy-test-XXXX.ue.r.appspot.com]Do you want to continue (Y/n)? YBeginning deployment of service [default]...╔════════════════════════════════════════════════════════════╗╠═ Uploading 1 file to Google Cloud Storage ═╣╚════════════════════════════════════════════════════════════╝File upload done.Updating service [default]...done.Setting traffic split for service [default]...done.Deployed service [default] to [https://chirpy-test-XXXX.ue.r.appspot.com]You can stream logs from the command line by running:$ gcloud app logs tail -s defaultTo view your application in the web browser run:$ gcloud app browse Visit the deployed service. Add a /admin to the end of the URL. Click on Authorize Users and make sure to add yourself as a managed user. If you get any errors, please Google it. The errors are self-explanatory and should be easy to fix. If everything went good, you’ll get this screen:Create Google Analytics QueryHead to https://PROJECT_ID.REGION_ID.r.appspot.com/admin and create a query after verifying the account. GA Core Reporting API query request can be created in Query Explorer.The query parameters are as follows: start-date: fill in the first day of blog posting end-date: fill in today (this is a parameter supported by GA Report, which means that it will always end according to the current query date) metrics: select ga:pageviews dimensions: select ga:pagePathIn order to reduce the returned results and reduce the network bandwidth, we add custom filtering rules 1: filters: fill in ga:pagePath=~^/posts/.*/$;ga:pagePath!@=. Among them, ; means using logical AND to concatenate two rules. If the site.baseurl is specified, change the first filtering rule to ga:pagePath=~^/BASE_URL/posts/.*/$, where BASE_URL is the value of site.baseurl. After Run Query, copy the generated contents of API Query URI at the bottom of the page and fill in the Encoded URI for the query of SuperProxy on GAE.After the query is saved on GAE, a Public Endpoint (public access address) will be generated, and we will get the query result in JSON format when accessing it. Finally, click Enable Endpoint in Public Request Endpoint to make the query effective, and click Start Scheduling in Scheduling to start the scheduled task.Configure Chirpy to Display Page ViewOnce all the hard part is done, it is very easy to enable the Page View on Chirpy theme. Your superProxy dashboard should look something like below and you can grab the required values.Update the _config.yml file of Chirpy project with the values from your dashboard, to look similar to the following:google_analytics: id: &#39;G-V6XXXXXXX&#39; # fill in your Google Analytics ID pv: proxy_endpoint: &#39;https://PROJECT_ID.REGION_ID.r.appspot.com/query?id=&amp;lt;ID FROM SUPER PROXY&amp;gt;&#39; cache_path: # the local PV cache data, friendly to visitors from GFW regionNow, you should see the Page View enabled on your blog.Reference Google Analytics Core Reporting API: Filters &amp;#8617; " }, { "title": "Customize the Favicon", "url": "/posts/customize-the-favicon/", "categories": "Blogging, Tutorial", "tags": "favicon", "date": "2019-08-10 12:34:00 -0400", "snippet": "The favicons of Chirpy are placed in the directory assets/img/favicons/. You may want to replace them with your own. The following sections will guide you to create and replace the default favicons.Generate the faviconPrepare a square image (PNG, JPG, or SVG) with a size of 512x512 or more, and then go to the online tool Real Favicon Generator and click the button Select your Favicon image to upload your image file.In the next step, the webpage will show all usage scenarios. You can keep the default options, scroll to the bottom of the page, and click the button Generate your Favicons and HTML code to generate the favicon.Download &amp;amp; ReplaceDownload the generated package, unzip and delete the following two from the extracted files: browserconfig.xml site.webmanifestAnd then copy the remaining image files (.PNG and .ICO) to cover the original files in the directory assets/img/favicons/ of your Jekyll site. If your Jekyll site doesn’t have this directory yet, just create one.The following table will help you understand the changes to the favicon files: File(s) From Online Tool From Chirpy *.PNG ✓ ✗ *.ICO ✓ ✗ ✓ means keep, ✗ means delete.The next time you build the site, the favicon will be replaced with a customized edition." }, { "title": "Getting Started", "url": "/posts/getting-started/", "categories": "Blogging, Tutorial", "tags": "getting started", "date": "2019-08-09 08:55:00 -0400", "snippet": "PrerequisitesFollow the instructions in the Jekyll Docs to complete the installation of Ruby, RubyGems, Jekyll, and Bundler. In addition, Git is also required to be installed.InstallationCreating a New SiteThere are two ways to create a new repository for this theme: Using the Chirpy Starter - Easy to upgrade, isolates irrelevant project files so you can focus on writing. Forking on GitHub - Convenient for custom development, but difficult to upgrade. Unless you are familiar with Jekyll and are determined to tweak or contribute to this project, this approach is not recommended.Option 1. Using the Chirpy StarterCreate a new repository from the Chirpy Starter and name it &amp;lt;GH_USERNAME&amp;gt;.github.io, where GH_USERNAME represents your GitHub username.Option 2. Forking on GitHubFork Chirpy on GitHub and rename it to &amp;lt;GH_USERNAME&amp;gt;.github.io. Please note that the default branch code is in development. If you want the site to be stable, please switch to the latest tag and start writing.And then execute:$ bash tools/init.sh If you don’t want to deploy your site on GitHub Pages, append option --no-gh at the end of the above command.The above command will: Removes some files or directories from your repository: .travis.yml files under _posts If the option --no-gh is provided, the directory .github will be deleted. Otherwise, set up the GitHub Action workflow by removing the extension .hook of .github/workflows/pages-deploy.yml.hook, and then remove the other files and directories in the folder .github. Removes item Gemfile.lock from .gitignore. Creates a new commit to save the changes automatically.Installing DependenciesBefore running for the first time, go to the root directory of your site, and install dependencies as follows:$ bundleUsageConfigurationUpdate the variables of _config.yml as needed. Some of them are typical options: url avatar timezone langCustoming StylesheetIf you need to customize the stylesheet, copy the theme’s assets/css/style.scss to the same path on your Jekyll site, and then add the custom style at the end of the style file.Starting from v4.1.0, if you want to overwrite the SASS variables defined in _sass/addon/variables.scss, create a new file _sass/variables-hook.scss and assign new values to the target variable in it.Customing Static AssetsStatic assets configuration was introduced in version 5.1.0. The CDN of the static assets is defined by file _data/assets/cross_origin.yml, and you can replace some of them according to the network conditions in the region where your website is published.Also, if you’d like to self-host the static assets, please refer to the chirpy-static-assets.Running Local ServerYou may want to preview the site contents before publishing, so just run it by:$ bundle exec jekyll sOr run the site on Docker with the following command:$ docker run -it --rm \\ --volume=&quot;$PWD:/srv/jekyll&quot; \\ -p 4000:4000 jekyll/jekyll \\ jekyll serveAfter a while, the local service will be published at http://127.0.0.1:4000.DeploymentBefore the deployment begins, check out the file _config.yml and make sure the url is configured correctly. Furthermore, if you prefer the project site and don’t use a custom domain, or you want to visit your website with a base URL on a web server other than GitHub Pages, remember to change the baseurl to your project name that starts with a slash, e.g, /project-name.Now you can choose ONE of the following methods to deploy your Jekyll site.Deploy by Using Github ActionsFor security reasons, GitHub Pages build runs on safe mode, which restricts us from using plugins to generate additional page files. Therefore, we can use GitHub Actions to build the site, store the built site files on a new branch, and use that branch as the source of the GitHub Pages service.Quickly check the files needed for GitHub Actions build: Ensure your Jekyll site has the file .github/workflows/pages-deploy.yml. Otherwise, create a new one and fill in the contents of the sample file, and the value of the on.push.branches should be the same as your repo’s default branch name. Ensure your Jekyll site has file tools/deploy.sh. Otherwise, copy it from here to your Jekyll site. Furthermore, if you have committed Gemfile.lock to the repo, and your runtime system is not Linux, don’t forget to update the platform list in the lock file: $ bundle lock --add-platform x86_64-linux After the above steps, rename your repository to &amp;lt;GH_USERNAME&amp;gt;.github.io on GitHub.Now publish your Jekyll site by: Push any commit to remote to trigger the GitHub Actions workflow. Once the build is complete and successful, a new remote branch named gh-pages will appear to store the built site files. Browse to your repository on GitHub. Select the tab Settings, then click Pages in the left navigation bar, and then in the section Source of GitHub Pages, select the /(root) directory of branch gh-pages as the publishing source. Remember to click Save before leaving. Visit your website at the address indicated by GitHub. Manually Build and DeployOn self-hosted servers, you cannot enjoy the convenience of GitHub Actions. Therefore, you should build the site on your local machine and then upload the site files to the server.Go to the root of the source project, and build your site as follows:$ JEKYLL_ENV=production bundle exec jekyll bOr build the site on Docker:$ docker run -it --rm \\ --env JEKYLL_ENV=production \\ --volume=&quot;$PWD:/srv/jekyll&quot; \\ jekyll/jekyll \\ jekyll buildUnless you specified the output path, the generated site files will be placed in folder _site of the project’s root directory. Now you should upload those files to the target server.UpgradingIt depends on how you use the theme: If you are using the theme gem (there will be gem &quot;jekyll-theme-chirpy&quot; in the Gemfile), editing the Gemfile and update the version number of the theme gem, for example: - gem &quot;jekyll-theme-chirpy&quot;, &quot;~&amp;gt; 3.2&quot;, &quot;&amp;gt;= 3.2.1&quot;+ gem &quot;jekyll-theme-chirpy&quot;, &quot;~&amp;gt; 3.3&quot;, &quot;&amp;gt;= 3.3.0&quot; And then execute the following command: $ bundle update jekyll-theme-chirpy As the version upgrades, the critical files (for details, see the Startup Template) and configuration options will change. Please refer to the Upgrade Guide to keep your repo’s files in sync with the latest version of the theme. If you forked from the source project (there will be gemspec in the Gemfile of your site), then merge the latest upstream tags into your Jekyll site to complete the upgrade.The merge is likely to conflict with your local modifications. Please be patient and careful to resolve these conflicts. " }, { "title": "Writing a New Post", "url": "/posts/write-a-new-post/", "categories": "Blogging, Tutorial", "tags": "writing", "date": "2019-08-08 02:10:00 -0400", "snippet": "This post will guide you how to write a post on Chirpy theme. Even if you have previous experience with Jekyll, this article is worth reading, because many features require specific variables to be set.Naming and PathCreate a new file named YYYY-MM-DD-TITLE.EXTENSION and put it in the _posts of the root directory. Please note that the EXTENSION must be one of md and markdown. If you want to save time of creating files, please consider using the plugin Jekyll-Compose to accomplish this.Front MatterBasically, you need to fill the Front Matter as below at the top of the post:---title: TITLEdate: YYYY-MM-DD HH:MM:SS +/-TTTTcategories: [TOP_CATEGORIE, SUB_CATEGORIE]tags: [TAG] # TAG names should always be lowercase--- The posts’ layout has been set to post by default, so there is no need to add the variable layout in the Front Matter block.Timezone of DateIn order to accurately record the release date of a post, you should not only set up the timezone of _config.yml but also provide the post’s timezone in variable date of its Front Matter block. Format: +/-TTTT, e.g. +0800.Categories and TagsThe categories of each post are designed to contain up to two elements, and the number of elements in tags can be zero to infinity. For instance:---categories: [Animal, Insect]tags: [bee]---Author InformationThe author information of the post usually does not need to be filled in the Front Matter , they will be obtained from variables social.name and the first entry of social.links of the configuration file by default. But you can also override it as follows:---author: name: Full Name link: https://example.com---Table of ContentsBy default, the Table of Contents (TOC) is displayed on the right panel of the post. If you want to turn it off globally, go to _config.yml and set the value of variable toc to false. If you want to turn off TOC for a specific post, add the following to the post’s Front Matter:---toc: false---CommentsThe global switch of comments is defined by variable comments.active in the file _config.yml. After selecting a comment system for this variable, comments will be turned on for all posts.If you want to close the comment for a specific post, add the following to the Front Matter of the post:---comments: false---MathematicsFor website performance reasons, the mathematical feature won’t be loaded by default. But it can be enabled by:---math: true---MermaidMermaid is a great diagrams generation tool. To enable it on your post, add the following to the YAML block:---mermaid: true---Then you can use it like other markdown languages: surround the graph code with ```mermaid and ```.ImagesCaptionAdd italics to the next line of an image，then it will become the caption and appear at the bottom of the image:![img-description](/path/to/image)_Image Caption_SizeIn order to prevent the page content layout from shifting when the image is loaded, we should set the width and height for each image:![Desktop View](/assets/img/sample/mockup.png){: width=&quot;700&quot; height=&quot;400&quot; }Starting from Chirpy v5.0.0, height and width support abbreviations (height → h, width → w). The following example has the same effect as the above:![Desktop View](/assets/img/sample/mockup.png){: w=&quot;700&quot; h=&quot;400&quot; }PositionBy default, the image is centered, but you can specify the position by using one of the classes normal, left, and right. Once the position is specified, the image caption should not be added. Normal position Image will be left aligned in below sample: ![Desktop View](/assets/img/sample/mockup.png){: .normal } Float to the left ![Desktop View](/assets/img/sample/mockup.png){: .left } Float to the right ![Desktop View](/assets/img/sample/mockup.png){: .right } ShadowThe screenshots of the program window can be considered to show the shadow effect, and the shadow will be visible in the light mode:![Desktop View](/assets/img/sample/mockup.png){: .shadow }CDN URLIf you host the images on the CDN, you can save the time of repeatedly writing the CDN URL by assigning the variable img_cdn of _config.yml file:img_cdn: https://cdn.comOnce img_cdn is assigned, the CDN URL will be added to the path of all images (images of site avatar and posts) starting with /.For instance, when using images:![The flower](/path/to/flower.png)The parsing result will automatically add the CDN prefix https://cdn.com before the image path:&amp;lt;img src=&quot;https://cdn.com/path/to/flower.png&quot; alt=&quot;The flower&quot;&amp;gt;Image PathWhen a post contains many images, it will be a time-consuming task to repeatedly define the path of the images. To solve this, we can define this path in the YAML block of the post:---img_path: /img/path/---And then, the image source of Markdown can write the file name directly:![The flower](flower.png)The output will be:&amp;lt;img src=&quot;/img/path/flower.png&quot; alt=&quot;The flower&quot;&amp;gt;Preview ImageIf you want to add an image to the top of the post contents, specify the attribute src, width, height, and alt for the image:---image: src: /path/to/image/file width: 1000 # in pixels height: 400 # in pixels alt: image alternative text---Except for alt, all other options are necessary, especially the width and height, which are related to user experience and web page loading performance. The above section “Size” also mentions this.Starting from Chirpy v5.0.0, the attributes height and width can be abbreviated: height → h, width → w. In addition, the img_path can also be passed to the preview image, that is, when it has been set, the attribute src only needs the image file name.Pinned PostsYou can pin one or more posts to the top of the home page, and the fixed posts are sorted in reverse order according to their release date. Enable by:---pin: true---PromptsThere are several types of prompts: tip, info, warning, and danger. They can be generated by adding the class prompt-{type} to the blockquote. For example, define a prompt of type info as follows:&amp;gt; Example line for prompt.{: .prompt-info }SyntaxInline Code`inline code part`Filepath Hightlight`/path/to/a/file.extend`{: .filepath}Code BlockMarkdown symbols ``` can easily create a code block as follows:```This is a plaintext code snippet.```Specifying LanguageUsing ```{language} you will get a code block with syntax highlight:```yamlkey: value``` The Jekyll tag {% highlight %} is not compatible with this theme.Line NumberBy default, all languages except plaintext, console, and terminal will display line numbers. When you want to hide the line number of a code block, add the class nolineno to it:```shellecho &#39;No more line numbers!&#39;```{: .nolineno }Specifying the FilenameYou may have noticed that the code language will be displayed at the top of the code block. If you want to replace it with the file name, you can add the attribute file to achieve this:```shell# content```{: file=&quot;path/to/file&quot; }Liquid CodesIf you want to display the Liquid snippet, surround the liquid code with {% raw %} and {% endraw %}:{% raw %}```liquid{% if product.title contains &#39;Pack&#39; %} This product&#39;s title contains the word Pack.{% endif %}```{% endraw %}Or adding render_with_liquid: false (Requires Jekyll 4.0 or higher) to the post’s YAML block.Learn MoreFor more knowledge about Jekyll posts, visit the Jekyll Docs: Posts." }, { "title": "Text and Typography", "url": "/posts/text-and-typography/", "categories": "Blogging, Demo", "tags": "typography", "date": "2019-08-07 23:33:00 -0400", "snippet": "This post is to show Markdown syntax rendering on Chirpy, you can also use it as an example of writing. Now, let’s start looking at text and typography.TitlesH1 - headingH2 - headingH3 - headingH4 - headingParagraphI wandered lonely as a cloudThat floats on high o’er vales and hills,When all at once I saw a crowd,A host, of golden daffodils;Beside the lake, beneath the trees,Fluttering and dancing in the breeze.ListsOrdered list Firstly Secondly ThirdlyUnordered list Chapter Section Paragraph Task list TODO Completed Defeat COVID-19 Vaccine production Economic recovery People smile again Description list Sun the star around which the earth orbits Moon the natural satellite of the earth, visible by reflected light from the sunBlock Quote This line shows the block quote.Prompts An example showing the tip type prompt. An example showing the info type prompt. An example showing the warning type prompt. An example showing the danger type prompt.Tables Company Contact Country Alfreds Futterkiste Maria Anders Germany Island Trading Helen Bennett UK Magazzini Alimentari Riuniti Giovanni Rovelli Italy Linkshttp://127.0.0.1:4000FootnoteClick the hook will locate the footnote1, and here is another footnote2.Images Default (with caption)Full screen width and center alignment Shadowshadow effect (visible in light mode) Left aligned Float to left “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Float to right “A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space. A repetitive and meaningless text is used to fill the space.” Mermaid SVG gantt title Adding GANTT diagram functionality to mermaid apple :a, 2017-07-20, 1w banana :crit, b, 2017-07-23, 1d cherry :active, c, after b a, 1dMathematicsThe mathematics powered by MathJax:\\[\\sum_{n=1}^\\infty 1/n^2 = \\frac{\\pi^2}{6}\\]When $a \\ne 0$, there are two solutions to $ax^2 + bx + c = 0$ and they are\\[x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}\\]Inline codeThis is an example of Inline Code.FilepathHere is the /path/to/the/file.extend.Code blockCommonThis is a common code snippet, without syntax highlight and line number.Specific LanguagesConsole$ env |grep SHELLSHELL=/usr/local/bin/bashPYENV_SHELL=bashShellif [ $? -ne 0 ]; then echo &quot;The command was not successful.&quot;; #do the needful / exitfi;Specific filename@import &quot;colors/light-typography&quot;, &quot;colors/dark-typography&quot;Reverse Footnote The footnote source &amp;#8617; The 2nd footnote source &amp;#8617; " } ]
